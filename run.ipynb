{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71b87532-9cf1-41ec-a947-e52421a3e73e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T00:55:40.291214Z",
     "iopub.status.busy": "2025-12-15T00:55:40.291214Z",
     "iopub.status.idle": "2025-12-15T01:02:49.003464Z",
     "shell.execute_reply": "2025-12-15T01:02:49.002444Z",
     "shell.execute_reply.started": "2025-12-15T00:55:40.291214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      " Agent0 Episode 1 | avgR=0.0117 | violations=2 | avgBarrier=79.9997 | gradNorm=53.1643\n",
      " Agent0 Episode 2 | avgR=0.0117 | violations=2 | avgBarrier=474.0942 | gradNorm=908.2316\n",
      " Agent0 Episode 3 | avgR=0.0156 | violations=7 | avgBarrier=344.9597 | gradNorm=501.1863\n",
      " Agent0 Episode 4 | avgR=0.0137 | violations=2 | avgBarrier=54.6904 | gradNorm=12.7372\n",
      " Agent0 Episode 5 | avgR=0.0148 | violations=4 | avgBarrier=58.2378 | gradNorm=16.0290\n",
      " Agent0 Episode 6 | avgR=0.0163 | violations=3 | avgBarrier=81.1523 | gradNorm=73.8261\n",
      " Agent0 Episode 7 | avgR=0.0156 | violations=4 | avgBarrier=94.1987 | gradNorm=98.8045\n",
      " Agent0 Episode 8 | avgR=0.0142 | violations=3 | avgBarrier=83.7932 | gradNorm=103.5903\n",
      " Agent0 Episode 9 | avgR=0.0139 | violations=6 | avgBarrier=123.5475 | gradNorm=211.3450\n",
      " Agent0 Episode 10 | avgR=0.0129 | violations=2 | avgBarrier=141.2651 | gradNorm=342.0185\n",
      " Agent0 Episode 11 | avgR=0.0121 | violations=5 | avgBarrier=106.5800 | gradNorm=132.3831\n",
      " Agent0 Episode 12 | avgR=0.0114 | violations=4 | avgBarrier=282.8283 | gradNorm=785.2758\n",
      " Agent0 Episode 13 | avgR=0.0108 | violations=4 | avgBarrier=67.4551 | gradNorm=49.7031\n",
      " Agent0 Episode 14 | avgR=0.0100 | violations=5 | avgBarrier=222.3611 | gradNorm=345.7590\n",
      " Agent0 Episode 15 | avgR=0.0094 | violations=2 | avgBarrier=102.2427 | gradNorm=233.5368\n",
      " Agent0 Episode 16 | avgR=0.0093 | violations=2 | avgBarrier=53.5451 | gradNorm=35.9615\n",
      " Agent0 Episode 17 | avgR=0.0087 | violations=4 | avgBarrier=75.7664 | gradNorm=77.8723\n",
      " Agent0 Episode 18 | avgR=0.0089 | violations=6 | avgBarrier=211.9572 | gradNorm=445.5490\n",
      " Agent0 Episode 19 | avgR=0.0084 | violations=1 | avgBarrier=45.1201 | gradNorm=10.2247\n",
      " Agent0 Episode 20 | avgR=0.0080 | violations=0 | avgBarrier=43.0456 | gradNorm=12.0383\n",
      " Agent0 Episode 21 | avgR=0.0076 | violations=3 | avgBarrier=69.9867 | gradNorm=59.1124\n",
      " Agent0 Episode 22 | avgR=0.0070 | violations=0 | avgBarrier=42.7294 | gradNorm=8.6726\n",
      " Agent0 Episode 23 | avgR=0.0059 | violations=0 | avgBarrier=42.7881 | gradNorm=6.5017\n",
      " Agent0 Episode 24 | avgR=0.0059 | violations=1 | avgBarrier=45.9810 | gradNorm=10.0039\n",
      " Agent0 Episode 25 | avgR=0.0051 | violations=2 | avgBarrier=53.5403 | gradNorm=32.8237\n",
      " Agent0 Episode 26 | avgR=0.0041 | violations=2 | avgBarrier=45.8077 | gradNorm=9.3275\n",
      " Agent0 Episode 27 | avgR=0.0037 | violations=0 | avgBarrier=42.5656 | gradNorm=12.9756\n",
      " Agent0 Episode 28 | avgR=0.0037 | violations=1 | avgBarrier=54.0553 | gradNorm=41.0608\n",
      " Agent0 Episode 29 | avgR=0.0033 | violations=1 | avgBarrier=44.4732 | gradNorm=11.9664\n",
      " Agent0 Episode 30 | avgR=0.0031 | violations=1 | avgBarrier=45.4056 | gradNorm=18.9480\n",
      " Agent0 Episode 31 | avgR=0.0029 | violations=1 | avgBarrier=71.3365 | gradNorm=72.7193\n",
      " Agent0 Episode 32 | avgR=0.0031 | violations=1 | avgBarrier=53.0353 | gradNorm=36.8609\n",
      " Agent0 Episode 33 | avgR=0.0029 | violations=2 | avgBarrier=54.2662 | gradNorm=41.2738\n",
      " Agent0 Episode 34 | avgR=0.0031 | violations=2 | avgBarrier=54.6397 | gradNorm=34.2761\n",
      " Agent0 Episode 35 | avgR=0.0033 | violations=2 | avgBarrier=46.0613 | gradNorm=14.3542\n",
      " Agent0 Episode 36 | avgR=0.0031 | violations=1 | avgBarrier=42.9282 | gradNorm=7.3256\n",
      " Agent0 Episode 37 | avgR=0.0035 | violations=1 | avgBarrier=42.3761 | gradNorm=9.1641\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 91\u001b[39m\n\u001b[32m     88\u001b[39m optimizers = [torch.optim.Adam(p.parameters(), lr=\u001b[32m0.1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m policies]\n\u001b[32m     90\u001b[39m initialize_policy_with_manual_probs(policies[\u001b[32m0\u001b[39m], probs_padded)\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m scores, barrier, violation = \u001b[43mreinforce_multi_rwd2go_alt_barrier\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m fig, ax1 = plt.subplots()\n\u001b[32m     94\u001b[39m ax2 = ax1.twinx()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive - sabanciuniv.edu\\Desktop\\SemesterPr_RL\\ContrainedRL\\training\\reinforce.py:32\u001b[39m, in \u001b[36mreinforce_multi_rwd2go_alt_barrier\u001b[39m\u001b[34m(env, policies, optimizers, n_episodes, max_t, gamma, batch_size, print_every, num_cons)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_episodes):\n\u001b[32m     30\u001b[39m     episode_count += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     batch_data, batch_violation_count  = \u001b[43mcollect_batch_trajectories\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m     C_tau_batch = torch.stack([traj[\u001b[33m\"\u001b[39m\u001b[33mC_tau\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m traj \u001b[38;5;129;01min\u001b[39;00m batch_data])\n\u001b[32m     34\u001b[39m     V_i_estimate = update_constraint_value_estimates_batch(C_tau_batch, V_i_estimate, alpha_vi)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive - sabanciuniv.edu\\Desktop\\SemesterPr_RL\\ContrainedRL\\training\\collect.py:27\u001b[39m, in \u001b[36mcollect_batch_trajectories\u001b[39m\u001b[34m(env, policies, phase, batch_size, max_t)\u001b[39m\n\u001b[32m     24\u001b[39m episode_has_violation = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_t):\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     a, lp, ent = \u001b[43mpolicies\u001b[49m\u001b[43m[\u001b[49m\u001b[43mphase\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m[\u001b[49m\u001b[43mphase\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m     next_states, step_rewards, dones = env.step([a])\n\u001b[32m     31\u001b[39m     C_tau, violated = log_barrier_penalty(next_states, C_tau, env)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive - sabanciuniv.edu\\Desktop\\SemesterPr_RL\\ContrainedRL\\policies\\softmax_policy.py:48\u001b[39m, in \u001b[36mSoftmaxPolicy.act\u001b[39m\u001b[34m(self, state)\u001b[39m\n\u001b[32m     46\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.logits[y, x]                    \u001b[38;5;66;03m# shape [num_actions]\u001b[39;00m\n\u001b[32m     47\u001b[39m probs  = torch.softmax(logits, dim=\u001b[32m0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m dist   = \u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m a      = dist.sample()\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m a.item(), dist.log_prob(a), dist.entropy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\RL_Project\\Lib\\site-packages\\torch\\distributions\\categorical.py:70\u001b[39m, in \u001b[36mCategorical.__init__\u001b[39m\u001b[34m(self, probs, logits, validate_args)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28mself\u001b[39m._num_events = \u001b[38;5;28mself\u001b[39m._param.size()[-\u001b[32m1\u001b[39m]\n\u001b[32m     67\u001b[39m batch_shape = (\n\u001b[32m     68\u001b[39m     \u001b[38;5;28mself\u001b[39m._param.size()[:-\u001b[32m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._param.ndimension() > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch.Size()\n\u001b[32m     69\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\RL_Project\\Lib\\site-packages\\torch\\distributions\\distribution.py:66\u001b[39m, in \u001b[36mDistribution.__init__\u001b[39m\u001b[34m(self, batch_shape, event_shape, validate_args)\u001b[39m\n\u001b[32m     64\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# skip checking lazily-constructed args\u001b[39;00m\n\u001b[32m     65\u001b[39m value = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, param)\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m valid = \u001b[43mconstraint\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid.all():\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     69\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     70\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value.shape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     73\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     74\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\RL_Project\\Lib\\site-packages\\torch\\distributions\\constraints.py:440\u001b[39m, in \u001b[36m_Simplex.check\u001b[39m\u001b[34m(self, value)\u001b[39m\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcheck\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.all(value >= \u001b[32m0\u001b[39m, dim=-\u001b[32m1\u001b[39m) & (\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m < \u001b[32m1e-6\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- env ---\n",
    "from envs import MultiAgentMazeEnv\n",
    "\n",
    "# --- policies ---\n",
    "from policies import SoftmaxPolicy\n",
    "from utils.init_policy import initialize_policy_with_manual_probs   # ðŸ‘ˆ REQUIRED\n",
    "\n",
    "# --- training ---\n",
    "from training import reinforce_multi_rwd2go_alt_barrier\n",
    "\n",
    "H0, W0 = 5, 5\n",
    "A = 4\n",
    "eps = 1e-3\n",
    "pad = 2\n",
    "# new padded size\n",
    "H = H0 + 2 * pad\n",
    "W = W0 + 2 * pad\n",
    "\n",
    "# Define your action probabilities manually for each cell\n",
    "probs = np.zeros((H0, W0, A))\n",
    "probs[0, 0] = [0.0, 1/2, 1/2, 0.0]   # up, right, down, left\n",
    "probs[0, 1] = [0.0, 1/3, 1/3, 1/3]   # up, right, down, left\n",
    "probs[0, 2] = [0.0, 0.0, 1/2, 1/2]   # up, right, down, left\n",
    "probs[0, 3] = [1/4, 1/4, 1/4, 1/4]   # up, right, down, left\n",
    "probs[0, 4] = [0.0, 0.0, 1.0, 0.0]   # up, right, down, left\n",
    "\n",
    "probs[1, 0] = [1/3, 1/3, 1/3, 0.0]   # up, right, down, left\n",
    "probs[1, 1] = [1/4, 1/4, 1/4, 1/4]   # up, right, down, left\n",
    "probs[1, 2] = [1/3, 0.0, 1/3, 1/3]    # up, right, down, left\n",
    "probs[1, 3] = [1/4, 1/4, 1/4, 1/4]   # up, right, down, left\n",
    "probs[1, 4] = [1/4, 1/4, 1/4, 1/4]   # up, right, down, left\n",
    "\n",
    "probs[2, 0] = [1/3, 1/3, 1/3, 0.0]   # up, right, down, left\n",
    "probs[2, 1] = [1/4, 1/4, 1/4, 1/4]   # up, right, down, left\n",
    "probs[2, 2] = [1/3, 0.0, 1/3, 1/3]    # up, right, down, left\n",
    "probs[2, 3] = [1/4, 1/4, 1/4, 1/4]   # up, right, down, left\n",
    "probs[2, 4] = [1/2, 0.0, 1/2, 0.0]   # up, right, down, left\n",
    "\n",
    "probs[3, 0] = [1/3, 1/3, 1/3, 0.0]   # up, right, down, left\n",
    "probs[3, 1] = [1/4, 1/4, 1/4, 1/4]   # up, right, down, left\n",
    "probs[3, 2] = [1/3, 0.0, 1/3, 1/3]    # up, right, down, left\n",
    "probs[3, 3] = [1/4, 1/4, 1/4, 1/4]   # up, right, down, left\n",
    "probs[3, 4] = [1/2, 0.0, 1/2, 0.0]   # up, right, down, left\n",
    "\n",
    "probs[4, 0] = [1/2, 1/2, 0.0, 0.0]   # up, right, down, left\n",
    "probs[4, 1] = [1/3, 1/3, 0.0, 1/3]   # up, right, down, left\n",
    "probs[4, 2] = [1/3, 1/3, 0.0, 1/3]    # up, right, down, left\n",
    "probs[4, 3] = [0.0, 1/2, 0.0, 1/2]   # up, right, down, left\n",
    "probs[4, 4] = [1/2, 0.0, 0.0, 1/2]   # up, right, down, left\n",
    "\n",
    "# initialize padded grid with uniform wall policy\n",
    "probs_padded = np.ones((H, W, A)) / A\n",
    "\n",
    "# copy original policy into the center\n",
    "probs_padded[pad:pad+H0, pad:pad+W0, :] = probs\n",
    "\n",
    "# numerical safety (optional but consistent)\n",
    "probs_padded = np.maximum(probs_padded, eps)\n",
    "probs_padded = probs_padded / probs_padded.sum(axis=2, keepdims=True)\n",
    "\n",
    "outer_walls = []\n",
    "for i in range(H):\n",
    "    for j in range(W):\n",
    "        if i < pad or i >= H - pad or j < pad or j >= W - pad:\n",
    "            outer_walls.append((i, j)) \n",
    "\n",
    "inner_walls = [(0,3), (1,3), (2,3), (3,3)]\n",
    "# shift internal walls into padded grid\n",
    "inner_walls = [(x + pad, y + pad) for (x, y) in inner_walls]\n",
    "\n",
    "\n",
    "# Initialize env with 2 agents\n",
    "env = MultiAgentMazeEnv(\n",
    "        size=(9,9),\n",
    "        starts=[(2,3)],\n",
    "        goals=[(3,6)],\n",
    "        inner_walls=inner_walls,\n",
    "        outer_walls=outer_walls\n",
    "    )\n",
    "env.render()\n",
    "\n",
    "policies = [SoftmaxPolicy(width=W, height=H, num_actions=A) for _ in range(env.n_agents)]\n",
    "optimizers = [torch.optim.Adam(p.parameters(), lr=0.1) for p in policies]\n",
    "\n",
    "initialize_policy_with_manual_probs(policies[0], probs_padded)\n",
    "scores, barrier, violation = reinforce_multi_rwd2go_alt_barrier(env, policies, optimizers)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "for i in range(len(barrier)):\n",
    "    ax1.plot(barrier[i], label=f\"Agent {i} Penalty\", linestyle='-')\n",
    "    ax2.plot(violation[i], label=f\"Agent {i} Violations\", linestyle='--', color='red')\n",
    "\n",
    "ax1.set_xlabel(\"Episode\")\n",
    "ax1.set_ylabel(\"Mean Barrier Penalty\", color='tab:blue')\n",
    "ax2.set_ylabel(\"Violation Number\", color='tab:red')\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "plt.title(\"Barrier Penalty and Constraint Violations Over Training\")\n",
    "plt.show()\n",
    "\n",
    "# --- Plot Expected Reward (Value Function) ---\n",
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "for i in range(len(V)):\n",
    "    plt.plot(V[i], label=f\"Agent {i} Value\", linewidth=2)\n",
    "\n",
    "plt.xlabel(\"Batch Number\")\n",
    "plt.ylabel(\"Estimated Expected Return V(s)\")\n",
    "plt.title(\"Expected Return (Value Function) Over Training\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c19be2-5482-4b3b-a34a-68c31c964796",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_Project",
   "language": "python",
   "name": "rl_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
