{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "881e4128-45dd-49aa-95d9-e2058d203882",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T17:55:03.612739Z",
     "iopub.status.busy": "2025-12-14T17:55:03.612739Z",
     "iopub.status.idle": "2025-12-14T17:55:07.292211Z",
     "shell.execute_reply": "2025-12-14T17:55:07.292211Z",
     "shell.execute_reply.started": "2025-12-14T17:55:03.612739Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from collections import deque\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb4047f3-35ed-4d38-9f14-973eefb5f334",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T17:55:07.294212Z",
     "iopub.status.busy": "2025-12-14T17:55:07.294212Z",
     "iopub.status.idle": "2025-12-14T17:55:07.322317Z",
     "shell.execute_reply": "2025-12-14T17:55:07.322317Z",
     "shell.execute_reply.started": "2025-12-14T17:55:07.294212Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultiAgentMazeEnv:\n",
    "    def __init__(self, size=(5, 5), starts=[(0, 0), (0, 4)], goals=[(5, 5), (5, 5)], inner_walls=None, outer_walls=None):\n",
    "        self.size = size\n",
    "        self.starts = starts\n",
    "        self.goals = goals\n",
    "        self.inner_walls = inner_walls if inner_walls else []\n",
    "        self.outer_walls = outer_walls if outer_walls else []\n",
    "        self.n_agents = len(starts)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.agent_positions = [list(start) for start in self.starts]\n",
    "        # Track last positions to detect \"staying still\"\n",
    "        self.last_positions = [list(start) for start in self.starts]\n",
    "        return [tuple(pos) for pos in self.agent_positions]\n",
    "\n",
    "    def step(self, actions, weights=None):\n",
    "        \"\"\"\n",
    "        actions: list of integers (0=up, 1=down, 2=left, 3=right)\n",
    "        returns: new positions, rewards, dones\n",
    "        \"\"\"\n",
    "        moves = {\n",
    "            0: (-1, 0),   # UP\n",
    "            1: (0, 1),    # RIGHT\n",
    "            2: (1, 0),    # DOWN\n",
    "            3: (0, -1),   # LEFT\n",
    "        }\n",
    "\n",
    "        n = len(actions)\n",
    "        order = list(range(n))  # default order\n",
    "        \n",
    "        if weights is None:\n",
    "            weights = np.ones(n, dtype=float)  # equal weights\n",
    "        else:\n",
    "            weights = np.array(weights, dtype=float)\n",
    "            \n",
    "    \n",
    "        if weights is not None:\n",
    "            # --- Weighted random order (no replacement) ---\n",
    "            order = []\n",
    "            remaining_agents = list(range(n))\n",
    "            remaining_weights = np.array(weights, dtype=float).copy()\n",
    "    \n",
    "            while remaining_agents:\n",
    "                probs = remaining_weights / remaining_weights.sum()\n",
    "                idx = np.random.choice(len(remaining_agents), p=probs)\n",
    "                order.append(remaining_agents.pop(idx))\n",
    "                remaining_weights = np.delete(remaining_weights, idx)\n",
    "\n",
    "        new_positions = []\n",
    "        next_pos = []\n",
    "        for i in range(0,n): \n",
    "            new_positions.append(tuple(map(int,[-1,-1])))\n",
    "            next_pos.append(tuple(map(int,[-1,-1])))\n",
    "\n",
    "        agent_pos_temp = np.array(self.agent_positions, dtype=float).copy()\n",
    "        for i in order:   # process agents in weighted-random order\n",
    "            action = actions[i]\n",
    "            move = moves[action]\n",
    "            next_pos[i] = tuple(map(int,[agent_pos_temp[i][0] + move[0],\n",
    "                       agent_pos_temp[i][1] + move[1]]))\n",
    "\n",
    "            if next_pos[i] not in new_positions:\n",
    "                new_positions[i] = next_pos[i]\n",
    "            elif next_pos[i] in new_positions:\n",
    "                new_positions[i] = tuple(map(int,self.agent_positions[i]))\n",
    "        \n",
    "        self.last_positions = [tuple(p) for p in self.agent_positions]  # remember old positions\n",
    "        self.agent_positions = new_positions\n",
    "        \n",
    "        rewards = np.zeros(n, dtype=float)\n",
    "        dones   = np.zeros(n, dtype=bool)  \n",
    "        # who reached their goal this step?\n",
    "        reached = [i for i in range(n) if tuple(new_positions[i]) == self.goals[i]]\n",
    "\n",
    "        \n",
    "        # --- New Blocking Logic ---\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                if i != j:\n",
    "                    # Agent i stayed still\n",
    "                    if tuple(self.agent_positions[i]) == tuple(self.last_positions[i]):\n",
    "                        # If opponent moved into that position -> blocking success\n",
    "                        if next_pos[j] == tuple(self.agent_positions[i]):\n",
    "                            rewards[i] -= 0.5   # small blocking reward\n",
    "                            rewards[j] += 0.5   # symmetric penalty\n",
    "                            dones[:] = True \n",
    "                            return new_positions, rewards, dones\n",
    "                            \n",
    "        \n",
    "        if len(reached) == 1:\n",
    "            w = reached[0]\n",
    "            rewards[w] = 1\n",
    "            for j in range(n):\n",
    "                if j != w:\n",
    "                    rewards[j] = -1\n",
    "            dones[:] = True  # end episode after a win\n",
    "        elif len(reached) >= 2:\n",
    "            # simultaneous arrival: tie → zeros, end episode\n",
    "            rewards[:] = 0.0\n",
    "            dones[:] = True\n",
    "        else:\n",
    "            # no one reached: keep going\n",
    "            dones[:] = False\n",
    "        \n",
    "        return new_positions, rewards, dones\n",
    "\n",
    "    def render(self):\n",
    "        grid = np.full(self.size, \" \", dtype=object)\n",
    "    \n",
    "        # Draw walls\n",
    "        for w in self.inner_walls:\n",
    "            grid[w] = \"#\"\n",
    "        for w in self.outer_walls:\n",
    "            grid[w] = \"#\"\n",
    "    \n",
    "        # Draw numbered goals (colored numbers)\n",
    "        colors = [\"\\033[92m\", \"\\033[94m\", \"\\033[91m\", \"\\033[93m\"]  # green, blue, red, yellow\n",
    "        for i, g in enumerate(self.goals):\n",
    "            color = colors[i % len(colors)]\n",
    "            grid[g] = f\"{color}{i}\\033[0m\"\n",
    "    \n",
    "        # Draw numbered agents\n",
    "        for i, pos in enumerate(self.agent_positions):\n",
    "            grid[tuple(pos)] = f\"{i}\"\n",
    "    \n",
    "        # Print\n",
    "        for row in grid:\n",
    "            print(\" \".join(row))\n",
    "        print(\"----------\")\n",
    "\n",
    "\n",
    "def random_maze_env(size=(5,5), n_agents=1, n_walls=6, seed=None):\n",
    "    \"\"\"\n",
    "    Build a random MultiAgentMazeEnv with given size, agents, and wall count.\n",
    "    \n",
    "    Args:\n",
    "        size (tuple): grid size (rows, cols)\n",
    "        n_agents (int): number of agents\n",
    "        n_walls (int): number of wall cells\n",
    "        seed (int or None): random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        env: a MultiAgentMazeEnv instance\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    else:\n",
    "        seed = random.randint(0, int(1e9))\n",
    "        random.seed(seed)\n",
    "        print(\"Generated seed:\", seed)\n",
    "\n",
    "    rows, cols = size\n",
    "\n",
    "    # --- choose starts and goals ---\n",
    "    all_cells = [(r, c) for r in range(rows) for c in range(cols)]\n",
    "    starts = random.sample(all_cells, n_agents)\n",
    "    remaining = [c for c in all_cells if c not in starts]\n",
    "    goals = random.sample(remaining, n_agents)\n",
    "\n",
    "    # --- choose walls (avoid starts + goals) ---\n",
    "    forbidden = set(starts + goals)\n",
    "    candidates = [c for c in all_cells if c not in forbidden]\n",
    "    walls = random.sample(candidates, min(n_walls, len(candidates)))\n",
    "\n",
    "    # --- build env ---\n",
    "    env = MultiAgentMazeEnv(\n",
    "        size=size,\n",
    "        starts=starts,\n",
    "        goals=goals,\n",
    "        walls=walls\n",
    "    )\n",
    "    return env\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d40c230b-d7e9-48f2-bd87-5e03c72b2e26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T17:56:33.910581Z",
     "iopub.status.busy": "2025-12-14T17:56:33.909585Z",
     "iopub.status.idle": "2025-12-14T17:56:33.917603Z",
     "shell.execute_reply": "2025-12-14T17:56:33.917603Z",
     "shell.execute_reply.started": "2025-12-14T17:56:33.910581Z"
    }
   },
   "outputs": [],
   "source": [
    "def solution(env, policies):\n",
    "    \"\"\"\n",
    "    Combined solution that works with both LinearPolicy and neural network policies\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- Run one greedy rollout ---\n",
    "    dones = [False] * env.n_agents\n",
    "    states = env.reset()\n",
    "    env.render()   \n",
    "    max_steps = 10 \n",
    "    for step in range(max_steps):\n",
    "        actions = []\n",
    "        for i in range(env.n_agents):\n",
    "            # Detect policy type and get greedy action accordingly\n",
    "            if hasattr(policies[i], '_phi'):\n",
    "                # LinearPolicy case - use _phi method\n",
    "                phi = policies[i]._phi(states[i])\n",
    "                logits = phi @ policies[i].W\n",
    "                probs = torch.softmax(logits, dim=0)\n",
    "                action = torch.argmax(probs, dim=0).item()\n",
    "                print(\"Agent:\", i)\n",
    "                print(\"State: \", states[i])\n",
    "                print(\"Prob: \", probs)\n",
    "            else:\n",
    "                # Neural network policy case - call directly\n",
    "                state_tensor = torch.tensor(states[i], dtype=torch.float32).unsqueeze(0)\n",
    "                probs = policies[i](state_tensor)\n",
    "                action = torch.argmax(probs, dim=1).item()\n",
    "                #print(\"Agent:\", i)\n",
    "                print(\"State: \", state_tensor)\n",
    "                print(\"Prob: \", probs)\n",
    "            \n",
    "            actions.append(action)\n",
    "\n",
    "        next_states, step_rewards, dones = env.step(actions)\n",
    "        states = next_states\n",
    "        env.render()\n",
    "        if any(dones):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec475482-1531-4481-ac4f-0f0edbf8ccd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T17:56:35.571127Z",
     "iopub.status.busy": "2025-12-14T17:56:35.571127Z",
     "iopub.status.idle": "2025-12-14T17:56:35.590205Z",
     "shell.execute_reply": "2025-12-14T17:56:35.590205Z",
     "shell.execute_reply.started": "2025-12-14T17:56:35.571127Z"
    }
   },
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module): # definie the policy network\n",
    "    def __init__(self, state_size=2, action_size=4, hidden_size=32):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, action_size)\n",
    "        \n",
    "        # force initialization for equal action probs\n",
    "        nn.init.zeros_(self.fc2.weight)\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1) # we just consider 1 dimensional probability of action\n",
    "\n",
    "    def act(self, state):\n",
    "        state = np.array(state, dtype=np.float32)\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(\"cpu\")\n",
    "        probs = self.forward(state).cpu()\n",
    "        model = Categorical(probs)\n",
    "        action = model.sample()\n",
    "        return action.item(), model.log_prob(action), model.entropy()\n",
    "\n",
    "class LinearPolicy:\n",
    "    def __init__(self, state_size=2, action_size=4, maze_size=None):\n",
    "        self.action_size = action_size\n",
    "        self.maze_size = np.array(maze_size if maze_size is not None else [1,1], dtype=np.float32)\n",
    "        self.W = torch.zeros(state_size + 1, action_size, requires_grad=True)\n",
    "        \n",
    "\n",
    "    def _phi(self, state):\n",
    "        s = np.array(state, dtype=np.float32) / self.maze_size\n",
    "        phi = np.append(s, 1.0)\n",
    "        return torch.from_numpy(phi).float()\n",
    "\n",
    "    def act(self, state):\n",
    "        phi = self._phi(state)\n",
    "        logits = phi @ self.W\n",
    "        probs = torch.softmax(logits, dim=0)\n",
    "        dist  = Categorical(probs)\n",
    "        a     = dist.sample()\n",
    "        return a.item(), dist.log_prob(a), dist.entropy()\n",
    "        #return a.item(), dist.log_prob(a)\n",
    "    def parameters(self):\n",
    "        return [self.W]\n",
    "\n",
    "\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, state_size=2, action_size=4, hidden_sizes=(64, 64)):\n",
    "        super(PolicyNet, self).__init__()\n",
    "\n",
    "        # Two hidden layers with normalization and nonlinearity\n",
    "        self.fc1 = nn.Linear(state_size, hidden_sizes[0])\n",
    "        self.fc2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
    "        self.fc_out = nn.Linear(hidden_sizes[1], action_size)\n",
    "\n",
    "        # Optional normalization for stability\n",
    "        self.ln1 = nn.LayerNorm(hidden_sizes[0])\n",
    "        self.ln2 = nn.LayerNorm(hidden_sizes[1])\n",
    "\n",
    "        # Initialization\n",
    "        nn.init.kaiming_uniform_(self.fc1.weight, nonlinearity='relu')\n",
    "        nn.init.kaiming_uniform_(self.fc2.weight, nonlinearity='relu')\n",
    "        nn.init.zeros_(self.fc_out.bias)\n",
    "        print(\"New\")\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Forward pass returning action probabilities.\n",
    "        \"\"\"\n",
    "        x = F.relu(self.ln1(self.fc1(state)))\n",
    "        x = F.relu(self.ln2(self.fc2(x)))\n",
    "        logits = self.fc_out(x)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        return probs\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Same as your original version: sample from categorical distribution.\n",
    "        \"\"\"\n",
    "        state = np.array(state, dtype=np.float32)\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(\"cpu\")\n",
    "        probs = self.forward(state).cpu()\n",
    "        model = Categorical(probs)\n",
    "        action = model.sample()\n",
    "        return action.item(), model.log_prob(action), model.entropy()\n",
    "\n",
    "\n",
    "class SoftmaxPolicy(nn.Module):\n",
    "    def __init__(self, width, height, num_actions=4):\n",
    "        super().__init__()\n",
    "        self.width  = width\n",
    "        self.height = height\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        # A logit vector per state (H × W × A)\n",
    "        # This is the analogue of policy.W in the paper\n",
    "        self.logits = nn.Parameter(torch.zeros(height, width, num_actions))\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        state: tensor of shape (2,) or (B,2) with (y,x) positions\n",
    "        returns: probs of shape (num_actions,) or (B,num_actions)\n",
    "        \"\"\"\n",
    "        state_t = state\n",
    "        if state_t.dim() == 1:\n",
    "            state_t = state_t.unsqueeze(0)  # -> (1,2)\n",
    "\n",
    "        ys = state_t[:, 0].long()\n",
    "        xs = state_t[:, 1].long()\n",
    "\n",
    "        # (B, A)\n",
    "        logits = self.logits[ys, xs]\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        return probs\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        state = (y, x) grid position\n",
    "        returns (action, log_prob, entropy)\n",
    "        \"\"\"\n",
    "        y, x = state\n",
    "        if y < 0 or y >= self.height or x < 0 or x >= self.width:\n",
    "            # return a random safe action\n",
    "            dummy_probs = torch.ones(self.num_actions) / self.num_actions\n",
    "            dist = Categorical(dummy_probs)\n",
    "            a = dist.sample()\n",
    "            return a.item(), dist.log_prob(a), dist.entropy()\n",
    "        else :\n",
    "            logits = self.logits[y, x]                    # shape [num_actions]\n",
    "            probs  = torch.softmax(logits, dim=0)\n",
    "            dist   = Categorical(probs)\n",
    "            a      = dist.sample()\n",
    "            return a.item(), dist.log_prob(a), dist.entropy()\n",
    "\n",
    "    def greedy_act(self, state):\n",
    "        \"\"\"Returns the argmax action without sampling.\"\"\"\n",
    "        y, x = state\n",
    "        logits = self.logits[y, x]\n",
    "        return torch.argmax(logits).item()\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.logits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eaf102d0-0cd3-4b7b-8603-f3f203463eae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T17:56:35.800330Z",
     "iopub.status.busy": "2025-12-14T17:56:35.800330Z",
     "iopub.status.idle": "2025-12-14T17:56:35.820883Z",
     "shell.execute_reply": "2025-12-14T17:56:35.820883Z",
     "shell.execute_reply.started": "2025-12-14T17:56:35.800330Z"
    }
   },
   "outputs": [],
   "source": [
    "def log_barrier_penalty(state, env, violation_count, eta=0.05, r_safe=0):\n",
    "    \"\"\"\n",
    "    Log-barrier penalty for violating safety constraints.\n",
    "    Returns a scalar torch.tensor.\n",
    "    \n",
    "    Args:\n",
    "        state: (x, y) tensor or tuple\n",
    "        env: MultiAgentMazeEnv\n",
    "        eta: barrier coefficient\n",
    "        r_safe: safety radius around each wall / border\n",
    "    \"\"\"\n",
    "    state_t = torch.tensor(state, dtype=torch.float32)\n",
    "    x, y = state_t[0], state_t[1]\n",
    "    cons = []\n",
    "\n",
    "    # --- outer boundaries ---\n",
    "    cons.append(x - (env.size[0] - r_safe))  # right wall\n",
    "    cons.append(-x + r_safe - 1)                     # left wall\n",
    "    cons.append(y - (env.size[1] - r_safe))  # top wall\n",
    "    cons.append(-y + r_safe - 1)                     # bottom wall\n",
    "\n",
    "    # --- internal walls ---\n",
    "    for (wx, wy) in env.walls:\n",
    "        dist_sq = (x - wx)**2 + (y - wy)**2\n",
    "        f = r_safe- dist_sq\n",
    "        cons.append(f)\n",
    "\n",
    "    penalty = 0.0\n",
    "    for c in cons:\n",
    "        if c >= 0:           # constraint violated or on the edge\n",
    "            violation_count += 1\n",
    "            penalty += 1   # large penalty (prevent instability)\n",
    "        else:\n",
    "            #penalty += -eta * torch.log(-torch.tensor(c + 1e-8))  # log barrier\n",
    "            penalty = penalty - eta * torch.log(-c + 1e-8)\n",
    "\n",
    "    return penalty, violation_count\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def log_barrier_penalty(state, env, eta=0.05, r_safe=0):\n",
    "    \"\"\"\n",
    "    Log-barrier penalty for violating safety constraints.\n",
    "    Returns a scalar torch.tensor.\n",
    "    \n",
    "    Args:\n",
    "        state: (x, y) tensor or tuple\n",
    "        env: MultiAgentMazeEnv\n",
    "        eta: barrier coefficient\n",
    "        r_safe: safety radius around each wall / border\n",
    "    \"\"\"\n",
    "    state_t = torch.tensor(state, dtype=torch.float32)\n",
    "    x, y = state_t[0], state_t[1]\n",
    "    cons = []\n",
    "    violated = False\n",
    "    if x - (env.size[0] - r_safe) >= 0:\n",
    "        cons.append(torch.tensor(1e-10))\n",
    "        violated = True\n",
    "    else:\n",
    "        cons.append(torch.tensor(1.0))\n",
    "        \n",
    "    if -x + r_safe - 1 >= 0:\n",
    "        cons.append(torch.tensor(1e-10))\n",
    "        violated = True\n",
    "    else:\n",
    "        cons.append(torch.tensor(1.0))\n",
    "\n",
    "    if y - (env.size[1] - r_safe) >= 0:\n",
    "        cons.append(torch.tensor(1e-10))\n",
    "        violated = True\n",
    "    else:\n",
    "        cons.append(torch.tensor(1.0))\n",
    "\n",
    "    if -y + r_safe - 1 >= 0:\n",
    "        cons.append(torch.tensor(1e-10))\n",
    "        violated = True\n",
    "    else:\n",
    "        cons.append(torch.tensor(1.0))\n",
    "\n",
    "    for (wx, wy) in env.walls:\n",
    "        if (x - wx)**2 + (y - wy)**2 == 0:\n",
    "            cons.append(torch.tensor(1e-10))\n",
    "            violated = True\n",
    "        else:\n",
    "            cons.append(torch.tensor(1.0))\n",
    "\n",
    "    # ---- 3) Log-barrier aggregation ----\n",
    "    penalty = 0.0\n",
    "    for c in cons:\n",
    "        # log barrier function:   -eta * log(c)\n",
    "        penalty += -eta * torch.log(c)\n",
    "\n",
    "    return penalty, violated\n",
    "\n",
    "\n",
    "def log_barrier_penalty(state, cost, env, r_safe=0, eps=1e-6, pad = 2):\n",
    "    \"\"\"\n",
    "    Log-barrier penalty for safety constraints.\n",
    "\n",
    "    Supports:\n",
    "      - state shape (2,)\n",
    "      - state shape (N, 2)\n",
    "      - state as list/tuple of (x, y)\n",
    "\n",
    "    Args:\n",
    "        state: (x,y) or (N,2)\n",
    "        cost: torch tensor of shape (n_constraints,)\n",
    "        env: environment\n",
    "        r_safe: safety radius\n",
    "    \"\"\"\n",
    "    state_t = torch.as_tensor(state, dtype=torch.float32)\n",
    "\n",
    "    # Ensure shape (N,2)\n",
    "    if state_t.ndim == 1:\n",
    "        state_t = state_t.unsqueeze(0)\n",
    "        \n",
    "    N = state_t.shape[0]   # number of (x,y) tuples\n",
    "    x = state_t[:, 0]\n",
    "    y = state_t[:, 1]\n",
    "\n",
    "    violated = False\n",
    "\n",
    "    # ----- borders -----\n",
    "    # right wall\n",
    "    mask = x >= (env.size[0] - r_safe - pad)\n",
    "    cost[0] += torch.where(mask, 10.0, 1e-4).sum()\n",
    "    violated |= mask.any().item()\n",
    "\n",
    "    # left wall\n",
    "    mask = -x + r_safe - 1 + pad >= 0\n",
    "    cost[1] += torch.where(mask, 10.0, 1e-4).sum()\n",
    "    violated |= mask.any().item()\n",
    "\n",
    "    # top wall\n",
    "    mask = y >= (env.size[1] - r_safe - pad)\n",
    "    cost[2] += torch.where(mask, 10.0, 1e-4).sum()\n",
    "    violated |= mask.any().item()\n",
    "\n",
    "    # bottom wall\n",
    "    mask = -y + r_safe - 1 + pad >= 0\n",
    "    cost[3] += torch.where(mask, 10.0, 1e-4).sum()\n",
    "    violated |= mask.any().item()\n",
    "\n",
    "    # ----- internal walls -----\n",
    "    for i, (wx, wy) in enumerate(env.inner_walls):\n",
    "        d2 = (x - wx)**2 + (y - wy)**2\n",
    "        mask = d2 == eps\n",
    "        cost[4 + i] += torch.where(mask, 10.0, 1e-4).sum()\n",
    "        violated |= mask.any().item()\n",
    "\n",
    "    # average over number of states\n",
    "    cost = cost / N\n",
    "\n",
    "    return cost, violated\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c25e4dea-1a82-433d-9417-223ac835e76e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T17:56:36.003126Z",
     "iopub.status.busy": "2025-12-14T17:56:36.003126Z",
     "iopub.status.idle": "2025-12-14T17:56:36.015365Z",
     "shell.execute_reply": "2025-12-14T17:56:36.014369Z",
     "shell.execute_reply.started": "2025-12-14T17:56:36.003126Z"
    }
   },
   "outputs": [],
   "source": [
    "def collect_batch_trajectories(env, policies, phase, batch_size, max_t):\n",
    "    batch_data = []\n",
    "    batch_violation_count = 0\n",
    "    \n",
    "    for _ in range(batch_size):\n",
    "        states = env.reset()\n",
    "        saved_log_probs = [[] for _ in range(env.n_agents)]\n",
    "        rewards = [[] for _ in range(env.n_agents)]\n",
    "        entropies = [[] for _ in range(env.n_agents)]\n",
    "\n",
    "        C_tau = torch.zeros(8, dtype=torch.float32)\n",
    "        episode_has_violation = False\n",
    "\n",
    "        for t in range(max_t + 1):\n",
    "            actions = []\n",
    "            for i in range(env.n_agents):\n",
    "                a, lp, ent = policies[i].act(states[i])\n",
    "                actions.append(a)\n",
    "                saved_log_probs[i].append(lp)\n",
    "                entropies[i].append(ent)\n",
    "\n",
    "            next_states, step_rewards, dones = env.step(actions)\n",
    "            C_tau, violated = log_barrier_penalty(next_states, C_tau, env)\n",
    "            episode_has_violation |= violated\n",
    "\n",
    "            for i in range(env.n_agents):\n",
    "                rewards[i].append(step_rewards[i])\n",
    "\n",
    "            states = next_states\n",
    "            if any(dones):\n",
    "                break\n",
    "\n",
    "        # increment ONCE per episode\n",
    "        if episode_has_violation:\n",
    "            batch_violation_count += 1\n",
    "\n",
    "        batch_data.append({\n",
    "            \"log_probs\": saved_log_probs[phase],\n",
    "            \"rewards\": rewards[phase],\n",
    "            \"entropies\": entropies[phase],\n",
    "            \"C_tau\": C_tau,\n",
    "            \"n_violation\": batch_violation_count,\n",
    "        })\n",
    "\n",
    "    return batch_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "382e6a13-8529-4bda-b07a-acdff2281a55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T17:56:36.510029Z",
     "iopub.status.busy": "2025-12-14T17:56:36.508999Z",
     "iopub.status.idle": "2025-12-14T17:56:36.515030Z",
     "shell.execute_reply": "2025-12-14T17:56:36.515030Z",
     "shell.execute_reply.started": "2025-12-14T17:56:36.510029Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_constraint_value_estimates(C_tau, V_i_estimate, alpha):\n",
    "    for i in range(len(C_tau)):\n",
    "        Ci = C_tau[i].detach()\n",
    "        if V_i_estimate[i] is None:\n",
    "            V_i_estimate[i] = Ci\n",
    "        else:\n",
    "            V_i_estimate[i] = (1 - alpha) * V_i_estimate[i] + alpha * Ci\n",
    "    return V_i_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94b1f063-8c58-41ab-81b2-e6a82feaa5ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T17:56:36.724087Z",
     "iopub.status.busy": "2025-12-14T17:56:36.724087Z",
     "iopub.status.idle": "2025-12-14T17:56:36.728561Z",
     "shell.execute_reply": "2025-12-14T17:56:36.728561Z",
     "shell.execute_reply.started": "2025-12-14T17:56:36.724087Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_constraint_value_estimates_batch(C_tau_batch, V_i_estimate, alpha):\n",
    "    \"\"\"\n",
    "    C_tau_batch: tensor of shape (batch_size, m)\n",
    "    \"\"\"\n",
    "    batch_mean = C_tau_batch.mean(dim=0).detach()\n",
    "\n",
    "    for i in range(len(batch_mean)):\n",
    "        if V_i_estimate[i] is None:\n",
    "            V_i_estimate[i] = batch_mean[i]\n",
    "        else:\n",
    "            V_i_estimate[i] = (1 - alpha) * V_i_estimate[i] + alpha * batch_mean[i]\n",
    "\n",
    "    return V_i_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c82ec04-8285-4340-9ca0-c8d05fa2b962",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T17:56:36.961500Z",
     "iopub.status.busy": "2025-12-14T17:56:36.961500Z",
     "iopub.status.idle": "2025-12-14T17:56:36.965787Z",
     "shell.execute_reply": "2025-12-14T17:56:36.964781Z",
     "shell.execute_reply.started": "2025-12-14T17:56:36.961500Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_barrier_weight(C_tau, V_i_estimate, lambda_maze):\n",
    "    bw = 0.0\n",
    "    for i in range(len(C_tau)):\n",
    "        bw += lambda_maze * (C_tau[i] / V_i_estimate[i])\n",
    "    return bw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f42515d9-1013-4f54-80b1-554038bef0a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T17:56:37.302465Z",
     "iopub.status.busy": "2025-12-14T17:56:37.302465Z",
     "iopub.status.idle": "2025-12-14T17:56:37.309680Z",
     "shell.execute_reply": "2025-12-14T17:56:37.309100Z",
     "shell.execute_reply.started": "2025-12-14T17:56:37.302465Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_policy_loss(log_probs, rewards, gamma, barrier_weight):\n",
    "    discounts = [gamma**k for k in range(len(rewards) + 1)]\n",
    "    G = torch.tensor([\n",
    "        sum(discounts[j] * rewards[j+t] for j in range(len(rewards) - t))\n",
    "        for t in range(len(rewards))\n",
    "    ], dtype=torch.float32)\n",
    "\n",
    "    loss_terms = []\n",
    "    for lp, Gt in zip(log_probs, G):\n",
    "        loss_terms.append(-lp * (Gt - barrier_weight))\n",
    "\n",
    "    return torch.stack(loss_terms).sum(), G.mean()\n",
    "\n",
    "def apply_policy_update(optimizer, loss):\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5946f29-9dfd-4e53-999b-c8e6f719b390",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T17:56:37.531048Z",
     "iopub.status.busy": "2025-12-14T17:56:37.531048Z",
     "iopub.status.idle": "2025-12-14T17:56:37.537757Z",
     "shell.execute_reply": "2025-12-14T17:56:37.536744Z",
     "shell.execute_reply.started": "2025-12-14T17:56:37.531048Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_lb_sgd_convergence(policies, phase, grad_log, below_counter, delta=0.1, K=5):\n",
    "    grad_sq = 0.0\n",
    "    for p in policies[phase].parameters():\n",
    "        if p.grad is not None:\n",
    "            grad_sq += p.grad.norm(2).item() ** 2\n",
    "\n",
    "    grad_norm = grad_sq ** 0.5\n",
    "    grad_log[phase].append(grad_norm)\n",
    "\n",
    "    threshold = delta / 2\n",
    "    if grad_norm < threshold:\n",
    "        below_counter[phase] += 1\n",
    "    else:\n",
    "        below_counter[phase] = 0\n",
    "\n",
    "    converged = below_counter[phase] >= K\n",
    "    return grad_norm, converged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17dea45d-7dee-4fb5-8d95-28acc6272979",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T17:56:37.882492Z",
     "iopub.status.busy": "2025-12-14T17:56:37.882492Z",
     "iopub.status.idle": "2025-12-14T17:56:37.888086Z",
     "shell.execute_reply": "2025-12-14T17:56:37.888086Z",
     "shell.execute_reply.started": "2025-12-14T17:56:37.882492Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_training_progress(phase, episode_count, avg_rewards, n_violation, avg_barrier, grad_norm, print_every):\n",
    "    if episode_count % print_every != 0:\n",
    "        return\n",
    "\n",
    "    msg  = f\" Agent{phase} Episode {episode_count}\"\n",
    "    msg += f\" | avgR={avg_rewards:.4f}\"\n",
    "    msg += f\" | violations={n_violation}\"\n",
    "    msg += f\" | avgBarrier={avg_barrier:.4f}\"\n",
    "    msg += f\" | gradNorm={grad_norm:.4f}\"\n",
    "    print(msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "57bef6b0-034f-4978-9019-46a3052f62a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:23:39.546908Z",
     "iopub.status.busy": "2025-12-14T18:23:39.545910Z",
     "iopub.status.idle": "2025-12-14T18:23:39.571093Z",
     "shell.execute_reply": "2025-12-14T18:23:39.570061Z",
     "shell.execute_reply.started": "2025-12-14T18:23:39.546908Z"
    }
   },
   "outputs": [],
   "source": [
    "def reinforce_multi_rwd2go_alt_barrier_new(env, policies, optimizers, n_episodes=30000, max_t=20, gamma=0.9, batch_size=256, print_every=1):\n",
    "    \n",
    "    # Add convergence parameters\n",
    "    window = 20\n",
    "    max_rounds = 200\n",
    "    \n",
    "    scores_deque = deque(maxlen=window)\n",
    "    # Track average return (already in scores) and rewards-to-go\n",
    "    episodic_return_log = [[] for _ in range(env.n_agents)]   # average episodic returns per agent\n",
    "    batch_rewards = []\n",
    "    V_estimate_log = [[] for _ in range(env.n_agents)]  # track rewards-to-go estimates\n",
    "    barrier_log = [[] for _ in range(env.n_agents)]\n",
    "    violation_log = [[] for _ in range(env.n_agents)]\n",
    "    grad_log = [[] for _ in range(env.n_agents)]\n",
    "    below_counter = [0,0]\n",
    "\n",
    "    # For WITHIN-ROUND convergence tracking\n",
    "    round_values = [[] for _ in range(env.n_agents)]\n",
    "    round_convergence_threshold = 7.5e-3  # can tune this\n",
    "    \n",
    "    round_count = 0\n",
    "    all_agents_stable = False\n",
    "\n",
    "    # Enhanced parameters for both methods\n",
    "    entropy_coef = [0.7, 0.7]\n",
    "    entropy_decay = [0.99, 0.98]\n",
    "    min_entropy_coef = 0.0\n",
    "    lambda_maze = 10\n",
    "    lambda_decay = 0.99\n",
    "    min_lambda = 0\n",
    "    \n",
    "    while round_count < max_rounds and not all_agents_stable:  \n",
    "        \n",
    "        round_count += 1\n",
    "        entropy_coef[0] = 0.7\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"=== Round {round_count} ===\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Alternate through each agent\n",
    "        for phase in range(env.n_agents):\n",
    "            print(f\"\\n--- Training Agent {phase} ---\")\n",
    "            \n",
    "            episode_count = 0\n",
    "            agent_converged = False  # WITHIN-ROUND convergence flag\n",
    "            V_i_estimate = [None] * 8   # running estimate of E[C_i]\n",
    "            alpha_vi = 0.02             # EMA rate (tuneable)\n",
    "            \n",
    "            while episode_count < n_episodes:\n",
    "                episode_count += 1\n",
    "\n",
    "                # ---- collect batch of trajectories ----\n",
    "                batch_data = collect_batch_trajectories(env, policies, phase, batch_size, max_t)\n",
    "                # ---- update constraint value estimates ONCE per batch ----\n",
    "                C_tau_batch = torch.stack([traj[\"C_tau\"] for traj in batch_data])\n",
    "                V_i_estimate = update_constraint_value_estimates_batch(C_tau_batch, V_i_estimate, alpha_vi)\n",
    "                \n",
    "                batch_loss = 0.0\n",
    "                avg_rtg = 0.0\n",
    "                barrier_penalties = []\n",
    "                \n",
    "                for traj in batch_data:         \n",
    "                    barrier_penalty = compute_barrier_weight(traj[\"C_tau\"], V_i_estimate, lambda_maze)\n",
    "                    loss, mean_G = compute_policy_loss(traj[\"log_probs\"], traj[\"rewards\"], gamma, barrier_penalty)\n",
    "\n",
    "                    batch_rewards.append(sum(traj[\"rewards\"]))\n",
    "                    barrier_penalties.append(barrier_penalty)\n",
    "                    batch_loss += loss\n",
    "                    avg_rtg += mean_G.item()\n",
    "                \n",
    "                batch_loss /= len(batch_data)                       # - entropy_coef[phase] * mean_entropy*0\n",
    "                apply_policy_update(optimizers[phase], batch_loss)\n",
    "\n",
    "                V_estimate_log[phase].append(avg_rtg / batch_size)\n",
    "                barrier_log[phase].append(np.mean(barrier_penalties))\n",
    "                violation_log[phase].append(traj[\"n_violation\"])\n",
    "                mean_entropy = torch.stack([e for traj in batch_data for e in traj[\"entropies\"]]).mean()  # Calculate mean entropy across the entire batch  \n",
    "                \n",
    "                if episode_count % 3 == 0:\n",
    "                    entropy_coef[phase] = max(min_entropy_coef,entropy_coef[phase] * entropy_decay[phase])\n",
    "                    lambda_maze = max(min_lambda,lambda_maze * lambda_decay)\n",
    "                \n",
    "                grad_norm, converged = check_lb_sgd_convergence(policies, phase, grad_log, below_counter)\n",
    "\n",
    "                avg_batch_rewards = np.mean(batch_rewards, axis=0)\n",
    "                scores_deque.append(avg_batch_rewards)\n",
    "                episodic_return_log[phase].append(avg_batch_rewards)\n",
    "                \n",
    "                if converged:\n",
    "                    print(f\"Agent {phase} converged by LB-SGD condition!\")\n",
    "                    break\n",
    "                if phase == 1 and episode_count > 99:\n",
    "                    #temp = check_convergence(V_estimate_log, phase, window, episode_count, min_episodes)\n",
    "                    break\n",
    "\n",
    "                \n",
    "                print_training_progress(phase, episode_count, np.mean(scores_deque, axis=0), traj[\"n_violation\"], np.mean(barrier_penalties), grad_norm, print_every)\n",
    "\n",
    "                if episode_count % 20 == 0:\n",
    "                    solution(env, policies) \n",
    "        # BETWEEN-ROUND convergence check (after all agents complete this round)\n",
    "        print(f\"\\n--- End of Round {round_count}: Computing Round Averages ---\")\n",
    "        solution(env, policies) \n",
    "        break\n",
    "        \n",
    "        for phase in range(env.n_agents):\n",
    "            if len(V_estimate_log[phase]) >= window:\n",
    "                mean_V_round = np.mean(V_estimate_log[phase][-window:])  # last window as representative of this round\n",
    "                round_values[phase].append(mean_V_round)   \n",
    "        \n",
    "        if round_count > 2:\n",
    "            all_stable = True\n",
    "            for phase in range(env.n_agents):\n",
    "                prev_mean = round_values[phase][-2]\n",
    "                curr_mean = round_values[phase][-1]\n",
    "                delta_round = abs(curr_mean - prev_mean)\n",
    "                print(f\"   Agent {phase}: ΔV_round = {delta_round:.6e}\")\n",
    "                \n",
    "                if delta_round < round_convergence_threshold:\n",
    "                    print(f\" Agent {phase} stabilized between Rounds {round_count-1} and {round_count}\")\n",
    "                else:\n",
    "                    all_stable = False\n",
    "                    print(f\" Agent {phase} still changing between rounds (Δ={delta_round:.6e})\")\n",
    "        \n",
    "            if all_stable:\n",
    "                print(f\"\\n All agents' VALUE functions stabilized between rounds — training converged.\")\n",
    "                all_agents_stable = True\n",
    "                break\n",
    "\n",
    "        \n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "    \n",
    "    return episodic_return_log, V_estimate_log, barrier_log, violation_log\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68733fdd-ad2b-4fcd-a2af-371aed798ff0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T00:27:26.237769Z",
     "iopub.status.busy": "2025-12-14T00:27:26.237769Z",
     "iopub.status.idle": "2025-12-14T00:27:26.264880Z",
     "shell.execute_reply": "2025-12-14T00:27:26.264880Z",
     "shell.execute_reply.started": "2025-12-14T00:27:26.237769Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def reinforce_multi_rwd2go_alt_barrier(env, policies, optimizers, n_episodes=30000, max_t=20, gamma=0.9, batch_size=256, print_every=1):\n",
    "    \n",
    "    # Add convergence parameters\n",
    "    window = 20\n",
    "    min_episodes = 2*window + 1        # minimum episodes before checking convergence\n",
    "    max_rounds = 200\n",
    "    \n",
    "    scores_deque = deque(maxlen=window)\n",
    "    # Track average return (already in scores) and rewards-to-go\n",
    "    scores = []   # average episodic returns per agent\n",
    "    values_log = [[] for _ in range(env.n_agents)]  # track rewards-to-go estimates\n",
    "    barrier_log = [[] for _ in range(env.n_agents)]\n",
    "    violation_log = [[] for _ in range(env.n_agents)]\n",
    "    grad_log = [[] for _ in range(env.n_agents)]\n",
    "    below_counter = [0,0]\n",
    "\n",
    "    # For WITHIN-ROUND convergence tracking\n",
    "    round_values = [[] for _ in range(env.n_agents)]\n",
    "    round_convergence_threshold = 7.5e-3  # can tune this\n",
    "    \n",
    "    round_count = 0\n",
    "    all_agents_stable = False\n",
    "\n",
    "    # Enhanced parameters for both methods\n",
    "    entropy_coef = [0.7, 0.7]\n",
    "    entropy_decay = [0.99, 0.98]\n",
    "    min_entropy_coef = 0.0\n",
    "    lambda_maze = 100\n",
    "    lambda_decay = 0.99\n",
    "    min_lambda = 0\n",
    "    \n",
    "    while round_count < max_rounds and not all_agents_stable:  \n",
    "        \n",
    "        round_count += 1\n",
    "        entropy_coef[0] = 0.7\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"=== Round {round_count} ===\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Alternate through each agent\n",
    "        for phase in range(env.n_agents):\n",
    "            print(f\"\\n--- Training Agent {phase} ---\")\n",
    "            \n",
    "            episode_count = 0\n",
    "            agent_converged = False  # WITHIN-ROUND convergence flag\n",
    "            \n",
    "            while episode_count < n_episodes:\n",
    "                episode_count += 1\n",
    "\n",
    "                avg_rtg = 0\n",
    "                # Accumulate batch loss for current agent\n",
    "                batch_loss = 0.0\n",
    "                batch_count = 0\n",
    "                batch_value_logs = defaultdict(list)\n",
    "                batch_rewards = []\n",
    "                batch_entropies = []\n",
    "                barrier_penalties = []  # store all penalty values in this batch\n",
    "                batch_violation_count = 0\n",
    "                \n",
    "                # ---- collect batch of episodes ----\n",
    "                for _ in range(batch_size):\n",
    "                    states = env.reset()\n",
    "            \n",
    "                    saved_log_probs = [[] for _ in range(env.n_agents)]\n",
    "                    rewards = [[] for _ in range(env.n_agents)]\n",
    "                    saved_entropies = [[] for _ in range(env.n_agents)]\n",
    "                    epi_states = [[] for _ in range(env.n_agents)]\n",
    "                    dones = [False] * env.n_agents\n",
    "                    episode_has_violation = False\n",
    "                    \n",
    "                    # episode\n",
    "                    for t in range(max_t+1):\n",
    "                        actions, log_probs, entropies = [], [], []\n",
    "                        for i in range(env.n_agents):\n",
    "                            a, lp, entropy = policies[i].act(states[i])\n",
    "                            actions.append(a)\n",
    "                            log_probs.append(lp)\n",
    "                            epi_states[i].append(states[i])\n",
    "                            \n",
    "                            entropies.append(entropy)\n",
    "            \n",
    "                        next_states, step_rewards, dones = env.step(actions)\n",
    "                        for i in range(env.n_agents):\n",
    "                            saved_log_probs[i].append(log_probs[i])\n",
    "                            rewards[i].append(step_rewards[i])\n",
    "                            saved_entropies[i].append(entropies[i])  # Save per step\n",
    "            \n",
    "                        states = next_states\n",
    "                        if any(dones):\n",
    "                            for i in range(env.n_agents):\n",
    "                                epi_states[i].append(states[i])\n",
    "                            break\n",
    "                        if t == max_t:\n",
    "                            for i in range(env.n_agents):\n",
    "                                epi_states[i].append(states[i])\n",
    "        \n",
    "                    # --- Process current agent's trajectory ---\n",
    "                    if len(rewards[phase]) > 0:\n",
    "                        # rewards-to-go (Monte Carlo return)\n",
    "                        discounts = [gamma**k for k in range(len(rewards[phase]) + 1)]\n",
    "                        G = torch.tensor([\n",
    "                            sum(discounts[j] * rewards[phase][j+t] for j in range(len(rewards[phase]) - t))\n",
    "                            for t in range(len(rewards[phase]))\n",
    "                        ], dtype=torch.float32)\n",
    "\n",
    "                        # Log average rewards-to-go as proxy for V(s)\n",
    "                        avg_rtg += G.mean().item()\n",
    "                        \n",
    "                        # --- policy loss for current agent ---\n",
    "                        pol_terms = []\n",
    "                        barrier_terms = []         \n",
    "                        for step_idx, (lp, entropy) in enumerate(zip(saved_log_probs[phase], saved_entropies[phase])):\n",
    "                            Gt = G[step_idx]   # default REINFORCE return\n",
    "                            next_state = epi_states[phase][step_idx + 1]\n",
    "                            Ct, violated = log_barrier_penalty(next_state, env, eta=0.20, r_safe=0)  \n",
    "                            if violated:\n",
    "                                episode_has_violation = True\n",
    "                            barrier_penalties.append(Ct.item())\n",
    "                           \n",
    "                            augmented_return = Gt - lambda_maze*Ct   # reward - theta * cost\n",
    "                            pol_loss = -lp * augmented_return\n",
    "                            pol_terms.append(pol_loss)\n",
    "                            batch_entropies.append(entropy)\n",
    "                        \n",
    "\n",
    "                        if episode_has_violation:\n",
    "                            batch_violation_count += 1\n",
    "                        ep_loss = torch.stack(pol_terms).sum()\n",
    "                        batch_loss += ep_loss\n",
    "                        batch_count += 1\n",
    "\n",
    "        \n",
    "                    # logging rewards per episode\n",
    "                    episode_rewards = [sum(r) for r in rewards]\n",
    "                    batch_rewards.append(episode_rewards)\n",
    "                    \n",
    "                values_log[phase].append(avg_rtg/batch_count)\n",
    "                barrier_log[phase].append(np.mean(barrier_penalties))\n",
    "                violation_rate = batch_violation_count / batch_count\n",
    "                violation_log[phase].append(violation_rate)\n",
    "                \n",
    "                # Calculate mean entropy across the entire batch\n",
    "                if batch_entropies:\n",
    "                    mean_entropy = torch.stack(batch_entropies).mean()\n",
    "                else:\n",
    "                    mean_entropy = torch.tensor(0.0)\n",
    "                \n",
    "                # Update current agent's policy\n",
    "                if batch_count > 0:\n",
    "                    loss = (batch_loss / batch_count) - entropy_coef[phase] * mean_entropy*0\n",
    "                    optimizers[phase].zero_grad()\n",
    "                    loss.backward()\n",
    "                    #torch.nn.utils.clip_grad_norm_(policies[phase].parameters(), max_norm=1.0)\n",
    "                    optimizers[phase].step()\n",
    "\n",
    "                    if phase == 1 and episode_count > 99:\n",
    "                        #temp = check_convergence(values_log, phase, window, episode_count, min_episodes)\n",
    "                        break\n",
    "\n",
    "                \n",
    "                # Decay entropy coefficient periodically\n",
    "                if episode_count % 3 == 0:\n",
    "                    entropy_coef[phase] = max(min_entropy_coef, entropy_coef[phase] * entropy_decay[phase])\n",
    "                    lambda_maze = max(min_lambda, lambda_maze * lambda_decay)\n",
    "                \n",
    "                # Update scores\n",
    "                avg_batch_rewards = np.mean(batch_rewards, axis=0)\n",
    "                scores_deque.append(avg_batch_rewards)\n",
    "                scores.append(avg_batch_rewards)\n",
    "\n",
    "                # --- LB-SGD gradient norm check ---\n",
    "                grad_sq = 0.0\n",
    "                for p in policies[phase].parameters():\n",
    "                    if p.grad is not None:\n",
    "                        grad_sq += p.grad.norm(2).item() ** 2\n",
    "                grad_norm = grad_sq ** 0.5\n",
    "                \n",
    "                grad_log[phase].append(grad_norm)\n",
    "                \n",
    "                # LB-SGD stopping condition:  ||∇B_η|| ≤ δ/2\n",
    "                delta = 0.1        # user-selected tolerance\n",
    "                threshold = delta / 2\n",
    "                K = 5              # must be below threshold for 5 consecutive batches\n",
    "                \n",
    "                if grad_norm < threshold:\n",
    "                    below_counter[phase] += 1\n",
    "                else:\n",
    "                    below_counter[phase] = 0\n",
    "                \n",
    "                if below_counter[phase] >= K:\n",
    "                    print(f\"Agent {phase} converged by LB-SGD condition!\")\n",
    "                    break\n",
    "                    \n",
    "                agent_converged = check_convergence(values_log, phase, window, episode_count, min_episodes)\n",
    "                                \n",
    "                if agent_converged and phase == 0 and episode_count > 799:\n",
    "                    print(f\"Agent {phase} converged after {episode_count} episodes\")\n",
    "                    break\n",
    "\n",
    "                # ---- print progress ----\n",
    "                if episode_count % print_every == 0:\n",
    "                    avg_rewards = np.mean(scores_deque, axis=0) if len(scores_deque) > 0 else [0]*env.n_agents\n",
    "                    avg_barrier = np.mean(barrier_penalties) if len(barrier_penalties) > 0 else 0.0\n",
    "                \n",
    "                    msg  = f\" Agent{phase} Episode {episode_count}\"\n",
    "                    msg += f\" | avgR={avg_rewards[phase]:.4f}\"\n",
    "                    msg += f\" | violations={violation_rate}\"\n",
    "                    msg += f\" | avgBarrier={avg_barrier:.4f}\"\n",
    "                    msg += f\" | gradNorm={grad_norm:.4f}\"\n",
    "                \n",
    "                    print(msg)\n",
    "\n",
    "                #if episode_count % 20 == 0:\n",
    "                #    solution(env, policies) \n",
    "        # BETWEEN-ROUND convergence check (after all agents complete this round)\n",
    "        print(f\"\\n--- End of Round {round_count}: Computing Round Averages ---\")\n",
    "        solution(env, policies) \n",
    "        break\n",
    "        \n",
    "        for phase in range(env.n_agents):\n",
    "            if len(values_log[phase]) >= window:\n",
    "                mean_V_round = np.mean(values_log[phase][-window:])  # last window as representative of this round\n",
    "                round_values[phase].append(mean_V_round)   \n",
    "        \n",
    "        if round_count > 2:\n",
    "            all_stable = True\n",
    "            for phase in range(env.n_agents):\n",
    "                prev_mean = round_values[phase][-2]\n",
    "                curr_mean = round_values[phase][-1]\n",
    "                delta_round = abs(curr_mean - prev_mean)\n",
    "                print(f\"   Agent {phase}: ΔV_round = {delta_round:.6e}\")\n",
    "                \n",
    "                if delta_round < round_convergence_threshold:\n",
    "                    print(f\" Agent {phase} stabilized between Rounds {round_count-1} and {round_count}\")\n",
    "                else:\n",
    "                    all_stable = False\n",
    "                    print(f\" Agent {phase} still changing between rounds (Δ={delta_round:.6e})\")\n",
    "        \n",
    "            if all_stable:\n",
    "                print(f\"\\n All agents' VALUE functions stabilized between rounds — training converged.\")\n",
    "                all_agents_stable = True\n",
    "                break\n",
    "\n",
    "        \n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "    \n",
    "    return scores, values_log, barrier_log, violation_log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5b68eca-58aa-4324-acd1-4f01a3ea83bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:23:42.694721Z",
     "iopub.status.busy": "2025-12-14T18:23:42.694721Z",
     "iopub.status.idle": "2025-12-14T18:23:42.701178Z",
     "shell.execute_reply": "2025-12-14T18:23:42.700181Z",
     "shell.execute_reply.started": "2025-12-14T18:23:42.694721Z"
    }
   },
   "outputs": [],
   "source": [
    "def initialize_policy_with_manual_probs(policy, prob_matrix):\n",
    "    \"\"\"\n",
    "    prob_matrix: tensor or array of shape [H, W, A]\n",
    "                  giving π(a|s) for each state.\n",
    "    \"\"\"\n",
    "    H, W, A = prob_matrix.shape\n",
    "    assert A == policy.num_actions\n",
    "\n",
    "    probs = torch.tensor(prob_matrix, dtype=torch.float32)\n",
    "\n",
    "    # Convert probabilities → logits\n",
    "    logits = torch.log(probs)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        policy.logits[:] = logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b8eb9a98-5a25-4112-8064-ce8f4578a928",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:23:42.983069Z",
     "iopub.status.busy": "2025-12-14T18:23:42.982098Z",
     "iopub.status.idle": "2025-12-14T18:23:43.003161Z",
     "shell.execute_reply": "2025-12-14T18:23:43.002204Z",
     "shell.execute_reply.started": "2025-12-14T18:23:42.983069Z"
    }
   },
   "outputs": [],
   "source": [
    "H0, W0 = 5, 5\n",
    "A = 4\n",
    "eps = 1e-3\n",
    "pad = 2\n",
    "# new padded size\n",
    "H = H0 + 2 * pad\n",
    "W = W0 + 2 * pad\n",
    "\n",
    "# Define your action probabilities manually for each cell\n",
    "probs = np.zeros((H0, W0, A))\n",
    "probs[0, 0] = [0.0, 1/2, 1/2, 0.0]   # up, right, down, left\n",
    "probs[0, 1] = [0.0, 1/3, 1/3, 1/3]   # up, right, down, left\n",
    "probs[0, 2] = [0.0, 0.0, 1/2, 1/2]   # up, right, down, left\n",
    "probs[0, 3] = [1/4, 1/4, 1/4, 1/4]   # up, right, down, left\n",
    "probs[0, 4] = [0.0, 0.0, 1.0, 0.0]   # up, right, down, left\n",
    "\n",
    "probs[1, 0] = [1/3, 1/3, 1/3, 0.0]   # up, right, down, left\n",
    "probs[1, 1] = [1/4, 1/4, 1/4, 1/4]   # up, right, down, left\n",
    "probs[1, 2] = [1/3, 0.0, 1/3, 1/3]    # up, right, down, left\n",
    "probs[1, 3] = [1/4, 1/4, 1/4, 1/4]   # up, right, down, left\n",
    "probs[1, 4] = [1/4, 1/4, 1/4, 1/4]   # up, right, down, left\n",
    "\n",
    "probs[2, 0] = [1/3, 1/3, 1/3, 0.0]   # up, right, down, left\n",
    "probs[2, 1] = [1/4, 1/4, 1/4, 1/4]   # up, right, down, left\n",
    "probs[2, 2] = [1/3, 0.0, 1/3, 1/3]    # up, right, down, left\n",
    "probs[2, 3] = [1/4, 1/4, 1/4, 1/4]   # up, right, down, left\n",
    "probs[2, 4] = [1/2, 0.0, 1/2, 0.0]   # up, right, down, left\n",
    "\n",
    "probs[3, 0] = [1/3, 1/3, 1/3, 0.0]   # up, right, down, left\n",
    "probs[3, 1] = [1/4, 1/4, 1/4, 1/4]   # up, right, down, left\n",
    "probs[3, 2] = [1/3, 0.0, 1/3, 1/3]    # up, right, down, left\n",
    "probs[3, 3] = [1/4, 1/4, 1/4, 1/4]   # up, right, down, left\n",
    "probs[3, 4] = [1/2, 0.0, 1/2, 0.0]   # up, right, down, left\n",
    "\n",
    "probs[4, 0] = [1/2, 1/2, 0.0, 0.0]   # up, right, down, left\n",
    "probs[4, 1] = [1/3, 1/3, 0.0, 1/3]   # up, right, down, left\n",
    "probs[4, 2] = [1/3, 1/3, 0.0, 1/3]    # up, right, down, left\n",
    "probs[4, 3] = [0.0, 1/2, 0.0, 1/2]   # up, right, down, left\n",
    "probs[4, 4] = [1/2, 0.0, 0.0, 1/2]   # up, right, down, left\n",
    "\n",
    "# initialize padded grid with uniform wall policy\n",
    "probs_padded = np.ones((H, W, A)) / A\n",
    "\n",
    "# copy original policy into the center\n",
    "probs_padded[pad:pad+H0, pad:pad+W0, :] = probs\n",
    "\n",
    "# numerical safety (optional but consistent)\n",
    "probs_padded = np.maximum(probs_padded, eps)\n",
    "probs_padded = probs_padded / probs_padded.sum(axis=2, keepdims=True)\n",
    "\n",
    "outer_walls = []\n",
    "for i in range(H):\n",
    "    for j in range(W):\n",
    "        if i < pad or i >= H - pad or j < pad or j >= W - pad:\n",
    "            outer_walls.append((i, j)) \n",
    "\n",
    "inner_walls = [(0,3), (1,3), (2,3), (3,3)]\n",
    "# shift internal walls into padded grid\n",
    "inner_walls = [(x + pad, y + pad) for (x, y) in inner_walls]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cdce4a-079b-4a07-9c45-6dc4c7454cd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:23:43.216973Z",
     "iopub.status.busy": "2025-12-14T18:23:43.216973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "\n",
      "==================================================\n",
      "=== Round 1 ===\n",
      "==================================================\n",
      "\n",
      "--- Training Agent 0 ---\n",
      " Agent0 Episode 1 | avgR=0.0273 | violations=3 | avgBarrier=80.0000 | gradNorm=64.3236\n",
      " Agent0 Episode 2 | avgR=0.0225 | violations=5 | avgBarrier=484.8870 | gradNorm=1353.3743\n",
      " Agent0 Episode 3 | avgR=0.0215 | violations=1 | avgBarrier=68.2251 | gradNorm=63.0205\n",
      " Agent0 Episode 4 | avgR=0.0205 | violations=3 | avgBarrier=109.0340 | gradNorm=139.1318\n",
      " Agent0 Episode 5 | avgR=0.0206 | violations=3 | avgBarrier=91.5904 | gradNorm=84.8258\n",
      " Agent0 Episode 6 | avgR=0.0203 | violations=6 | avgBarrier=110.2818 | gradNorm=111.6696\n",
      " Agent0 Episode 7 | avgR=0.0199 | violations=3 | avgBarrier=411.9534 | gradNorm=1273.4263\n",
      " Agent0 Episode 8 | avgR=0.0195 | violations=3 | avgBarrier=97.3111 | gradNorm=115.3945\n",
      " Agent0 Episode 9 | avgR=0.0191 | violations=3 | avgBarrier=367.8102 | gradNorm=817.3580\n",
      " Agent0 Episode 10 | avgR=0.0187 | violations=3 | avgBarrier=78.5435 | gradNorm=90.1461\n",
      " Agent0 Episode 11 | avgR=0.0183 | violations=1 | avgBarrier=60.8562 | gradNorm=51.3036\n",
      " Agent0 Episode 12 | avgR=0.0179 | violations=1 | avgBarrier=52.6007 | gradNorm=28.7700\n",
      " Agent0 Episode 13 | avgR=0.0174 | violations=3 | avgBarrier=96.6030 | gradNorm=224.3778\n",
      " Agent0 Episode 14 | avgR=0.0170 | violations=1 | avgBarrier=47.6195 | gradNorm=19.3185\n",
      " Agent0 Episode 15 | avgR=0.0166 | violations=3 | avgBarrier=143.4630 | gradNorm=305.2778\n",
      " Agent0 Episode 16 | avgR=0.0162 | violations=4 | avgBarrier=145.7551 | gradNorm=211.6533\n",
      " Agent0 Episode 17 | avgR=0.0158 | violations=1 | avgBarrier=47.9902 | gradNorm=16.5755\n",
      " Agent0 Episode 18 | avgR=0.0155 | violations=2 | avgBarrier=53.1642 | gradNorm=33.1236\n",
      " Agent0 Episode 19 | avgR=0.0151 | violations=3 | avgBarrier=221.5618 | gradNorm=574.8712\n",
      " Agent0 Episode 20 | avgR=0.0148 | violations=3 | avgBarrier=75.0317 | gradNorm=83.0649\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[2.7326e-04, 2.6330e-01, 1.8034e-01, 5.5608e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[3.1648e-04, 5.3855e-01, 4.6096e-01, 1.7743e-04]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[2.7326e-04, 2.6330e-01, 1.8034e-01, 5.5608e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[3.1648e-04, 5.3855e-01, 4.6096e-01, 1.7743e-04]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[2.7326e-04, 2.6330e-01, 1.8034e-01, 5.5608e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[3.1648e-04, 5.3855e-01, 4.6096e-01, 1.7743e-04]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[2.7326e-04, 2.6330e-01, 1.8034e-01, 5.5608e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[3.1648e-04, 5.3855e-01, 4.6096e-01, 1.7743e-04]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[2.7326e-04, 2.6330e-01, 1.8034e-01, 5.5608e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[3.1648e-04, 5.3855e-01, 4.6096e-01, 1.7743e-04]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      " Agent0 Episode 21 | avgR=0.0139 | violations=2 | avgBarrier=266.1353 | gradNorm=703.3372\n",
      " Agent0 Episode 22 | avgR=0.0134 | violations=0 | avgBarrier=40.7994 | gradNorm=6.5358\n",
      " Agent0 Episode 23 | avgR=0.0128 | violations=1 | avgBarrier=45.8839 | gradNorm=18.8353\n",
      " Agent0 Episode 24 | avgR=0.0123 | violations=1 | avgBarrier=42.6044 | gradNorm=13.0622\n",
      " Agent0 Episode 25 | avgR=0.0117 | violations=3 | avgBarrier=50.1605 | gradNorm=24.0300\n",
      " Agent0 Episode 26 | avgR=0.0111 | violations=1 | avgBarrier=44.0401 | gradNorm=14.5997\n",
      " Agent0 Episode 27 | avgR=0.0105 | violations=0 | avgBarrier=40.6039 | gradNorm=7.4758\n",
      " Agent0 Episode 28 | avgR=0.0100 | violations=1 | avgBarrier=50.7108 | gradNorm=38.6615\n",
      " Agent0 Episode 29 | avgR=0.0096 | violations=0 | avgBarrier=40.3389 | gradNorm=8.5564\n",
      " Agent0 Episode 30 | avgR=0.0091 | violations=1 | avgBarrier=70.4816 | gradNorm=68.1148\n",
      " Agent0 Episode 31 | avgR=0.0087 | violations=1 | avgBarrier=43.6062 | gradNorm=16.6971\n",
      " Agent0 Episode 32 | avgR=0.0084 | violations=1 | avgBarrier=41.9300 | gradNorm=7.7342\n",
      " Agent0 Episode 33 | avgR=0.0080 | violations=0 | avgBarrier=40.0495 | gradNorm=6.8804\n",
      " Agent0 Episode 34 | avgR=0.0078 | violations=0 | avgBarrier=39.8010 | gradNorm=10.7095\n",
      " Agent0 Episode 35 | avgR=0.0075 | violations=2 | avgBarrier=44.2322 | gradNorm=17.1010\n",
      " Agent0 Episode 36 | avgR=0.0073 | violations=0 | avgBarrier=39.9146 | gradNorm=8.1963\n",
      " Agent0 Episode 37 | avgR=0.0071 | violations=2 | avgBarrier=59.3149 | gradNorm=71.9374\n",
      " Agent0 Episode 38 | avgR=0.0069 | violations=2 | avgBarrier=61.7447 | gradNorm=85.0640\n",
      " Agent0 Episode 39 | avgR=0.0067 | violations=1 | avgBarrier=43.6393 | gradNorm=22.5142\n",
      " Agent0 Episode 40 | avgR=0.0065 | violations=0 | avgBarrier=39.2964 | gradNorm=9.2010\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[1.3313e-04, 1.9188e-01, 1.6199e-01, 6.4600e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[1.8860e-04, 3.5157e-01, 6.4810e-01, 1.3707e-04]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[3.0261e-01, 5.7227e-01, 1.2492e-01, 2.0763e-04]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #   0   # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 3.]])\n",
      "Prob:  tensor([[0.3146, 0.1393, 0.4033, 0.1428]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #   0   #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[4., 3.]])\n",
      "Prob:  tensor([[0.3322, 0.4336, 0.0827, 0.1515]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #     0 #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[4., 4.]])\n",
      "Prob:  tensor([[6.8554e-01, 6.7602e-04, 1.7433e-01, 1.3945e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #     0 # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 4.]])\n",
      "Prob:  tensor([[0.1952, 0.0008, 0.3842, 0.4197]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #   0   # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 3.]])\n",
      "Prob:  tensor([[0.3146, 0.1393, 0.4033, 0.1428]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #   0   #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[4., 3.]])\n",
      "Prob:  tensor([[0.3322, 0.4336, 0.0827, 0.1515]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #     0 #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[4., 4.]])\n",
      "Prob:  tensor([[6.8554e-01, 6.7602e-04, 1.7433e-01, 1.3945e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #     0 # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      " Agent0 Episode 41 | avgR=0.0063 | violations=2 | avgBarrier=49.1069 | gradNorm=27.6908\n",
      " Agent0 Episode 42 | avgR=0.0062 | violations=0 | avgBarrier=39.4155 | gradNorm=7.7357\n",
      " Agent0 Episode 43 | avgR=0.0060 | violations=2 | avgBarrier=135.5480 | gradNorm=286.0526\n",
      " Agent0 Episode 44 | avgR=0.0059 | violations=1 | avgBarrier=48.7584 | gradNorm=33.4448\n",
      " Agent0 Episode 45 | avgR=0.0058 | violations=0 | avgBarrier=39.0394 | gradNorm=7.0333\n",
      " Agent0 Episode 46 | avgR=0.0057 | violations=0 | avgBarrier=38.7078 | gradNorm=10.6785\n",
      " Agent0 Episode 47 | avgR=0.0056 | violations=0 | avgBarrier=38.7390 | gradNorm=7.4878\n",
      " Agent0 Episode 48 | avgR=0.0055 | violations=0 | avgBarrier=38.7997 | gradNorm=9.5258\n",
      " Agent0 Episode 49 | avgR=0.0054 | violations=1 | avgBarrier=58.2367 | gradNorm=77.9821\n",
      " Agent0 Episode 50 | avgR=0.0054 | violations=1 | avgBarrier=105.2655 | gradNorm=337.6784\n",
      " Agent0 Episode 51 | avgR=0.0053 | violations=1 | avgBarrier=40.8629 | gradNorm=10.9442\n",
      " Agent0 Episode 52 | avgR=0.0052 | violations=0 | avgBarrier=38.2483 | gradNorm=9.3820\n",
      " Agent0 Episode 53 | avgR=0.0052 | violations=0 | avgBarrier=38.3315 | gradNorm=8.9245\n",
      " Agent0 Episode 54 | avgR=0.0051 | violations=0 | avgBarrier=38.3940 | gradNorm=9.2909\n",
      " Agent0 Episode 55 | avgR=0.0050 | violations=2 | avgBarrier=50.2973 | gradNorm=34.6646\n",
      " Agent0 Episode 56 | avgR=0.0050 | violations=0 | avgBarrier=38.1316 | gradNorm=7.7809\n",
      " Agent0 Episode 57 | avgR=0.0049 | violations=1 | avgBarrier=40.7373 | gradNorm=9.8251\n",
      " Agent0 Episode 58 | avgR=0.0049 | violations=0 | avgBarrier=37.8762 | gradNorm=8.9548\n",
      " Agent0 Episode 59 | avgR=0.0048 | violations=0 | avgBarrier=37.9405 | gradNorm=7.7768\n",
      " Agent0 Episode 60 | avgR=0.0048 | violations=0 | avgBarrier=37.9985 | gradNorm=8.7111\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[7.9637e-05, 1.9530e-01, 1.4140e-01, 6.6322e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[8.3335e-05, 3.1104e-01, 6.8879e-01, 8.1633e-05]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[2.7097e-01, 6.0639e-01, 1.2249e-01, 1.4874e-04]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #   0   # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 3.]])\n",
      "Prob:  tensor([[0.2824, 0.1438, 0.4207, 0.1531]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #   0   #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[4., 3.]])\n",
      "Prob:  tensor([[0.4079, 0.4066, 0.0640, 0.1215]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #   0   # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 3.]])\n",
      "Prob:  tensor([[0.2824, 0.1438, 0.4207, 0.1531]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #   0   #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[4., 3.]])\n",
      "Prob:  tensor([[0.4079, 0.4066, 0.0640, 0.1215]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #   0   # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 3.]])\n",
      "Prob:  tensor([[0.2824, 0.1438, 0.4207, 0.1531]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #   0   #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[4., 3.]])\n",
      "Prob:  tensor([[0.4079, 0.4066, 0.0640, 0.1215]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #   0   # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 3.]])\n",
      "Prob:  tensor([[0.2824, 0.1438, 0.4207, 0.1531]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #   0   #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      " Agent0 Episode 61 | avgR=0.0047 | violations=0 | avgBarrier=37.6904 | gradNorm=7.7330\n",
      " Agent0 Episode 62 | avgR=0.0046 | violations=0 | avgBarrier=37.7352 | gradNorm=9.4563\n",
      " Agent0 Episode 63 | avgR=0.0046 | violations=1 | avgBarrier=113.2632 | gradNorm=269.8863\n",
      " Agent0 Episode 64 | avgR=0.0045 | violations=0 | avgBarrier=37.4824 | gradNorm=8.2818\n",
      " Agent0 Episode 65 | avgR=0.0045 | violations=0 | avgBarrier=37.5485 | gradNorm=7.4073\n",
      " Agent0 Episode 66 | avgR=0.0044 | violations=0 | avgBarrier=37.6151 | gradNorm=7.1607\n",
      " Agent0 Episode 67 | avgR=0.0044 | violations=0 | avgBarrier=37.2442 | gradNorm=6.8431\n",
      " Agent0 Episode 68 | avgR=0.0043 | violations=1 | avgBarrier=45.2224 | gradNorm=28.0009\n",
      " Agent0 Episode 69 | avgR=0.0042 | violations=1 | avgBarrier=176.0570 | gradNorm=543.8708\n",
      " Agent0 Episode 70 | avgR=0.0042 | violations=0 | avgBarrier=36.6327 | gradNorm=11.5090\n",
      " Agent0 Episode 71 | avgR=0.0041 | violations=0 | avgBarrier=36.6936 | gradNorm=9.7760\n",
      " Agent0 Episode 72 | avgR=0.0041 | violations=0 | avgBarrier=36.6208 | gradNorm=8.6832\n",
      " Agent0 Episode 73 | avgR=0.0040 | violations=1 | avgBarrier=39.4456 | gradNorm=17.9132\n",
      " Agent0 Episode 74 | avgR=0.0040 | violations=1 | avgBarrier=39.3809 | gradNorm=11.2610\n",
      " Agent0 Episode 75 | avgR=0.0039 | violations=0 | avgBarrier=36.5726 | gradNorm=6.7217\n",
      " Agent0 Episode 76 | avgR=0.0039 | violations=0 | avgBarrier=36.2225 | gradNorm=9.4113\n",
      " Agent0 Episode 77 | avgR=0.0039 | violations=1 | avgBarrier=156.6607 | gradNorm=515.3600\n",
      " Agent0 Episode 78 | avgR=0.0038 | violations=0 | avgBarrier=36.0064 | gradNorm=8.5671\n",
      " Agent0 Episode 79 | avgR=0.0038 | violations=0 | avgBarrier=35.7637 | gradNorm=8.7497\n",
      " Agent0 Episode 80 | avgR=0.0038 | violations=0 | avgBarrier=35.8151 | gradNorm=6.7853\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[6.9228e-05, 2.5507e-01, 1.2800e-01, 6.1685e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[6.2760e-05, 2.1471e-01, 7.8517e-01, 6.0689e-05]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[2.2914e-01, 6.6796e-01, 1.0283e-01, 6.2422e-05]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #   0   # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 3.]])\n",
      "Prob:  tensor([[0.3436, 0.1623, 0.3378, 0.1563]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[6.9228e-05, 2.5507e-01, 1.2800e-01, 6.1685e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[6.2760e-05, 2.1471e-01, 7.8517e-01, 6.0689e-05]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[2.2914e-01, 6.6796e-01, 1.0283e-01, 6.2422e-05]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #   0   # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 3.]])\n",
      "Prob:  tensor([[0.3436, 0.1623, 0.3378, 0.1563]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[6.9228e-05, 2.5507e-01, 1.2800e-01, 6.1685e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[6.2760e-05, 2.1471e-01, 7.8517e-01, 6.0689e-05]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      " Agent0 Episode 81 | avgR=0.0037 | violations=1 | avgBarrier=39.0828 | gradNorm=7.9430\n",
      " Agent0 Episode 82 | avgR=0.0037 | violations=0 | avgBarrier=35.4221 | gradNorm=7.3278\n",
      " Agent0 Episode 83 | avgR=0.0037 | violations=0 | avgBarrier=35.6393 | gradNorm=10.1374\n",
      " Agent0 Episode 84 | avgR=0.0037 | violations=0 | avgBarrier=35.6264 | gradNorm=9.6994\n",
      " Agent0 Episode 85 | avgR=0.0036 | violations=0 | avgBarrier=35.4010 | gradNorm=7.9512\n",
      " Agent0 Episode 86 | avgR=0.0036 | violations=0 | avgBarrier=35.4599 | gradNorm=6.5927\n",
      " Agent0 Episode 87 | avgR=0.0036 | violations=0 | avgBarrier=35.5192 | gradNorm=8.9043\n",
      " Agent0 Episode 88 | avgR=0.0036 | violations=0 | avgBarrier=35.2231 | gradNorm=7.0184\n",
      " Agent0 Episode 89 | avgR=0.0035 | violations=0 | avgBarrier=35.2824 | gradNorm=7.1413\n",
      " Agent0 Episode 90 | avgR=0.0035 | violations=0 | avgBarrier=35.3422 | gradNorm=6.6353\n",
      " Agent0 Episode 91 | avgR=0.0035 | violations=0 | avgBarrier=35.0483 | gradNorm=6.7569\n",
      " Agent0 Episode 92 | avgR=0.0035 | violations=0 | avgBarrier=35.1017 | gradNorm=6.5157\n",
      " Agent0 Episode 93 | avgR=0.0034 | violations=1 | avgBarrier=39.1854 | gradNorm=12.0318\n",
      " Agent0 Episode 94 | avgR=0.0034 | violations=0 | avgBarrier=34.8761 | gradNorm=9.5979\n",
      " Agent0 Episode 95 | avgR=0.0034 | violations=0 | avgBarrier=34.8533 | gradNorm=6.4432\n",
      " Agent0 Episode 96 | avgR=0.0034 | violations=1 | avgBarrier=39.2541 | gradNorm=14.9831\n",
      " Agent0 Episode 97 | avgR=0.0034 | violations=1 | avgBarrier=38.7897 | gradNorm=13.4344\n",
      " Agent0 Episode 98 | avgR=0.0033 | violations=0 | avgBarrier=34.7641 | gradNorm=5.0955\n",
      " Agent0 Episode 99 | avgR=0.0033 | violations=0 | avgBarrier=34.8247 | gradNorm=6.6084\n",
      " Agent0 Episode 100 | avgR=0.0033 | violations=0 | avgBarrier=34.5369 | gradNorm=7.4543\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[6.8808e-05, 2.9543e-01, 1.2467e-01, 5.7983e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[5.8714e-05, 1.9086e-01, 8.0903e-01, 5.3645e-05]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[2.6340e-01, 6.4992e-01, 8.6627e-02, 4.9867e-05]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #   0   # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 3.]])\n",
      "Prob:  tensor([[0.4307, 0.1733, 0.2665, 0.1296]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[6.8808e-05, 2.9543e-01, 1.2467e-01, 5.7983e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[5.8714e-05, 1.9086e-01, 8.0903e-01, 5.3645e-05]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[2.6340e-01, 6.4992e-01, 8.6627e-02, 4.9867e-05]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #   0   # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 3.]])\n",
      "Prob:  tensor([[0.4307, 0.1733, 0.2665, 0.1296]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[6.8808e-05, 2.9543e-01, 1.2467e-01, 5.7983e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[5.8714e-05, 1.9086e-01, 8.0903e-01, 5.3645e-05]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      " Agent0 Episode 101 | avgR=0.0033 | violations=0 | avgBarrier=34.5976 | gradNorm=7.0082\n",
      " Agent0 Episode 102 | avgR=0.0033 | violations=1 | avgBarrier=39.3217 | gradNorm=19.6268\n",
      " Agent0 Episode 103 | avgR=0.0032 | violations=0 | avgBarrier=34.3693 | gradNorm=6.0414\n",
      " Agent0 Episode 104 | avgR=0.0032 | violations=0 | avgBarrier=34.4303 | gradNorm=6.0999\n",
      " Agent0 Episode 105 | avgR=0.0032 | violations=1 | avgBarrier=39.2372 | gradNorm=17.4029\n",
      " Agent0 Episode 106 | avgR=0.0032 | violations=0 | avgBarrier=34.2054 | gradNorm=4.4813\n",
      " Agent0 Episode 107 | avgR=0.0032 | violations=0 | avgBarrier=34.2665 | gradNorm=6.3342\n",
      " Agent0 Episode 108 | avgR=0.0031 | violations=0 | avgBarrier=34.3216 | gradNorm=7.8083\n",
      " Agent0 Episode 109 | avgR=0.0031 | violations=0 | avgBarrier=34.0458 | gradNorm=7.3813\n",
      " Agent0 Episode 110 | avgR=0.0031 | violations=1 | avgBarrier=43.9296 | gradNorm=31.9720\n",
      " Agent0 Episode 111 | avgR=0.0031 | violations=1 | avgBarrier=39.3968 | gradNorm=18.3019\n",
      " Agent0 Episode 112 | avgR=0.0031 | violations=0 | avgBarrier=33.8762 | gradNorm=6.9193\n",
      " Agent0 Episode 113 | avgR=0.0030 | violations=0 | avgBarrier=33.9376 | gradNorm=7.4400\n",
      " Agent0 Episode 114 | avgR=0.0030 | violations=0 | avgBarrier=33.9993 | gradNorm=6.2591\n",
      " Agent0 Episode 115 | avgR=0.0030 | violations=0 | avgBarrier=33.7206 | gradNorm=6.8007\n",
      " Agent0 Episode 116 | avgR=0.0030 | violations=0 | avgBarrier=33.7822 | gradNorm=9.5053\n",
      " Agent0 Episode 117 | avgR=0.0030 | violations=0 | avgBarrier=33.8441 | gradNorm=6.6126\n",
      " Agent0 Episode 118 | avgR=0.0029 | violations=3 | avgBarrier=215.9993 | gradNorm=434.5886\n",
      " Agent0 Episode 119 | avgR=0.0029 | violations=3 | avgBarrier=45.5974 | gradNorm=31.9062\n",
      " Agent0 Episode 120 | avgR=0.0029 | violations=0 | avgBarrier=33.5108 | gradNorm=8.0679\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[6.6291e-05, 3.2341e-01, 1.2841e-01, 5.4811e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[3.2992e-05, 1.6726e-01, 8.3266e-01, 4.2982e-05]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[2.5165e-01, 6.6553e-01, 8.2767e-02, 4.7663e-05]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #   0   # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 3.]])\n",
      "Prob:  tensor([[0.4397, 0.1753, 0.2608, 0.1242]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[6.6291e-05, 3.2341e-01, 1.2841e-01, 5.4811e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[3.2992e-05, 1.6726e-01, 8.3266e-01, 4.2982e-05]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[2.5165e-01, 6.6553e-01, 8.2767e-02, 4.7663e-05]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #   0   # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 3.]])\n",
      "Prob:  tensor([[0.4397, 0.1753, 0.2608, 0.1242]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[6.6291e-05, 3.2341e-01, 1.2841e-01, 5.4811e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[3.2992e-05, 1.6726e-01, 8.3266e-01, 4.2982e-05]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      " Agent0 Episode 121 | avgR=0.0029 | violations=0 | avgBarrier=33.2341 | gradNorm=8.4177\n",
      " Agent0 Episode 122 | avgR=0.0028 | violations=0 | avgBarrier=33.2927 | gradNorm=6.9191\n",
      " Agent0 Episode 123 | avgR=0.0028 | violations=0 | avgBarrier=33.3516 | gradNorm=5.1846\n",
      " Agent0 Episode 124 | avgR=0.0028 | violations=0 | avgBarrier=33.0765 | gradNorm=4.7367\n",
      " Agent0 Episode 125 | avgR=0.0028 | violations=0 | avgBarrier=33.1352 | gradNorm=6.2755\n",
      " Agent0 Episode 126 | avgR=0.0028 | violations=0 | avgBarrier=33.1395 | gradNorm=5.8894\n",
      " Agent0 Episode 127 | avgR=0.0027 | violations=0 | avgBarrier=32.8796 | gradNorm=6.4524\n",
      " Agent0 Episode 128 | avgR=0.0027 | violations=0 | avgBarrier=32.9811 | gradNorm=9.3883\n",
      " Agent0 Episode 129 | avgR=0.0027 | violations=1 | avgBarrier=37.7411 | gradNorm=18.5888\n",
      " Agent0 Episode 130 | avgR=0.0027 | violations=0 | avgBarrier=32.7645 | gradNorm=4.1940\n",
      " Agent0 Episode 131 | avgR=0.0027 | violations=0 | avgBarrier=32.7452 | gradNorm=6.0043\n",
      " Agent0 Episode 132 | avgR=0.0026 | violations=1 | avgBarrier=37.7424 | gradNorm=15.8495\n",
      " Agent0 Episode 133 | avgR=0.0026 | violations=0 | avgBarrier=32.6091 | gradNorm=7.6399\n",
      " Agent0 Episode 134 | avgR=0.0026 | violations=0 | avgBarrier=32.5902 | gradNorm=7.2407\n",
      " Agent0 Episode 135 | avgR=0.0026 | violations=0 | avgBarrier=32.6982 | gradNorm=6.8867\n",
      " Agent0 Episode 136 | avgR=0.0026 | violations=0 | avgBarrier=32.4598 | gradNorm=5.1619\n",
      " Agent0 Episode 137 | avgR=0.0026 | violations=0 | avgBarrier=32.5184 | gradNorm=5.5535\n",
      " Agent0 Episode 138 | avgR=0.0026 | violations=0 | avgBarrier=32.5773 | gradNorm=6.1583\n",
      " Agent0 Episode 139 | avgR=0.0025 | violations=0 | avgBarrier=32.3100 | gradNorm=5.5008\n",
      " Agent0 Episode 140 | avgR=0.0025 | violations=0 | avgBarrier=32.3273 | gradNorm=5.2013\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[5.8817e-05, 3.3989e-01, 1.4640e-01, 5.1365e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[1.2097e-05, 1.2640e-01, 8.7355e-01, 3.0705e-05]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[2.2151e-01, 6.9342e-01, 8.5018e-02, 4.7246e-05]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #   0   # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 3.]])\n",
      "Prob:  tensor([[0.4248, 0.1839, 0.2590, 0.1323]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[5.8817e-05, 3.3989e-01, 1.4640e-01, 5.1365e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[1.2097e-05, 1.2640e-01, 8.7355e-01, 3.0705e-05]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[2.2151e-01, 6.9342e-01, 8.5018e-02, 4.7246e-05]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #   0   # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 3.]])\n",
      "Prob:  tensor([[0.4248, 0.1839, 0.2590, 0.1323]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[5.8817e-05, 3.3989e-01, 1.4640e-01, 5.1365e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[1.2097e-05, 1.2640e-01, 8.7355e-01, 3.0705e-05]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      " Agent0 Episode 141 | avgR=0.0025 | violations=0 | avgBarrier=32.4283 | gradNorm=6.2272\n",
      " Agent0 Episode 142 | avgR=0.0025 | violations=0 | avgBarrier=32.1097 | gradNorm=6.3403\n",
      " Agent0 Episode 143 | avgR=0.0025 | violations=0 | avgBarrier=32.2223 | gradNorm=6.7248\n",
      " Agent0 Episode 144 | avgR=0.0025 | violations=0 | avgBarrier=32.2812 | gradNorm=7.2136\n",
      " Agent0 Episode 145 | avgR=0.0025 | violations=0 | avgBarrier=31.9702 | gradNorm=5.6825\n",
      " Agent0 Episode 146 | avgR=0.0025 | violations=2 | avgBarrier=105.2901 | gradNorm=207.7779\n",
      " Agent0 Episode 147 | avgR=0.0025 | violations=0 | avgBarrier=31.7900 | gradNorm=9.1430\n",
      " Agent0 Episode 148 | avgR=0.0025 | violations=0 | avgBarrier=31.4928 | gradNorm=5.0314\n",
      " Agent0 Episode 149 | avgR=0.0025 | violations=0 | avgBarrier=31.5835 | gradNorm=5.4799\n",
      " Agent0 Episode 150 | avgR=0.0025 | violations=1 | avgBarrier=44.0371 | gradNorm=36.9659\n",
      " Agent0 Episode 151 | avgR=0.0025 | violations=1 | avgBarrier=168.6299 | gradNorm=472.4426\n",
      " Agent0 Episode 152 | avgR=0.0025 | violations=0 | avgBarrier=30.6241 | gradNorm=6.2238\n",
      " Agent0 Episode 153 | avgR=0.0025 | violations=0 | avgBarrier=30.7742 | gradNorm=4.6827\n",
      " Agent0 Episode 154 | avgR=0.0025 | violations=1 | avgBarrier=36.9045 | gradNorm=27.2719\n",
      " Agent0 Episode 155 | avgR=0.0025 | violations=1 | avgBarrier=36.9557 | gradNorm=16.1199\n",
      " Agent0 Episode 156 | avgR=0.0025 | violations=2 | avgBarrier=48.6428 | gradNorm=45.0302\n",
      " Agent0 Episode 157 | avgR=0.0025 | violations=0 | avgBarrier=30.2574 | gradNorm=4.2926\n",
      " Agent0 Episode 158 | avgR=0.0025 | violations=0 | avgBarrier=30.3661 | gradNorm=7.6941\n",
      " Agent0 Episode 159 | avgR=0.0025 | violations=0 | avgBarrier=30.4131 | gradNorm=8.7995\n",
      " Agent0 Episode 160 | avgR=0.0025 | violations=0 | avgBarrier=30.1556 | gradNorm=5.1044\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[6.1529e-05, 4.2138e-01, 1.9749e-01, 3.8107e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #     0 #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 4.]])\n",
      "Prob:  tensor([[1.0419e-04, 2.5560e-04, 1.1796e-01, 8.8168e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[6.1529e-05, 4.2138e-01, 1.9749e-01, 3.8107e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #     0 #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 4.]])\n",
      "Prob:  tensor([[1.0419e-04, 2.5560e-04, 1.1796e-01, 8.8168e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[6.1529e-05, 4.2138e-01, 1.9749e-01, 3.8107e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #     0 #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 4.]])\n",
      "Prob:  tensor([[1.0419e-04, 2.5560e-04, 1.1796e-01, 8.8168e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[6.1529e-05, 4.2138e-01, 1.9749e-01, 3.8107e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #     0 #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 4.]])\n",
      "Prob:  tensor([[1.0419e-04, 2.5560e-04, 1.1796e-01, 8.8168e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[6.1529e-05, 4.2138e-01, 1.9749e-01, 3.8107e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #     0 #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 4.]])\n",
      "Prob:  tensor([[1.0419e-04, 2.5560e-04, 1.1796e-01, 8.8168e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      " Agent0 Episode 161 | avgR=0.0025 | violations=0 | avgBarrier=30.1420 | gradNorm=5.6171\n",
      " Agent0 Episode 162 | avgR=0.0025 | violations=0 | avgBarrier=30.2510 | gradNorm=5.3776\n",
      " Agent0 Episode 163 | avgR=0.0025 | violations=0 | avgBarrier=29.9462 | gradNorm=7.1335\n",
      " Agent0 Episode 164 | avgR=0.0025 | violations=0 | avgBarrier=30.0382 | gradNorm=6.3785\n",
      " Agent0 Episode 165 | avgR=0.0026 | violations=0 | avgBarrier=30.0914 | gradNorm=8.6356\n",
      " Agent0 Episode 166 | avgR=0.0026 | violations=0 | avgBarrier=29.7561 | gradNorm=6.1812\n",
      " Agent0 Episode 167 | avgR=0.0026 | violations=0 | avgBarrier=29.8870 | gradNorm=5.1412\n",
      " Agent0 Episode 168 | avgR=0.0026 | violations=1 | avgBarrier=38.2076 | gradNorm=24.7650\n",
      " Agent0 Episode 169 | avgR=0.0026 | violations=0 | avgBarrier=29.6646 | gradNorm=5.1123\n",
      " Agent0 Episode 170 | avgR=0.0026 | violations=0 | avgBarrier=29.7180 | gradNorm=8.6196\n",
      " Agent0 Episode 171 | avgR=0.0026 | violations=0 | avgBarrier=29.7661 | gradNorm=5.2834\n",
      " Agent0 Episode 172 | avgR=0.0026 | violations=0 | avgBarrier=29.4787 | gradNorm=6.0244\n",
      " Agent0 Episode 173 | avgR=0.0026 | violations=0 | avgBarrier=29.5384 | gradNorm=3.7899\n",
      " Agent0 Episode 174 | avgR=0.0026 | violations=0 | avgBarrier=29.4793 | gradNorm=4.4123\n",
      " Agent0 Episode 175 | avgR=0.0026 | violations=0 | avgBarrier=29.3692 | gradNorm=6.7649\n",
      " Agent0 Episode 176 | avgR=0.0026 | violations=0 | avgBarrier=29.4018 | gradNorm=6.4251\n",
      " Agent0 Episode 177 | avgR=0.0026 | violations=1 | avgBarrier=45.3321 | gradNorm=70.7424\n",
      " Agent0 Episode 178 | avgR=0.0026 | violations=1 | avgBarrier=36.9842 | gradNorm=27.8222\n",
      " Agent0 Episode 179 | avgR=0.0026 | violations=0 | avgBarrier=29.1808 | gradNorm=5.7085\n",
      " Agent0 Episode 180 | avgR=0.0026 | violations=0 | avgBarrier=29.2840 | gradNorm=3.8147\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[6.0367e-05, 4.3178e-01, 2.2676e-01, 3.4140e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #     0 #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 4.]])\n",
      "Prob:  tensor([[9.0559e-05, 2.1818e-04, 1.0572e-01, 8.9397e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[6.0367e-05, 4.3178e-01, 2.2676e-01, 3.4140e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #     0 #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 4.]])\n",
      "Prob:  tensor([[9.0559e-05, 2.1818e-04, 1.0572e-01, 8.9397e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[6.0367e-05, 4.3178e-01, 2.2676e-01, 3.4140e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #     0 #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 4.]])\n",
      "Prob:  tensor([[9.0559e-05, 2.1818e-04, 1.0572e-01, 8.9397e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[6.0367e-05, 4.3178e-01, 2.2676e-01, 3.4140e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #     0 #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 4.]])\n",
      "Prob:  tensor([[9.0559e-05, 2.1818e-04, 1.0572e-01, 8.9397e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[6.0367e-05, 4.3178e-01, 2.2676e-01, 3.4140e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #     0 #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 4.]])\n",
      "Prob:  tensor([[9.0559e-05, 2.1818e-04, 1.0572e-01, 8.9397e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      " Agent0 Episode 181 | avgR=0.0026 | violations=0 | avgBarrier=28.9815 | gradNorm=9.6910\n",
      " Agent0 Episode 182 | avgR=0.0027 | violations=0 | avgBarrier=29.0474 | gradNorm=5.6563\n",
      " Agent0 Episode 183 | avgR=0.0027 | violations=1 | avgBarrier=39.1246 | gradNorm=26.8044\n",
      " Agent0 Episode 184 | avgR=0.0027 | violations=0 | avgBarrier=28.8510 | gradNorm=6.1433\n",
      " Agent0 Episode 185 | avgR=0.0027 | violations=0 | avgBarrier=28.9270 | gradNorm=5.4522\n",
      " Agent0 Episode 186 | avgR=0.0027 | violations=0 | avgBarrier=28.8972 | gradNorm=4.8990\n",
      " Agent0 Episode 187 | avgR=0.0027 | violations=0 | avgBarrier=28.6695 | gradNorm=4.6364\n",
      " Agent0 Episode 188 | avgR=0.0027 | violations=0 | avgBarrier=28.7887 | gradNorm=7.3731\n",
      " Agent0 Episode 189 | avgR=0.0027 | violations=0 | avgBarrier=28.7809 | gradNorm=5.1128\n",
      " Agent0 Episode 190 | avgR=0.0027 | violations=0 | avgBarrier=28.5753 | gradNorm=4.6384\n",
      " Agent0 Episode 191 | avgR=0.0027 | violations=0 | avgBarrier=28.5842 | gradNorm=5.9504\n",
      " Agent0 Episode 192 | avgR=0.0027 | violations=0 | avgBarrier=28.6517 | gradNorm=5.6253\n",
      " Agent0 Episode 193 | avgR=0.0027 | violations=0 | avgBarrier=28.4686 | gradNorm=5.5995\n",
      " Agent0 Episode 194 | avgR=0.0027 | violations=0 | avgBarrier=28.4934 | gradNorm=5.1961\n",
      " Agent0 Episode 195 | avgR=0.0027 | violations=0 | avgBarrier=28.5138 | gradNorm=8.6353\n",
      " Agent0 Episode 196 | avgR=0.0028 | violations=0 | avgBarrier=28.2030 | gradNorm=4.6965\n",
      " Agent0 Episode 197 | avgR=0.0028 | violations=0 | avgBarrier=28.1011 | gradNorm=6.5545\n",
      " Agent0 Episode 198 | avgR=0.0028 | violations=0 | avgBarrier=28.3967 | gradNorm=5.8569\n",
      " Agent0 Episode 199 | avgR=0.0028 | violations=0 | avgBarrier=28.2167 | gradNorm=4.8732\n",
      " Agent0 Episode 200 | avgR=0.0028 | violations=0 | avgBarrier=28.2686 | gradNorm=3.9421\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[5.6564e-05, 4.2596e-01, 2.4768e-01, 3.2630e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #     0 #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 4.]])\n",
      "Prob:  tensor([[8.6559e-05, 1.9665e-04, 1.0014e-01, 8.9958e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[5.6564e-05, 4.2596e-01, 2.4768e-01, 3.2630e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #     0 #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 4.]])\n",
      "Prob:  tensor([[8.6559e-05, 1.9665e-04, 1.0014e-01, 8.9958e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[5.6564e-05, 4.2596e-01, 2.4768e-01, 3.2630e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #     0 #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 4.]])\n",
      "Prob:  tensor([[8.6559e-05, 1.9665e-04, 1.0014e-01, 8.9958e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[5.6564e-05, 4.2596e-01, 2.4768e-01, 3.2630e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #     0 #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 4.]])\n",
      "Prob:  tensor([[8.6559e-05, 1.9665e-04, 1.0014e-01, 8.9958e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[5.6564e-05, 4.2596e-01, 2.4768e-01, 3.2630e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #     0 #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 4.]])\n",
      "Prob:  tensor([[8.6559e-05, 1.9665e-04, 1.0014e-01, 8.9958e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      " Agent0 Episode 201 | avgR=0.0028 | violations=1 | avgBarrier=100.1211 | gradNorm=273.8193\n",
      " Agent0 Episode 202 | avgR=0.0028 | violations=0 | avgBarrier=27.8529 | gradNorm=5.0331\n",
      " Agent0 Episode 203 | avgR=0.0028 | violations=0 | avgBarrier=27.8885 | gradNorm=4.5475\n",
      " Agent0 Episode 204 | avgR=0.0028 | violations=0 | avgBarrier=28.0271 | gradNorm=4.9841\n",
      " Agent0 Episode 205 | avgR=0.0028 | violations=0 | avgBarrier=27.7359 | gradNorm=6.1762\n",
      " Agent0 Episode 206 | avgR=0.0029 | violations=0 | avgBarrier=27.7924 | gradNorm=4.2716\n",
      " Agent0 Episode 207 | avgR=0.0029 | violations=0 | avgBarrier=27.8643 | gradNorm=8.4006\n",
      " Agent0 Episode 208 | avgR=0.0029 | violations=1 | avgBarrier=41.0776 | gradNorm=59.6919\n",
      " Agent0 Episode 209 | avgR=0.0029 | violations=0 | avgBarrier=27.4591 | gradNorm=6.4453\n",
      " Agent0 Episode 210 | avgR=0.0029 | violations=0 | avgBarrier=27.7210 | gradNorm=5.3373\n",
      " Agent0 Episode 211 | avgR=0.0029 | violations=0 | avgBarrier=27.4342 | gradNorm=6.2773\n",
      " Agent0 Episode 212 | avgR=0.0029 | violations=2 | avgBarrier=226.3557 | gradNorm=516.5222\n",
      " Agent0 Episode 213 | avgR=0.0030 | violations=0 | avgBarrier=23.6932 | gradNorm=4.2895\n",
      " Agent0 Episode 214 | avgR=0.0030 | violations=0 | avgBarrier=23.6213 | gradNorm=4.9053\n",
      " Agent0 Episode 215 | avgR=0.0030 | violations=1 | avgBarrier=37.5878 | gradNorm=44.7543\n",
      " Agent0 Episode 216 | avgR=0.0030 | violations=0 | avgBarrier=23.6890 | gradNorm=4.3881\n",
      " Agent0 Episode 217 | avgR=0.0031 | violations=0 | avgBarrier=23.5514 | gradNorm=3.0001\n",
      " Agent0 Episode 218 | avgR=0.0031 | violations=1 | avgBarrier=33.4263 | gradNorm=29.1181\n",
      " Agent0 Episode 219 | avgR=0.0031 | violations=0 | avgBarrier=23.6509 | gradNorm=3.7385\n",
      " Agent0 Episode 220 | avgR=0.0031 | violations=0 | avgBarrier=23.4753 | gradNorm=8.9004\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[5.4118e-05, 3.6823e-01, 2.4990e-01, 3.8181e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[1.0141e-05, 1.9359e-01, 8.0639e-01, 1.3737e-05]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[4.5481e-01, 4.7059e-01, 7.4555e-02, 4.0804e-05]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #   0   # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 3.]])\n",
      "Prob:  tensor([[0.5809, 0.2092, 0.0784, 0.1315]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[5.4118e-05, 3.6823e-01, 2.4990e-01, 3.8181e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[1.0141e-05, 1.9359e-01, 8.0639e-01, 1.3737e-05]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[4.5481e-01, 4.7059e-01, 7.4555e-02, 4.0804e-05]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #   0   # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 3.]])\n",
      "Prob:  tensor([[0.5809, 0.2092, 0.0784, 0.1315]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[5.4118e-05, 3.6823e-01, 2.4990e-01, 3.8181e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[1.0141e-05, 1.9359e-01, 8.0639e-01, 1.3737e-05]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      " Agent0 Episode 221 | avgR=0.0031 | violations=0 | avgBarrier=23.4809 | gradNorm=3.3176\n",
      " Agent0 Episode 222 | avgR=0.0031 | violations=0 | avgBarrier=23.5999 | gradNorm=4.0263\n",
      " Agent0 Episode 223 | avgR=0.0032 | violations=0 | avgBarrier=23.4260 | gradNorm=7.0991\n",
      " Agent0 Episode 224 | avgR=0.0032 | violations=0 | avgBarrier=23.4887 | gradNorm=5.8481\n",
      " Agent0 Episode 225 | avgR=0.0032 | violations=0 | avgBarrier=23.5520 | gradNorm=6.0675\n",
      " Agent0 Episode 226 | avgR=0.0032 | violations=0 | avgBarrier=23.3798 | gradNorm=5.0492\n",
      " Agent0 Episode 227 | avgR=0.0032 | violations=1 | avgBarrier=34.1417 | gradNorm=30.7461\n",
      " Agent0 Episode 228 | avgR=0.0032 | violations=0 | avgBarrier=23.4802 | gradNorm=5.0492\n",
      " Agent0 Episode 229 | avgR=0.0032 | violations=0 | avgBarrier=23.2625 | gradNorm=5.9679\n",
      " Agent0 Episode 230 | avgR=0.0032 | violations=0 | avgBarrier=23.3747 | gradNorm=4.0642\n",
      " Agent0 Episode 231 | avgR=0.0032 | violations=1 | avgBarrier=34.2189 | gradNorm=37.0883\n",
      " Agent0 Episode 232 | avgR=0.0032 | violations=0 | avgBarrier=23.2427 | gradNorm=5.1515\n",
      " Agent0 Episode 233 | avgR=0.0032 | violations=0 | avgBarrier=23.3079 | gradNorm=3.8621\n",
      " Agent0 Episode 234 | avgR=0.0032 | violations=0 | avgBarrier=23.3736 | gradNorm=3.6363\n",
      " Agent0 Episode 235 | avgR=0.0032 | violations=0 | avgBarrier=23.1588 | gradNorm=6.8060\n",
      " Agent0 Episode 236 | avgR=0.0032 | violations=0 | avgBarrier=23.2724 | gradNorm=5.2290\n",
      " Agent0 Episode 237 | avgR=0.0032 | violations=0 | avgBarrier=23.3390 | gradNorm=4.6387\n",
      " Agent0 Episode 238 | avgR=0.0032 | violations=0 | avgBarrier=23.1679 | gradNorm=3.9348\n",
      " Agent0 Episode 239 | avgR=0.0032 | violations=0 | avgBarrier=23.2392 | gradNorm=4.7298\n",
      " Agent0 Episode 240 | avgR=0.0031 | violations=0 | avgBarrier=23.3068 | gradNorm=4.0378\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[4.2082e-05, 3.4847e-01, 2.0661e-01, 4.4487e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[8.5490e-06, 1.6483e-01, 8.3515e-01, 9.1127e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[4.9053e-01, 4.3226e-01, 7.7167e-02, 4.1004e-05]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[8.5490e-06, 1.6483e-01, 8.3515e-01, 9.1127e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[4.9053e-01, 4.3226e-01, 7.7167e-02, 4.1004e-05]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[8.5490e-06, 1.6483e-01, 8.3515e-01, 9.1127e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[4.9053e-01, 4.3226e-01, 7.7167e-02, 4.1004e-05]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[8.5490e-06, 1.6483e-01, 8.3515e-01, 9.1127e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[4.9053e-01, 4.3226e-01, 7.7167e-02, 4.1004e-05]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[8.5490e-06, 1.6483e-01, 8.3515e-01, 9.1127e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      " Agent0 Episode 241 | avgR=0.0031 | violations=0 | avgBarrier=23.1369 | gradNorm=4.8200\n",
      " Agent0 Episode 242 | avgR=0.0031 | violations=0 | avgBarrier=23.2090 | gradNorm=4.5225\n",
      " Agent0 Episode 243 | avgR=0.0031 | violations=2 | avgBarrier=64.6840 | gradNorm=91.9541\n",
      " Agent0 Episode 244 | avgR=0.0031 | violations=0 | avgBarrier=22.9516 | gradNorm=2.3855\n",
      " Agent0 Episode 245 | avgR=0.0031 | violations=0 | avgBarrier=23.0183 | gradNorm=5.9880\n",
      " Agent0 Episode 246 | avgR=0.0031 | violations=0 | avgBarrier=23.0854 | gradNorm=4.5111\n",
      " Agent0 Episode 247 | avgR=0.0031 | violations=0 | avgBarrier=22.9215 | gradNorm=3.0958\n",
      " Agent0 Episode 248 | avgR=0.0031 | violations=0 | avgBarrier=22.9888 | gradNorm=6.3500\n",
      " Agent0 Episode 249 | avgR=0.0031 | violations=0 | avgBarrier=23.0567 | gradNorm=3.9993\n",
      " Agent0 Episode 250 | avgR=0.0031 | violations=0 | avgBarrier=22.8937 | gradNorm=4.7985\n",
      " Agent0 Episode 251 | avgR=0.0031 | violations=0 | avgBarrier=22.9617 | gradNorm=3.3250\n",
      " Agent0 Episode 252 | avgR=0.0030 | violations=0 | avgBarrier=23.0301 | gradNorm=4.1062\n",
      " Agent0 Episode 253 | avgR=0.0030 | violations=0 | avgBarrier=22.7843 | gradNorm=5.8388\n",
      " Agent0 Episode 254 | avgR=0.0030 | violations=0 | avgBarrier=22.9379 | gradNorm=4.7002\n",
      " Agent0 Episode 255 | avgR=0.0030 | violations=1 | avgBarrier=35.7531 | gradNorm=36.6964\n",
      " Agent0 Episode 256 | avgR=0.0030 | violations=0 | avgBarrier=22.8018 | gradNorm=3.8247\n",
      " Agent0 Episode 257 | avgR=0.0030 | violations=0 | avgBarrier=22.8702 | gradNorm=6.7095\n",
      " Agent0 Episode 258 | avgR=0.0030 | violations=0 | avgBarrier=22.9265 | gradNorm=8.2347\n",
      " Agent0 Episode 259 | avgR=0.0030 | violations=0 | avgBarrier=22.7783 | gradNorm=3.3545\n",
      " Agent0 Episode 260 | avgR=0.0030 | violations=0 | avgBarrier=22.8472 | gradNorm=1.7781\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[3.6506e-05, 3.7544e-01, 2.0445e-01, 4.2007e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[7.4750e-06, 1.4746e-01, 8.5253e-01, 4.9025e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[5.4782e-01, 3.6982e-01, 8.2314e-02, 4.1087e-05]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[7.4750e-06, 1.4746e-01, 8.5253e-01, 4.9025e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[5.4782e-01, 3.6982e-01, 8.2314e-02, 4.1087e-05]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[7.4750e-06, 1.4746e-01, 8.5253e-01, 4.9025e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[5.4782e-01, 3.6982e-01, 8.2314e-02, 4.1087e-05]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[7.4750e-06, 1.4746e-01, 8.5253e-01, 4.9025e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[5.4782e-01, 3.6982e-01, 8.2314e-02, 4.1087e-05]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[7.4750e-06, 1.4746e-01, 8.5253e-01, 4.9025e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      " Agent0 Episode 261 | avgR=0.0030 | violations=0 | avgBarrier=22.9163 | gradNorm=3.7174\n",
      " Agent0 Episode 262 | avgR=0.0030 | violations=0 | avgBarrier=22.7560 | gradNorm=3.6166\n",
      " Agent0 Episode 263 | avgR=0.0030 | violations=0 | avgBarrier=22.8251 | gradNorm=4.3575\n",
      " Agent0 Episode 264 | avgR=0.0029 | violations=0 | avgBarrier=22.8946 | gradNorm=4.5422\n",
      " Agent0 Episode 265 | avgR=0.0029 | violations=1 | avgBarrier=262.1679 | gradNorm=777.3583\n",
      " Agent0 Episode 266 | avgR=0.0029 | violations=0 | avgBarrier=20.7826 | gradNorm=4.8389\n",
      " Agent0 Episode 267 | avgR=0.0029 | violations=0 | avgBarrier=20.7992 | gradNorm=4.6530\n",
      " Agent0 Episode 268 | avgR=0.0029 | violations=0 | avgBarrier=20.6919 | gradNorm=3.9131\n",
      " Agent0 Episode 269 | avgR=0.0029 | violations=0 | avgBarrier=20.7508 | gradNorm=5.3155\n",
      " Agent0 Episode 270 | avgR=0.0029 | violations=0 | avgBarrier=20.8102 | gradNorm=4.8145\n",
      " Agent0 Episode 271 | avgR=0.0029 | violations=0 | avgBarrier=20.6613 | gradNorm=3.7635\n",
      " Agent0 Episode 272 | avgR=0.0029 | violations=1 | avgBarrier=43.2605 | gradNorm=52.1648\n",
      " Agent0 Episode 273 | avgR=0.0029 | violations=0 | avgBarrier=20.6260 | gradNorm=4.2800\n",
      " Agent0 Episode 274 | avgR=0.0029 | violations=1 | avgBarrier=41.0667 | gradNorm=102.6784\n",
      " Agent0 Episode 275 | avgR=0.0028 | violations=0 | avgBarrier=20.4085 | gradNorm=5.5625\n",
      " Agent0 Episode 276 | avgR=0.0028 | violations=0 | avgBarrier=20.4673 | gradNorm=1.8691\n",
      " Agent0 Episode 277 | avgR=0.0028 | violations=0 | avgBarrier=20.3212 | gradNorm=3.4459\n",
      " Agent0 Episode 278 | avgR=0.0028 | violations=1 | avgBarrier=33.6704 | gradNorm=35.6439\n",
      " Agent0 Episode 279 | avgR=0.0028 | violations=0 | avgBarrier=20.3875 | gradNorm=2.8901\n",
      " Agent0 Episode 280 | avgR=0.0028 | violations=0 | avgBarrier=20.2423 | gradNorm=3.2714\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[2.3583e-05, 1.2280e-01, 2.3164e-01, 6.4554e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[7.6121e-06, 1.6070e-01, 8.3929e-01, 4.6175e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[4.4587e-01, 4.6543e-01, 8.8677e-02, 2.5958e-05]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #   0   # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 3.]])\n",
      "Prob:  tensor([[0.6875, 0.1596, 0.0580, 0.0949]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[2.3583e-05, 1.2280e-01, 2.3164e-01, 6.4554e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[7.6121e-06, 1.6070e-01, 8.3929e-01, 4.6175e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[4.4587e-01, 4.6543e-01, 8.8677e-02, 2.5958e-05]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #   0   # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 3.]])\n",
      "Prob:  tensor([[0.6875, 0.1596, 0.0580, 0.0949]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[2.3583e-05, 1.2280e-01, 2.3164e-01, 6.4554e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[7.6121e-06, 1.6070e-01, 8.3929e-01, 4.6175e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      " Agent0 Episode 281 | avgR=0.0028 | violations=0 | avgBarrier=20.2829 | gradNorm=3.0785\n",
      " Agent0 Episode 282 | avgR=0.0028 | violations=0 | avgBarrier=20.3614 | gradNorm=3.0435\n",
      " Agent0 Episode 283 | avgR=0.0028 | violations=0 | avgBarrier=20.2172 | gradNorm=4.5950\n",
      " Agent0 Episode 284 | avgR=0.0028 | violations=0 | avgBarrier=20.2770 | gradNorm=5.0219\n",
      " Agent0 Episode 285 | avgR=0.0028 | violations=0 | avgBarrier=20.3372 | gradNorm=2.9780\n",
      " Agent0 Episode 286 | avgR=0.0027 | violations=0 | avgBarrier=20.1939 | gradNorm=4.7935\n",
      " Agent0 Episode 287 | avgR=0.0027 | violations=0 | avgBarrier=20.2543 | gradNorm=2.8931\n",
      " Agent0 Episode 288 | avgR=0.0027 | violations=0 | avgBarrier=20.3151 | gradNorm=3.0978\n",
      " Agent0 Episode 289 | avgR=0.0027 | violations=0 | avgBarrier=20.1725 | gradNorm=3.0099\n",
      " Agent0 Episode 290 | avgR=0.0027 | violations=0 | avgBarrier=20.2334 | gradNorm=4.6222\n",
      " Agent0 Episode 291 | avgR=0.0027 | violations=0 | avgBarrier=20.2948 | gradNorm=2.7855\n",
      " Agent0 Episode 292 | avgR=0.0027 | violations=0 | avgBarrier=20.1529 | gradNorm=3.7084\n",
      " Agent0 Episode 293 | avgR=0.0027 | violations=0 | avgBarrier=20.2143 | gradNorm=5.1272\n",
      " Agent0 Episode 294 | avgR=0.0027 | violations=0 | avgBarrier=20.2760 | gradNorm=2.4269\n",
      " Agent0 Episode 295 | avgR=0.0027 | violations=0 | avgBarrier=20.1347 | gradNorm=4.6421\n",
      " Agent0 Episode 296 | avgR=0.0027 | violations=0 | avgBarrier=20.1965 | gradNorm=2.6032\n",
      " Agent0 Episode 297 | avgR=0.0026 | violations=0 | avgBarrier=20.2586 | gradNorm=4.0024\n",
      " Agent0 Episode 298 | avgR=0.0026 | violations=0 | avgBarrier=20.1178 | gradNorm=5.4783\n",
      " Agent0 Episode 299 | avgR=0.0026 | violations=0 | avgBarrier=20.1799 | gradNorm=3.3608\n",
      " Agent0 Episode 300 | avgR=0.0026 | violations=1 | avgBarrier=43.7034 | gradNorm=61.5751\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[1.8109e-05, 9.1114e-02, 2.3067e-01, 6.7820e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[7.8056e-06, 1.7314e-01, 8.2685e-01, 4.6890e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[3.8833e-01, 5.2218e-01, 8.9468e-02, 1.7571e-05]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #   0   # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 3.]])\n",
      "Prob:  tensor([[0.7122, 0.1533, 0.0560, 0.0784]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[1.8109e-05, 9.1114e-02, 2.3067e-01, 6.7820e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[7.8056e-06, 1.7314e-01, 8.2685e-01, 4.6890e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[3.8833e-01, 5.2218e-01, 8.9468e-02, 1.7571e-05]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #   0   # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 3.]])\n",
      "Prob:  tensor([[0.7122, 0.1533, 0.0560, 0.0784]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[1.8109e-05, 9.1114e-02, 2.3067e-01, 6.7820e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[7.8056e-06, 1.7314e-01, 8.2685e-01, 4.6890e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      " Agent0 Episode 301 | avgR=0.0026 | violations=0 | avgBarrier=19.9164 | gradNorm=3.2256\n",
      " Agent0 Episode 302 | avgR=0.0026 | violations=0 | avgBarrier=19.9778 | gradNorm=3.0406\n",
      " Agent0 Episode 303 | avgR=0.0026 | violations=0 | avgBarrier=20.0395 | gradNorm=1.5352\n",
      " Agent0 Episode 304 | avgR=0.0026 | violations=0 | avgBarrier=19.9005 | gradNorm=3.4312\n",
      " Agent0 Episode 305 | avgR=0.0026 | violations=0 | avgBarrier=19.9622 | gradNorm=3.2408\n",
      " Agent0 Episode 306 | avgR=0.0026 | violations=0 | avgBarrier=19.9838 | gradNorm=4.1119\n",
      " Agent0 Episode 307 | avgR=0.0026 | violations=0 | avgBarrier=19.8861 | gradNorm=4.9531\n",
      " Agent0 Episode 308 | avgR=0.0026 | violations=0 | avgBarrier=19.9480 | gradNorm=2.2243\n",
      " Agent0 Episode 309 | avgR=0.0025 | violations=0 | avgBarrier=19.9625 | gradNorm=3.8608\n",
      " Agent0 Episode 310 | avgR=0.0025 | violations=0 | avgBarrier=19.8725 | gradNorm=3.6069\n",
      " Agent0 Episode 311 | avgR=0.0025 | violations=0 | avgBarrier=19.9344 | gradNorm=3.0024\n",
      " Agent0 Episode 312 | avgR=0.0025 | violations=0 | avgBarrier=19.9966 | gradNorm=3.9853\n",
      " Agent0 Episode 313 | avgR=0.0025 | violations=0 | avgBarrier=19.8185 | gradNorm=4.1447\n",
      " Agent0 Episode 314 | avgR=0.0025 | violations=0 | avgBarrier=19.9211 | gradNorm=2.8523\n",
      " Agent0 Episode 315 | avgR=0.0025 | violations=0 | avgBarrier=19.9833 | gradNorm=5.9087\n",
      " Agent0 Episode 316 | avgR=0.0025 | violations=0 | avgBarrier=19.8452 | gradNorm=5.9384\n",
      " Agent0 Episode 317 | avgR=0.0025 | violations=0 | avgBarrier=19.9071 | gradNorm=3.9698\n",
      " Agent0 Episode 318 | avgR=0.0025 | violations=0 | avgBarrier=19.9692 | gradNorm=3.5322\n",
      " Agent0 Episode 319 | avgR=0.0025 | violations=1 | avgBarrier=43.6136 | gradNorm=51.9470\n",
      " Agent0 Episode 320 | avgR=0.0025 | violations=0 | avgBarrier=19.6835 | gradNorm=2.6903\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[1.6863e-05, 8.1919e-02, 2.1038e-01, 7.0768e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[8.1683e-06, 1.9768e-01, 8.0231e-01, 4.9015e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[4.0034e-01, 5.0945e-01, 9.0201e-02, 1.0323e-05]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #   0   # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 3.]])\n",
      "Prob:  tensor([[0.7391, 0.1435, 0.0537, 0.0636]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[1.6863e-05, 8.1919e-02, 2.1038e-01, 7.0768e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[8.1683e-06, 1.9768e-01, 8.0231e-01, 4.9015e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[4.0034e-01, 5.0945e-01, 9.0201e-02, 1.0323e-05]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #   0   # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 3.]])\n",
      "Prob:  tensor([[0.7391, 0.1435, 0.0537, 0.0636]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[1.6863e-05, 8.1919e-02, 2.1038e-01, 7.0768e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[8.1683e-06, 1.9768e-01, 8.0231e-01, 4.9015e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      " Agent0 Episode 321 | avgR=0.0025 | violations=0 | avgBarrier=19.6906 | gradNorm=3.4450\n",
      " Agent0 Episode 322 | avgR=0.0025 | violations=0 | avgBarrier=19.6090 | gradNorm=4.8088\n",
      " Agent0 Episode 323 | avgR=0.0025 | violations=1 | avgBarrier=39.3789 | gradNorm=55.4778\n",
      " Agent0 Episode 324 | avgR=0.0025 | violations=0 | avgBarrier=19.5895 | gradNorm=1.8304\n",
      " Agent0 Episode 325 | avgR=0.0024 | violations=0 | avgBarrier=19.4071 | gradNorm=2.6434\n",
      " Agent0 Episode 326 | avgR=0.0024 | violations=0 | avgBarrier=19.5140 | gradNorm=3.4183\n",
      " Agent0 Episode 327 | avgR=0.0024 | violations=0 | avgBarrier=19.5740 | gradNorm=2.7311\n",
      " Agent0 Episode 328 | avgR=0.0024 | violations=0 | avgBarrier=19.4378 | gradNorm=3.2552\n",
      " Agent0 Episode 329 | avgR=0.0024 | violations=0 | avgBarrier=19.4975 | gradNorm=4.2369\n",
      " Agent0 Episode 330 | avgR=0.0024 | violations=0 | avgBarrier=19.5574 | gradNorm=3.8773\n",
      " Agent0 Episode 331 | avgR=0.0024 | violations=0 | avgBarrier=19.4211 | gradNorm=3.2822\n",
      " Agent0 Episode 332 | avgR=0.0024 | violations=0 | avgBarrier=19.4806 | gradNorm=6.0762\n",
      " Agent0 Episode 333 | avgR=0.0024 | violations=0 | avgBarrier=19.5402 | gradNorm=2.4882\n",
      " Agent0 Episode 334 | avgR=0.0024 | violations=0 | avgBarrier=19.4038 | gradNorm=4.5193\n",
      " Agent0 Episode 335 | avgR=0.0024 | violations=0 | avgBarrier=19.4594 | gradNorm=3.9327\n",
      " Agent0 Episode 336 | avgR=0.0024 | violations=0 | avgBarrier=19.5222 | gradNorm=3.5904\n",
      " Agent0 Episode 337 | avgR=0.0024 | violations=0 | avgBarrier=19.3857 | gradNorm=4.1703\n",
      " Agent0 Episode 338 | avgR=0.0024 | violations=0 | avgBarrier=19.4444 | gradNorm=4.1265\n",
      " Agent0 Episode 339 | avgR=0.0024 | violations=0 | avgBarrier=19.4997 | gradNorm=3.4899\n",
      " Agent0 Episode 340 | avgR=0.0024 | violations=0 | avgBarrier=19.3665 | gradNorm=1.5376\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[1.1795e-05, 7.5751e-02, 1.8831e-01, 7.3593e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[7.8253e-06, 1.7424e-01, 8.2575e-01, 4.6958e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[3.4525e-01, 5.7028e-01, 8.4463e-02, 6.6384e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #   0   # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 3.]])\n",
      "Prob:  tensor([[0.7064, 0.1642, 0.0599, 0.0695]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[1.1795e-05, 7.5751e-02, 1.8831e-01, 7.3593e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[7.8253e-06, 1.7424e-01, 8.2575e-01, 4.6958e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[3.4525e-01, 5.7028e-01, 8.4463e-02, 6.6384e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #   0   # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 3.]])\n",
      "Prob:  tensor([[0.7064, 0.1642, 0.0599, 0.0695]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[1.1795e-05, 7.5751e-02, 1.8831e-01, 7.3593e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[7.8253e-06, 1.7424e-01, 8.2575e-01, 4.6958e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      " Agent0 Episode 341 | avgR=0.0024 | violations=0 | avgBarrier=19.4248 | gradNorm=5.0157\n",
      " Agent0 Episode 342 | avgR=0.0024 | violations=0 | avgBarrier=19.4831 | gradNorm=2.6966\n",
      " Agent0 Episode 343 | avgR=0.0024 | violations=0 | avgBarrier=19.3460 | gradNorm=2.3728\n",
      " Agent0 Episode 344 | avgR=0.0023 | violations=0 | avgBarrier=19.4037 | gradNorm=2.6457\n",
      " Agent0 Episode 345 | avgR=0.0023 | violations=0 | avgBarrier=19.4614 | gradNorm=4.1084\n",
      " Agent0 Episode 346 | avgR=0.0023 | violations=0 | avgBarrier=19.3239 | gradNorm=5.3449\n",
      " Agent0 Episode 347 | avgR=0.0023 | violations=0 | avgBarrier=19.3810 | gradNorm=2.3658\n",
      " Agent0 Episode 348 | avgR=0.0023 | violations=0 | avgBarrier=19.4381 | gradNorm=3.9562\n",
      " Agent0 Episode 349 | avgR=0.0023 | violations=0 | avgBarrier=19.3002 | gradNorm=3.2920\n",
      " Agent0 Episode 350 | avgR=0.0023 | violations=1 | avgBarrier=95.7593 | gradNorm=262.9118\n",
      " Agent0 Episode 351 | avgR=0.0023 | violations=0 | avgBarrier=18.5941 | gradNorm=1.5877\n",
      " Agent0 Episode 352 | avgR=0.0023 | violations=0 | avgBarrier=18.4607 | gradNorm=2.8509\n",
      " Agent0 Episode 353 | avgR=0.0023 | violations=0 | avgBarrier=18.5032 | gradNorm=3.1009\n",
      " Agent0 Episode 354 | avgR=0.0023 | violations=0 | avgBarrier=18.5662 | gradNorm=3.0885\n",
      " Agent0 Episode 355 | avgR=0.0023 | violations=0 | avgBarrier=18.4328 | gradNorm=3.2981\n",
      " Agent0 Episode 356 | avgR=0.0023 | violations=0 | avgBarrier=18.4850 | gradNorm=3.3236\n",
      " Agent0 Episode 357 | avgR=0.0023 | violations=0 | avgBarrier=18.5373 | gradNorm=2.4030\n",
      " Agent0 Episode 358 | avgR=0.0023 | violations=0 | avgBarrier=18.4037 | gradNorm=3.5840\n",
      " Agent0 Episode 359 | avgR=0.0023 | violations=0 | avgBarrier=18.4555 | gradNorm=4.1423\n",
      " Agent0 Episode 360 | avgR=0.0023 | violations=0 | avgBarrier=18.5073 | gradNorm=3.6221\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[1.2315e-05, 8.7293e-02, 2.4887e-01, 6.6382e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[7.1801e-06, 1.3763e-01, 8.6236e-01, 4.3093e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[1.8366e-01, 7.4370e-01, 7.2640e-02, 2.2707e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #   0   # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 3.]])\n",
      "Prob:  tensor([[0.6934, 0.1716, 0.0619, 0.0731]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[1.2315e-05, 8.7293e-02, 2.4887e-01, 6.6382e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[7.1801e-06, 1.3763e-01, 8.6236e-01, 4.3093e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[1.8366e-01, 7.4370e-01, 7.2640e-02, 2.2707e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #   0   # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 3.]])\n",
      "Prob:  tensor([[0.6934, 0.1716, 0.0619, 0.0731]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[1.2315e-05, 8.7293e-02, 2.4887e-01, 6.6382e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[7.1801e-06, 1.3763e-01, 8.6236e-01, 4.3093e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      " Agent0 Episode 361 | avgR=0.0023 | violations=0 | avgBarrier=18.3735 | gradNorm=3.0348\n",
      " Agent0 Episode 362 | avgR=0.0022 | violations=0 | avgBarrier=18.4248 | gradNorm=2.6212\n",
      " Agent0 Episode 363 | avgR=0.0022 | violations=0 | avgBarrier=18.4761 | gradNorm=3.8516\n",
      " Agent0 Episode 364 | avgR=0.0022 | violations=0 | avgBarrier=18.3421 | gradNorm=2.0751\n",
      " Agent0 Episode 365 | avgR=0.0022 | violations=0 | avgBarrier=18.3928 | gradNorm=3.9708\n",
      " Agent0 Episode 366 | avgR=0.0022 | violations=0 | avgBarrier=18.4435 | gradNorm=2.6057\n",
      " Agent0 Episode 367 | avgR=0.0022 | violations=0 | avgBarrier=18.3092 | gradNorm=4.5613\n",
      " Agent0 Episode 368 | avgR=0.0022 | violations=0 | avgBarrier=18.3593 | gradNorm=3.9502\n",
      " Agent0 Episode 369 | avgR=0.0022 | violations=0 | avgBarrier=18.4094 | gradNorm=4.2772\n",
      " Agent0 Episode 370 | avgR=0.0022 | violations=0 | avgBarrier=18.2514 | gradNorm=4.1492\n",
      " Agent0 Episode 371 | avgR=0.0022 | violations=0 | avgBarrier=18.3246 | gradNorm=2.4243\n",
      " Agent0 Episode 372 | avgR=0.0022 | violations=0 | avgBarrier=18.3740 | gradNorm=3.3972\n",
      " Agent0 Episode 373 | avgR=0.0022 | violations=1 | avgBarrier=42.5932 | gradNorm=58.8570\n",
      " Agent0 Episode 374 | avgR=0.0022 | violations=0 | avgBarrier=17.9912 | gradNorm=4.4255\n",
      " Agent0 Episode 375 | avgR=0.0022 | violations=0 | avgBarrier=18.0634 | gradNorm=2.7588\n",
      " Agent0 Episode 376 | avgR=0.0022 | violations=0 | avgBarrier=17.9310 | gradNorm=2.5444\n",
      " Agent0 Episode 377 | avgR=0.0022 | violations=0 | avgBarrier=17.9792 | gradNorm=2.8746\n",
      " Agent0 Episode 378 | avgR=0.0022 | violations=0 | avgBarrier=18.0273 | gradNorm=3.6335\n",
      " Agent0 Episode 379 | avgR=0.0022 | violations=0 | avgBarrier=17.8946 | gradNorm=1.9022\n",
      " Agent0 Episode 380 | avgR=0.0022 | violations=0 | avgBarrier=17.9421 | gradNorm=2.5271\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[9.2869e-06, 8.4692e-02, 2.5756e-01, 6.5773e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[6.5529e-06, 1.0880e-01, 8.9119e-01, 3.9334e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[1.4400e-01, 7.8798e-01, 6.8025e-02, 1.4605e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #   0   # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 3.]])\n",
      "Prob:  tensor([[0.6810, 0.1811, 0.0587, 0.0791]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[9.2869e-06, 8.4692e-02, 2.5756e-01, 6.5773e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[6.5529e-06, 1.0880e-01, 8.9119e-01, 3.9334e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[1.4400e-01, 7.8798e-01, 6.8025e-02, 1.4605e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #   0   # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 3.]])\n",
      "Prob:  tensor([[0.6810, 0.1811, 0.0587, 0.0791]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[9.2869e-06, 8.4692e-02, 2.5756e-01, 6.5773e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[6.5529e-06, 1.0880e-01, 8.9119e-01, 3.9334e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      " Agent0 Episode 381 | avgR=0.0021 | violations=0 | avgBarrier=17.9896 | gradNorm=2.0124\n",
      " Agent0 Episode 382 | avgR=0.0021 | violations=0 | avgBarrier=17.8566 | gradNorm=2.3805\n",
      " Agent0 Episode 383 | avgR=0.0021 | violations=0 | avgBarrier=17.9033 | gradNorm=3.3938\n",
      " Agent0 Episode 384 | avgR=0.0021 | violations=0 | avgBarrier=17.9500 | gradNorm=3.0290\n",
      " Agent0 Episode 385 | avgR=0.0021 | violations=0 | avgBarrier=17.8166 | gradNorm=4.1491\n",
      " Agent0 Episode 386 | avgR=0.0021 | violations=0 | avgBarrier=17.8626 | gradNorm=2.6826\n",
      " Agent0 Episode 387 | avgR=0.0021 | violations=0 | avgBarrier=17.9085 | gradNorm=3.0729\n",
      " Agent0 Episode 388 | avgR=0.0021 | violations=0 | avgBarrier=17.7747 | gradNorm=1.7113\n",
      " Agent0 Episode 389 | avgR=0.0021 | violations=0 | avgBarrier=17.7841 | gradNorm=4.2849\n",
      " Agent0 Episode 390 | avgR=0.0021 | violations=0 | avgBarrier=17.8656 | gradNorm=2.1079\n",
      " Agent0 Episode 391 | avgR=0.0021 | violations=0 | avgBarrier=17.7313 | gradNorm=2.0898\n",
      " Agent0 Episode 392 | avgR=0.0021 | violations=0 | avgBarrier=17.7756 | gradNorm=2.8816\n",
      " Agent0 Episode 393 | avgR=0.0021 | violations=0 | avgBarrier=17.8197 | gradNorm=2.6414\n",
      " Agent0 Episode 394 | avgR=0.0021 | violations=0 | avgBarrier=17.6560 | gradNorm=3.4364\n",
      " Agent0 Episode 395 | avgR=0.0021 | violations=0 | avgBarrier=17.7290 | gradNorm=2.5769\n",
      " Agent0 Episode 396 | avgR=0.0021 | violations=0 | avgBarrier=17.7721 | gradNorm=3.2078\n",
      " Agent0 Episode 397 | avgR=0.0021 | violations=0 | avgBarrier=17.6370 | gradNorm=2.1691\n",
      " Agent0 Episode 398 | avgR=0.0021 | violations=0 | avgBarrier=17.6794 | gradNorm=2.5679\n",
      " Agent0 Episode 399 | avgR=0.0021 | violations=0 | avgBarrier=17.7216 | gradNorm=3.6205\n",
      " Agent0 Episode 400 | avgR=0.0021 | violations=0 | avgBarrier=17.5860 | gradNorm=4.8667\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[7.6329e-06, 8.1823e-02, 2.5547e-01, 6.6270e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[6.1647e-06, 9.3517e-02, 9.0647e-01, 3.7008e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[1.4524e-01, 7.8685e-01, 6.7904e-02, 1.3952e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #   0   # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 3.]])\n",
      "Prob:  tensor([[0.6664, 0.1919, 0.0563, 0.0854]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[7.6329e-06, 8.1823e-02, 2.5547e-01, 6.6270e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[6.1647e-06, 9.3517e-02, 9.0647e-01, 3.7008e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[1.4524e-01, 7.8685e-01, 6.7904e-02, 1.3952e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #   0   # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 3.]])\n",
      "Prob:  tensor([[0.6664, 0.1919, 0.0563, 0.0854]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[7.6329e-06, 8.1823e-02, 2.5547e-01, 6.6270e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[6.1647e-06, 9.3517e-02, 9.0647e-01, 3.7008e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      " Agent0 Episode 401 | avgR=0.0021 | violations=0 | avgBarrier=17.6273 | gradNorm=2.0665\n",
      " Agent0 Episode 402 | avgR=0.0021 | violations=0 | avgBarrier=17.6685 | gradNorm=2.8855\n",
      " Agent0 Episode 403 | avgR=0.0020 | violations=0 | avgBarrier=17.5324 | gradNorm=2.0179\n",
      " Agent0 Episode 404 | avgR=0.0020 | violations=0 | avgBarrier=17.5728 | gradNorm=3.4872\n",
      " Agent0 Episode 405 | avgR=0.0020 | violations=0 | avgBarrier=17.6129 | gradNorm=3.5534\n",
      " Agent0 Episode 406 | avgR=0.0020 | violations=0 | avgBarrier=17.4763 | gradNorm=3.3361\n",
      " Agent0 Episode 407 | avgR=0.0020 | violations=0 | avgBarrier=17.5156 | gradNorm=1.9875\n",
      " Agent0 Episode 408 | avgR=0.0020 | violations=0 | avgBarrier=17.5547 | gradNorm=4.1106\n",
      " Agent0 Episode 409 | avgR=0.0020 | violations=0 | avgBarrier=17.4175 | gradNorm=3.5108\n",
      " Agent0 Episode 410 | avgR=0.0020 | violations=0 | avgBarrier=17.4558 | gradNorm=3.2723\n",
      " Agent0 Episode 411 | avgR=0.0020 | violations=0 | avgBarrier=17.4937 | gradNorm=3.4690\n",
      " Agent0 Episode 412 | avgR=0.0020 | violations=0 | avgBarrier=17.3561 | gradNorm=3.5361\n",
      " Agent0 Episode 413 | avgR=0.0020 | violations=0 | avgBarrier=17.3932 | gradNorm=2.3302\n",
      " Agent0 Episode 414 | avgR=0.0020 | violations=0 | avgBarrier=17.4205 | gradNorm=3.0087\n",
      " Agent0 Episode 415 | avgR=0.0020 | violations=0 | avgBarrier=17.2763 | gradNorm=1.4118\n",
      " Agent0 Episode 416 | avgR=0.0020 | violations=0 | avgBarrier=17.3284 | gradNorm=2.4597\n",
      " Agent0 Episode 417 | avgR=0.0020 | violations=0 | avgBarrier=17.3641 | gradNorm=3.3926\n",
      " Agent0 Episode 418 | avgR=0.0020 | violations=0 | avgBarrier=17.2255 | gradNorm=3.0043\n",
      " Agent0 Episode 419 | avgR=0.0020 | violations=0 | avgBarrier=17.2603 | gradNorm=2.3693\n",
      " Agent0 Episode 420 | avgR=0.0020 | violations=0 | avgBarrier=17.2949 | gradNorm=1.8100\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[7.5067e-06, 8.2932e-02, 2.5809e-01, 6.5897e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[6.1720e-06, 9.3755e-02, 9.0624e-01, 3.7056e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[1.4169e-01, 7.9294e-01, 6.5365e-02, 1.3625e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #   0   # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 3.]])\n",
      "Prob:  tensor([[0.6802, 0.1816, 0.0546, 0.0836]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[7.5067e-06, 8.2932e-02, 2.5809e-01, 6.5897e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[6.1720e-06, 9.3755e-02, 9.0624e-01, 3.7056e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[1.4169e-01, 7.9294e-01, 6.5365e-02, 1.3625e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #   0   # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 3.]])\n",
      "Prob:  tensor([[0.6802, 0.1816, 0.0546, 0.0836]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[7.5067e-06, 8.2932e-02, 2.5809e-01, 6.5897e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[6.1720e-06, 9.3755e-02, 9.0624e-01, 3.7056e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      " Agent0 Episode 421 | avgR=0.0020 | violations=0 | avgBarrier=17.1558 | gradNorm=2.7466\n",
      " Agent0 Episode 422 | avgR=0.0020 | violations=0 | avgBarrier=17.1895 | gradNorm=2.4824\n",
      " Agent0 Episode 423 | avgR=0.0020 | violations=0 | avgBarrier=17.2197 | gradNorm=2.7890\n",
      " Agent0 Episode 424 | avgR=0.0020 | violations=0 | avgBarrier=17.0835 | gradNorm=1.7188\n",
      " Agent0 Episode 425 | avgR=0.0020 | violations=0 | avgBarrier=17.1160 | gradNorm=2.7902\n",
      " Agent0 Episode 426 | avgR=0.0020 | violations=0 | avgBarrier=17.1482 | gradNorm=2.2831\n",
      " Agent0 Episode 427 | avgR=0.0020 | violations=0 | avgBarrier=17.0083 | gradNorm=2.5318\n",
      " Agent0 Episode 428 | avgR=0.0019 | violations=0 | avgBarrier=17.0397 | gradNorm=2.5348\n",
      " Agent0 Episode 429 | avgR=0.0019 | violations=0 | avgBarrier=17.0707 | gradNorm=2.5934\n",
      " Agent0 Episode 430 | avgR=0.0019 | violations=0 | avgBarrier=16.9305 | gradNorm=3.4064\n",
      " Agent0 Episode 431 | avgR=0.0019 | violations=0 | avgBarrier=16.9607 | gradNorm=3.2394\n",
      " Agent0 Episode 432 | avgR=0.0019 | violations=0 | avgBarrier=16.9906 | gradNorm=2.5518\n",
      " Agent0 Episode 433 | avgR=0.0019 | violations=0 | avgBarrier=16.8500 | gradNorm=2.8635\n",
      " Agent0 Episode 434 | avgR=0.0019 | violations=1 | avgBarrier=42.0290 | gradNorm=104.1614\n",
      " Agent0 Episode 435 | avgR=0.0019 | violations=0 | avgBarrier=16.4845 | gradNorm=2.1325\n",
      " Agent0 Episode 436 | avgR=0.0019 | violations=0 | avgBarrier=16.3960 | gradNorm=2.2637\n",
      " Agent0 Episode 437 | avgR=0.0019 | violations=0 | avgBarrier=16.4265 | gradNorm=4.7321\n",
      " Agent0 Episode 438 | avgR=0.0019 | violations=0 | avgBarrier=16.4567 | gradNorm=2.5548\n",
      " Agent0 Episode 439 | avgR=0.0019 | violations=0 | avgBarrier=16.3219 | gradNorm=2.8774\n",
      " Agent0 Episode 440 | avgR=0.0019 | violations=0 | avgBarrier=16.3513 | gradNorm=2.1796\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[5.2408e-06, 7.2679e-02, 1.9804e-01, 7.2928e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[6.3168e-06, 9.9204e-02, 9.0079e-01, 3.7929e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[1.4550e-01, 7.8907e-01, 6.5429e-02, 1.3692e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #   0   # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 3.]])\n",
      "Prob:  tensor([[0.6685, 0.1860, 0.0556, 0.0899]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[5.2408e-06, 7.2679e-02, 1.9804e-01, 7.2928e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[6.3168e-06, 9.9204e-02, 9.0079e-01, 3.7929e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[1.4550e-01, 7.8907e-01, 6.5429e-02, 1.3692e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #   0   # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 3.]])\n",
      "Prob:  tensor([[0.6685, 0.1860, 0.0556, 0.0899]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[5.2408e-06, 7.2679e-02, 1.9804e-01, 7.2928e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[6.3168e-06, 9.9204e-02, 9.0079e-01, 3.7929e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      " Agent0 Episode 441 | avgR=0.0019 | violations=0 | avgBarrier=16.3806 | gradNorm=1.7850\n",
      " Agent0 Episode 442 | avgR=0.0019 | violations=0 | avgBarrier=16.2454 | gradNorm=3.7199\n",
      " Agent0 Episode 443 | avgR=0.0019 | violations=0 | avgBarrier=16.2649 | gradNorm=2.0535\n",
      " Agent0 Episode 444 | avgR=0.0019 | violations=0 | avgBarrier=16.3022 | gradNorm=2.6695\n",
      " Agent0 Episode 445 | avgR=0.0019 | violations=0 | avgBarrier=16.1403 | gradNorm=4.7511\n",
      " Agent0 Episode 446 | avgR=0.0019 | violations=0 | avgBarrier=16.1947 | gradNorm=3.1298\n",
      " Agent0 Episode 447 | avgR=0.0019 | violations=0 | avgBarrier=16.2219 | gradNorm=2.5568\n",
      " Agent0 Episode 448 | avgR=0.0019 | violations=0 | avgBarrier=16.0863 | gradNorm=1.6582\n",
      " Agent0 Episode 449 | avgR=0.0019 | violations=0 | avgBarrier=16.1126 | gradNorm=3.0024\n",
      " Agent0 Episode 450 | avgR=0.0019 | violations=0 | avgBarrier=16.1387 | gradNorm=4.5231\n",
      " Agent0 Episode 451 | avgR=0.0019 | violations=0 | avgBarrier=16.0029 | gradNorm=2.0514\n",
      " Agent0 Episode 452 | avgR=0.0019 | violations=0 | avgBarrier=16.0283 | gradNorm=1.6286\n",
      " Agent0 Episode 453 | avgR=0.0019 | violations=0 | avgBarrier=16.0534 | gradNorm=2.5522\n",
      " Agent0 Episode 454 | avgR=0.0019 | violations=0 | avgBarrier=15.9174 | gradNorm=3.0657\n",
      " Agent0 Episode 455 | avgR=0.0019 | violations=0 | avgBarrier=15.9418 | gradNorm=2.0166\n",
      " Agent0 Episode 456 | avgR=0.0019 | violations=0 | avgBarrier=15.9659 | gradNorm=1.9110\n",
      " Agent0 Episode 457 | avgR=0.0019 | violations=0 | avgBarrier=15.8298 | gradNorm=2.3359\n",
      " Agent0 Episode 458 | avgR=0.0019 | violations=0 | avgBarrier=15.8531 | gradNorm=2.5408\n",
      " Agent0 Episode 459 | avgR=0.0018 | violations=0 | avgBarrier=15.8762 | gradNorm=2.6066\n",
      " Agent0 Episode 460 | avgR=0.0018 | violations=0 | avgBarrier=15.7401 | gradNorm=2.0251\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[3.9031e-06, 6.5595e-02, 1.6187e-01, 7.7253e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[6.4596e-06, 1.0480e-01, 8.9519e-01, 3.8791e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[1.4469e-01, 7.8878e-01, 6.6530e-02, 1.3754e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #   0   # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 3.]])\n",
      "Prob:  tensor([[0.6494, 0.1932, 0.0573, 0.1001]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[3.9031e-06, 6.5595e-02, 1.6187e-01, 7.7253e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[6.4596e-06, 1.0480e-01, 8.9519e-01, 3.8791e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[1.4469e-01, 7.8878e-01, 6.6530e-02, 1.3754e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #   0   # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 3.]])\n",
      "Prob:  tensor([[0.6494, 0.1932, 0.0573, 0.1001]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[3.9031e-06, 6.5595e-02, 1.6187e-01, 7.7253e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[6.4596e-06, 1.0480e-01, 8.9519e-01, 3.8791e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      " Agent0 Episode 461 | avgR=0.0018 | violations=0 | avgBarrier=15.7624 | gradNorm=1.8095\n",
      " Agent0 Episode 462 | avgR=0.0018 | violations=0 | avgBarrier=15.7845 | gradNorm=2.3205\n",
      " Agent0 Episode 463 | avgR=0.0018 | violations=0 | avgBarrier=15.6483 | gradNorm=3.6760\n",
      " Agent0 Episode 464 | avgR=0.0018 | violations=1 | avgBarrier=39.3156 | gradNorm=65.9553\n",
      " Agent0 Episode 465 | avgR=0.0018 | violations=0 | avgBarrier=15.3171 | gradNorm=2.1252\n",
      " Agent0 Episode 466 | avgR=0.0018 | violations=0 | avgBarrier=15.1876 | gradNorm=2.5887\n",
      " Agent0 Episode 467 | avgR=0.0018 | violations=0 | avgBarrier=15.2111 | gradNorm=1.8861\n",
      " Agent0 Episode 468 | avgR=0.0018 | violations=0 | avgBarrier=15.2344 | gradNorm=3.7431\n",
      " Agent0 Episode 469 | avgR=0.0018 | violations=0 | avgBarrier=15.1049 | gradNorm=4.8062\n",
      " Agent0 Episode 470 | avgR=0.0018 | violations=0 | avgBarrier=15.1275 | gradNorm=3.5996\n",
      " Agent0 Episode 471 | avgR=0.0018 | violations=0 | avgBarrier=15.1499 | gradNorm=3.5407\n",
      " Agent0 Episode 472 | avgR=0.0018 | violations=0 | avgBarrier=15.0204 | gradNorm=2.1078\n",
      " Agent0 Episode 473 | avgR=0.0018 | violations=0 | avgBarrier=15.0422 | gradNorm=4.0421\n",
      " Agent0 Episode 474 | avgR=0.0018 | violations=0 | avgBarrier=15.0637 | gradNorm=1.5788\n",
      " Agent0 Episode 475 | avgR=0.0018 | violations=0 | avgBarrier=14.9342 | gradNorm=4.0902\n",
      " Agent0 Episode 476 | avgR=0.0018 | violations=0 | avgBarrier=14.9551 | gradNorm=1.8025\n",
      " Agent0 Episode 477 | avgR=0.0018 | violations=0 | avgBarrier=14.9759 | gradNorm=2.6322\n",
      " Agent0 Episode 478 | avgR=0.0018 | violations=0 | avgBarrier=14.8464 | gradNorm=2.8199\n",
      " Agent0 Episode 479 | avgR=0.0018 | violations=0 | avgBarrier=14.8665 | gradNorm=2.7778\n",
      " Agent0 Episode 480 | avgR=0.0018 | violations=1 | avgBarrier=166.2242 | gradNorm=590.5577\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[3.8981e-06, 7.0272e-02, 1.6325e-01, 7.6647e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[6.1018e-06, 1.1324e-01, 8.8675e-01, 1.6814e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[1.4858e-01, 7.8785e-01, 6.3568e-02, 1.3700e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #   0   # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 3.]])\n",
      "Prob:  tensor([[0.5712, 0.2343, 0.0657, 0.1288]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[3.8981e-06, 7.0272e-02, 1.6325e-01, 7.6647e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[6.1018e-06, 1.1324e-01, 8.8675e-01, 1.6814e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[1.4858e-01, 7.8785e-01, 6.3568e-02, 1.3700e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #   0   # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 3.]])\n",
      "Prob:  tensor([[0.5712, 0.2343, 0.0657, 0.1288]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[3.8981e-06, 7.0272e-02, 1.6325e-01, 7.6647e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[6.1018e-06, 1.1324e-01, 8.8675e-01, 1.6814e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      " Agent0 Episode 481 | avgR=0.0018 | violations=0 | avgBarrier=12.2505 | gradNorm=2.4445\n",
      " Agent0 Episode 482 | avgR=0.0018 | violations=0 | avgBarrier=12.2734 | gradNorm=1.7885\n",
      " Agent0 Episode 483 | avgR=0.0018 | violations=0 | avgBarrier=12.2963 | gradNorm=2.6720\n",
      " Agent0 Episode 484 | avgR=0.0018 | violations=0 | avgBarrier=12.1961 | gradNorm=1.9526\n",
      " Agent0 Episode 485 | avgR=0.0018 | violations=0 | avgBarrier=12.2188 | gradNorm=0.9355\n",
      " Agent0 Episode 486 | avgR=0.0017 | violations=0 | avgBarrier=12.2416 | gradNorm=2.8207\n",
      " Agent0 Episode 487 | avgR=0.0017 | violations=0 | avgBarrier=12.1417 | gradNorm=1.5632\n",
      " Agent0 Episode 488 | avgR=0.0017 | violations=0 | avgBarrier=12.1643 | gradNorm=2.4396\n",
      " Agent0 Episode 489 | avgR=0.0017 | violations=0 | avgBarrier=12.1868 | gradNorm=2.9373\n",
      " Agent0 Episode 490 | avgR=0.0017 | violations=0 | avgBarrier=12.0873 | gradNorm=2.2578\n",
      " Agent0 Episode 491 | avgR=0.0017 | violations=0 | avgBarrier=12.1097 | gradNorm=3.0138\n",
      " Agent0 Episode 492 | avgR=0.0017 | violations=0 | avgBarrier=12.1321 | gradNorm=1.8481\n",
      " Agent0 Episode 493 | avgR=0.0017 | violations=0 | avgBarrier=12.0176 | gradNorm=1.4958\n",
      " Agent0 Episode 494 | avgR=0.0017 | violations=0 | avgBarrier=12.0554 | gradNorm=3.0558\n",
      " Agent0 Episode 495 | avgR=0.0017 | violations=0 | avgBarrier=12.0489 | gradNorm=1.5456\n",
      " Agent0 Episode 496 | avgR=0.0017 | violations=0 | avgBarrier=11.9728 | gradNorm=2.1115\n",
      " Agent0 Episode 497 | avgR=0.0017 | violations=0 | avgBarrier=11.9905 | gradNorm=2.0656\n",
      " Agent0 Episode 498 | avgR=0.0017 | violations=0 | avgBarrier=11.9973 | gradNorm=2.5317\n",
      " Agent0 Episode 499 | avgR=0.0017 | violations=0 | avgBarrier=11.8931 | gradNorm=2.2173\n",
      " Agent0 Episode 500 | avgR=0.0017 | violations=0 | avgBarrier=11.9481 | gradNorm=1.9556\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[4.2080e-06, 7.9386e-02, 2.0173e-01, 7.1888e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[7.0829e-06, 1.7520e-01, 8.2479e-01, 1.6965e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[3.9455e-02, 9.4343e-01, 1.7116e-02, 6.6437e-07]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #   0   # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 3.]])\n",
      "Prob:  tensor([[0.5590, 0.2347, 0.0671, 0.1393]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[4.2080e-06, 7.9386e-02, 2.0173e-01, 7.1888e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[7.0829e-06, 1.7520e-01, 8.2479e-01, 1.6965e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[3.9455e-02, 9.4343e-01, 1.7116e-02, 6.6437e-07]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #   0   # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 3.]])\n",
      "Prob:  tensor([[0.5590, 0.2347, 0.0671, 0.1393]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[4.2080e-06, 7.9386e-02, 2.0173e-01, 7.1888e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[7.0829e-06, 1.7520e-01, 8.2479e-01, 1.6965e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      " Agent0 Episode 501 | avgR=0.0017 | violations=0 | avgBarrier=11.9700 | gradNorm=1.4673\n",
      " Agent0 Episode 502 | avgR=0.0017 | violations=0 | avgBarrier=11.8719 | gradNorm=1.8348\n",
      " Agent0 Episode 503 | avgR=0.0017 | violations=0 | avgBarrier=11.8935 | gradNorm=1.4226\n",
      " Agent0 Episode 504 | avgR=0.0017 | violations=0 | avgBarrier=11.9151 | gradNorm=2.2521\n",
      " Agent0 Episode 505 | avgR=0.0017 | violations=0 | avgBarrier=11.8174 | gradNorm=1.6936\n",
      " Agent0 Episode 506 | avgR=0.0017 | violations=0 | avgBarrier=11.8389 | gradNorm=1.4302\n",
      " Agent0 Episode 507 | avgR=0.0017 | violations=0 | avgBarrier=11.8603 | gradNorm=1.8603\n",
      " Agent0 Episode 508 | avgR=0.0017 | violations=0 | avgBarrier=11.7630 | gradNorm=1.2955\n",
      " Agent0 Episode 509 | avgR=0.0017 | violations=0 | avgBarrier=11.7843 | gradNorm=2.1272\n",
      " Agent0 Episode 510 | avgR=0.0017 | violations=1 | avgBarrier=33.7265 | gradNorm=63.2006\n",
      " Agent0 Episode 511 | avgR=0.0017 | violations=0 | avgBarrier=11.3297 | gradNorm=1.1156\n",
      " Agent0 Episode 512 | avgR=0.0017 | violations=0 | avgBarrier=11.3548 | gradNorm=1.3573\n",
      " Agent0 Episode 513 | avgR=0.0017 | violations=0 | avgBarrier=11.3800 | gradNorm=2.5085\n",
      " Agent0 Episode 514 | avgR=0.0017 | violations=0 | avgBarrier=11.2849 | gradNorm=1.2918\n",
      " Agent0 Episode 515 | avgR=0.0017 | violations=0 | avgBarrier=11.3162 | gradNorm=2.0607\n",
      " Agent0 Episode 516 | avgR=0.0017 | violations=0 | avgBarrier=11.3411 | gradNorm=1.3402\n",
      " Agent0 Episode 517 | avgR=0.0017 | violations=0 | avgBarrier=11.2524 | gradNorm=1.7044\n",
      " Agent0 Episode 518 | avgR=0.0017 | violations=0 | avgBarrier=11.2771 | gradNorm=0.8985\n",
      " Agent0 Episode 519 | avgR=0.0017 | violations=0 | avgBarrier=11.2997 | gradNorm=1.8237\n",
      " Agent0 Episode 520 | avgR=0.0017 | violations=1 | avgBarrier=29.1065 | gradNorm=53.1466\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[3.9390e-06, 7.4311e-02, 1.9964e-01, 7.2605e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[4.9839e-06, 1.4869e-01, 8.5131e-01, 1.4621e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[3.3631e-02, 9.5190e-01, 1.4469e-02, 6.0373e-07]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #   0   # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 3.]])\n",
      "Prob:  tensor([[0.5673, 0.2129, 0.0672, 0.1527]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[3.9390e-06, 7.4311e-02, 1.9964e-01, 7.2605e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[4.9839e-06, 1.4869e-01, 8.5131e-01, 1.4621e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[3.3631e-02, 9.5190e-01, 1.4469e-02, 6.0373e-07]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #   0   # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 3.]])\n",
      "Prob:  tensor([[0.5673, 0.2129, 0.0672, 0.1527]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[3.9390e-06, 7.4311e-02, 1.9964e-01, 7.2605e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[4.9839e-06, 1.4869e-01, 8.5131e-01, 1.4621e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      " Agent0 Episode 521 | avgR=0.0017 | violations=0 | avgBarrier=10.9851 | gradNorm=1.8757\n",
      " Agent0 Episode 522 | avgR=0.0017 | violations=0 | avgBarrier=11.0109 | gradNorm=2.4544\n",
      " Agent0 Episode 523 | avgR=0.0017 | violations=0 | avgBarrier=10.9263 | gradNorm=1.4717\n",
      " Agent0 Episode 524 | avgR=0.0017 | violations=0 | avgBarrier=10.9219 | gradNorm=1.5518\n",
      " Agent0 Episode 525 | avgR=0.0017 | violations=0 | avgBarrier=10.9781 | gradNorm=1.4489\n",
      " Agent0 Episode 526 | avgR=0.0017 | violations=0 | avgBarrier=10.8937 | gradNorm=1.9295\n",
      " Agent0 Episode 527 | avgR=0.0017 | violations=0 | avgBarrier=10.9091 | gradNorm=1.4653\n",
      " Agent0 Episode 528 | avgR=0.0017 | violations=0 | avgBarrier=10.9346 | gradNorm=1.2890\n",
      " Agent0 Episode 529 | avgR=0.0017 | violations=0 | avgBarrier=10.8605 | gradNorm=1.5460\n",
      " Agent0 Episode 530 | avgR=0.0017 | violations=0 | avgBarrier=10.8856 | gradNorm=1.3038\n",
      " Agent0 Episode 531 | avgR=0.0017 | violations=0 | avgBarrier=10.9108 | gradNorm=1.8205\n",
      " Agent0 Episode 532 | avgR=0.0017 | violations=0 | avgBarrier=10.8265 | gradNorm=1.2174\n",
      " Agent0 Episode 533 | avgR=0.0017 | violations=0 | avgBarrier=10.8514 | gradNorm=1.3982\n",
      " Agent0 Episode 534 | avgR=0.0017 | violations=0 | avgBarrier=10.8465 | gradNorm=1.3489\n",
      " Agent0 Episode 535 | avgR=0.0017 | violations=0 | avgBarrier=10.7828 | gradNorm=1.0352\n",
      " Agent0 Episode 536 | avgR=0.0017 | violations=0 | avgBarrier=10.8174 | gradNorm=1.3950\n",
      " Agent0 Episode 537 | avgR=0.0017 | violations=0 | avgBarrier=10.8419 | gradNorm=1.9078\n",
      " Agent0 Episode 538 | avgR=0.0017 | violations=0 | avgBarrier=10.7578 | gradNorm=1.7449\n",
      " Agent0 Episode 539 | avgR=0.0017 | violations=0 | avgBarrier=10.7821 | gradNorm=1.8976\n",
      " Agent0 Episode 540 | avgR=0.0017 | violations=0 | avgBarrier=10.8063 | gradNorm=1.7824\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[2.7762e-06, 7.2004e-02, 1.8371e-01, 7.4428e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[3.7777e-06, 9.9021e-02, 9.0097e-01, 1.2099e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[3.3856e-02, 9.5188e-01, 1.4263e-02, 6.0173e-07]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #   0   # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 3.]])\n",
      "Prob:  tensor([[0.5276, 0.2379, 0.0741, 0.1603]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[2.7762e-06, 7.2004e-02, 1.8371e-01, 7.4428e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[3.7777e-06, 9.9021e-02, 9.0097e-01, 1.2099e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[3.3856e-02, 9.5188e-01, 1.4263e-02, 6.0173e-07]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #   0   # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 3.]])\n",
      "Prob:  tensor([[0.5276, 0.2379, 0.0741, 0.1603]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[2.7762e-06, 7.2004e-02, 1.8371e-01, 7.4428e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[3.7777e-06, 9.9021e-02, 9.0097e-01, 1.2099e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      " Agent0 Episode 541 | avgR=0.0017 | violations=0 | avgBarrier=10.7124 | gradNorm=1.4446\n",
      " Agent0 Episode 542 | avgR=0.0017 | violations=0 | avgBarrier=10.7462 | gradNorm=2.5046\n",
      " Agent0 Episode 543 | avgR=0.0017 | violations=0 | avgBarrier=10.7701 | gradNorm=2.5979\n",
      " Agent0 Episode 544 | avgR=0.0017 | violations=0 | avgBarrier=10.6860 | gradNorm=1.0145\n",
      " Agent0 Episode 545 | avgR=0.0017 | violations=0 | avgBarrier=10.7095 | gradNorm=1.8608\n",
      " Agent0 Episode 546 | avgR=0.0016 | violations=0 | avgBarrier=10.7330 | gradNorm=2.2995\n",
      " Agent0 Episode 547 | avgR=0.0016 | violations=0 | avgBarrier=10.6488 | gradNorm=2.0650\n",
      " Agent0 Episode 548 | avgR=0.0016 | violations=0 | avgBarrier=10.6466 | gradNorm=1.5331\n",
      " Agent0 Episode 549 | avgR=0.0016 | violations=0 | avgBarrier=10.6955 | gradNorm=1.9877\n",
      " Agent0 Episode 550 | avgR=0.0016 | violations=0 | avgBarrier=10.6113 | gradNorm=0.7960\n",
      " Agent0 Episode 551 | avgR=0.0016 | violations=0 | avgBarrier=10.6126 | gradNorm=2.0481\n",
      " Agent0 Episode 552 | avgR=0.0016 | violations=0 | avgBarrier=10.6570 | gradNorm=2.2957\n",
      " Agent0 Episode 553 | avgR=0.0016 | violations=0 | avgBarrier=10.5728 | gradNorm=1.2222\n",
      " Agent0 Episode 554 | avgR=0.0016 | violations=0 | avgBarrier=10.5950 | gradNorm=2.6474\n",
      " Agent0 Episode 555 | avgR=0.0016 | violations=0 | avgBarrier=10.6172 | gradNorm=1.8764\n",
      " Agent0 Episode 556 | avgR=0.0016 | violations=0 | avgBarrier=10.5329 | gradNorm=1.1145\n",
      " Agent0 Episode 557 | avgR=0.0016 | violations=0 | avgBarrier=10.5547 | gradNorm=0.7023\n",
      " Agent0 Episode 558 | avgR=0.0016 | violations=0 | avgBarrier=10.5763 | gradNorm=1.2363\n",
      " Agent0 Episode 559 | avgR=0.0016 | violations=0 | avgBarrier=10.4920 | gradNorm=1.6957\n",
      " Agent0 Episode 560 | avgR=0.0016 | violations=0 | avgBarrier=10.5133 | gradNorm=2.0928\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[2.6680e-06, 7.2202e-02, 1.8335e-01, 7.4444e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[3.6716e-06, 9.5720e-02, 9.0427e-01, 1.1888e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[3.3190e-02, 9.5265e-01, 1.4157e-02, 5.9736e-07]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #   0   # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 3.]])\n",
      "Prob:  tensor([[0.5341, 0.2356, 0.0743, 0.1560]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[2.6680e-06, 7.2202e-02, 1.8335e-01, 7.4444e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[3.6716e-06, 9.5720e-02, 9.0427e-01, 1.1888e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[3.3190e-02, 9.5265e-01, 1.4157e-02, 5.9736e-07]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #   0   # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 3.]])\n",
      "Prob:  tensor([[0.5341, 0.2356, 0.0743, 0.1560]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[2.6680e-06, 7.2202e-02, 1.8335e-01, 7.4444e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[3.6716e-06, 9.5720e-02, 9.0427e-01, 1.1888e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      " Agent0 Episode 561 | avgR=0.0016 | violations=0 | avgBarrier=10.5210 | gradNorm=1.8729\n",
      " Agent0 Episode 562 | avgR=0.0016 | violations=0 | avgBarrier=10.4369 | gradNorm=1.6670\n",
      " Agent0 Episode 563 | avgR=0.0016 | violations=0 | avgBarrier=10.4713 | gradNorm=1.5646\n",
      " Agent0 Episode 564 | avgR=0.0016 | violations=0 | avgBarrier=10.4824 | gradNorm=2.0504\n",
      " Agent0 Episode 565 | avgR=0.0016 | violations=0 | avgBarrier=10.4076 | gradNorm=1.1832\n",
      " Agent0 Episode 566 | avgR=0.0016 | violations=0 | avgBarrier=10.4279 | gradNorm=1.8349\n",
      " Agent0 Episode 567 | avgR=0.0016 | violations=0 | avgBarrier=10.4481 | gradNorm=1.6904\n",
      " Agent0 Episode 568 | avgR=0.0016 | violations=0 | avgBarrier=10.3635 | gradNorm=1.8151\n",
      " Agent0 Episode 569 | avgR=0.0016 | violations=0 | avgBarrier=10.3832 | gradNorm=1.4070\n",
      " Agent0 Episode 570 | avgR=0.0016 | violations=0 | avgBarrier=10.4028 | gradNorm=1.2272\n",
      " Agent0 Episode 571 | avgR=0.0016 | violations=0 | avgBarrier=10.3181 | gradNorm=1.8924\n",
      " Agent0 Episode 572 | avgR=0.0016 | violations=0 | avgBarrier=10.3373 | gradNorm=1.3799\n",
      " Agent0 Episode 573 | avgR=0.0016 | violations=0 | avgBarrier=10.3564 | gradNorm=1.2953\n",
      " Agent0 Episode 574 | avgR=0.0016 | violations=0 | avgBarrier=10.2716 | gradNorm=1.5595\n",
      " Agent0 Episode 575 | avgR=0.0016 | violations=0 | avgBarrier=10.2903 | gradNorm=1.5523\n",
      " Agent0 Episode 576 | avgR=0.0016 | violations=0 | avgBarrier=10.3088 | gradNorm=1.3366\n",
      " Agent0 Episode 577 | avgR=0.0016 | violations=0 | avgBarrier=10.2240 | gradNorm=1.6108\n",
      " Agent0 Episode 578 | avgR=0.0016 | violations=0 | avgBarrier=10.2290 | gradNorm=2.1666\n",
      " Agent0 Episode 579 | avgR=0.0016 | violations=0 | avgBarrier=10.2603 | gradNorm=1.5875\n",
      " Agent0 Episode 580 | avgR=0.0016 | violations=0 | avgBarrier=10.1754 | gradNorm=1.2035\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[2.6507e-06, 7.1390e-02, 1.8604e-01, 7.4257e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[3.6289e-06, 9.3427e-02, 9.0657e-01, 1.1766e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[3.2285e-02, 9.5386e-01, 1.3853e-02, 5.8900e-07]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #   0   # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 3.]])\n",
      "Prob:  tensor([[0.5339, 0.2363, 0.0737, 0.1560]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[2.6507e-06, 7.1390e-02, 1.8604e-01, 7.4257e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[3.6289e-06, 9.3427e-02, 9.0657e-01, 1.1766e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 2.]])\n",
      "Prob:  tensor([[3.2285e-02, 9.5386e-01, 1.3853e-02, 5.8900e-07]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# #   0   # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[3., 3.]])\n",
      "Prob:  tensor([[0.5339, 0.2363, 0.0737, 0.1560]], grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 3.]])\n",
      "Prob:  tensor([[2.6507e-06, 7.1390e-02, 1.8604e-01, 7.4257e-01]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# # 0     #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      "State:  tensor([[2., 2.]])\n",
      "Prob:  tensor([[3.6289e-06, 9.3427e-02, 9.0657e-01, 1.1766e-06]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #       #   # #\n",
      "# # 0     # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n",
      " Agent0 Episode 581 | avgR=0.0016 | violations=0 | avgBarrier=10.1929 | gradNorm=2.7629\n",
      " Agent0 Episode 582 | avgR=0.0016 | violations=0 | avgBarrier=10.2103 | gradNorm=2.2067\n",
      " Agent0 Episode 583 | avgR=0.0016 | violations=0 | avgBarrier=10.1198 | gradNorm=2.1509\n",
      " Agent0 Episode 584 | avgR=0.0016 | violations=0 | avgBarrier=10.1424 | gradNorm=1.7044\n",
      " Agent0 Episode 585 | avgR=0.0016 | violations=0 | avgBarrier=10.1499 | gradNorm=1.4909\n",
      " Agent0 Episode 586 | avgR=0.0016 | violations=0 | avgBarrier=10.0743 | gradNorm=1.7442\n",
      " Agent0 Episode 587 | avgR=0.0016 | violations=0 | avgBarrier=10.0907 | gradNorm=1.1095\n",
      " Agent0 Episode 588 | avgR=0.0016 | violations=0 | avgBarrier=10.0978 | gradNorm=1.4903\n",
      " Agent0 Episode 589 | avgR=0.0016 | violations=0 | avgBarrier=10.0221 | gradNorm=1.3095\n",
      " Agent0 Episode 590 | avgR=0.0016 | violations=0 | avgBarrier=10.0379 | gradNorm=1.2493\n",
      " Agent0 Episode 591 | avgR=0.0016 | violations=0 | avgBarrier=10.0536 | gradNorm=2.1556\n",
      " Agent0 Episode 592 | avgR=0.0016 | violations=1 | avgBarrier=36.1227 | gradNorm=71.8610\n",
      " Agent0 Episode 593 | avgR=0.0016 | violations=0 | avgBarrier=9.5387 | gradNorm=1.9097\n",
      " Agent0 Episode 594 | avgR=0.0016 | violations=0 | avgBarrier=9.5450 | gradNorm=1.1589\n",
      " Agent0 Episode 595 | avgR=0.0016 | violations=0 | avgBarrier=9.4800 | gradNorm=1.1017\n",
      " Agent0 Episode 596 | avgR=0.0016 | violations=0 | avgBarrier=9.4981 | gradNorm=1.9092\n",
      " Agent0 Episode 597 | avgR=0.0016 | violations=0 | avgBarrier=9.5161 | gradNorm=1.6421\n"
     ]
    }
   ],
   "source": [
    "# Initialize env with 2 agents\n",
    "env = MultiAgentMazeEnv(\n",
    "        size=(9,9),\n",
    "        starts=[(2,3)],\n",
    "        goals=[(3,6)],\n",
    "        inner_walls=inner_walls,\n",
    "        outer_walls=outer_walls\n",
    "    )\n",
    "env.render()\n",
    "\n",
    "policies = [SoftmaxPolicy(width=W, height=H, num_actions=A) for _ in range(env.n_agents)]\n",
    "optimizers = [torch.optim.Adam(p.parameters(), lr=0.1) for p in policies]\n",
    "\n",
    "initialize_policy_with_manual_probs(policies[0], probs_padded)\n",
    "scores, V, barrier, violation = reinforce_multi_rwd2go_alt_barrier_new(env, policies, optimizers)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "for i in range(len(barrier)):\n",
    "    ax1.plot(barrier[i], label=f\"Agent {i} Penalty\", linestyle='-')\n",
    "    ax2.plot(violation[i], label=f\"Agent {i} Violations\", linestyle='--', color='red')\n",
    "\n",
    "ax1.set_xlabel(\"Episode\")\n",
    "ax1.set_ylabel(\"Mean Barrier Penalty\", color='tab:blue')\n",
    "ax2.set_ylabel(\"Violation Number\", color='tab:red')\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "plt.title(\"Barrier Penalty and Constraint Violations Over Training\")\n",
    "plt.show()\n",
    "\n",
    "# --- Plot Expected Reward (Value Function) ---\n",
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "for i in range(len(V)):\n",
    "    plt.plot(V[i], label=f\"Agent {i} Value\", linewidth=2)\n",
    "\n",
    "plt.xlabel(\"Batch Number\")\n",
    "plt.ylabel(\"Estimated Expected Return V(s)\")\n",
    "plt.title(\"Expected Return (Value Function) Over Training\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d36b83-e52e-4257-99f0-ad30500797bd",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-12-14T00:22:35.896146Z",
     "iopub.status.idle": "2025-12-14T00:22:35.896146Z",
     "shell.execute_reply": "2025-12-14T00:22:35.896146Z",
     "shell.execute_reply.started": "2025-12-14T00:22:35.896146Z"
    }
   },
   "outputs": [],
   "source": [
    "H, W = 5, 5\n",
    "A = 4\n",
    "eps = 1e-3\n",
    "# Define your action probabilities manually for each cell\n",
    "probs = np.zeros((H, W, A))\n",
    "probs[0, 0] = [0.0, 1/2, 1/2, 0.0]   # up, right, down, left\n",
    "probs[0, 1] = [0.0, 1/2, 0.0, 1/2]   # up, right, down, left\n",
    "probs[0, 2] = [0.0, 1/2, 0.0, 1/2]   # up, right, down, left\n",
    "probs[0, 3] = [0.0, 1/2, 0.0, 1/2]   # up, right, down, left\n",
    "probs[0, 4] = [0.0, 0.0, 1/2, 1/2]   # up, right, down, left\n",
    "\n",
    "probs[1, 0] = [1/2, 0.0, 1/2, 0.0]   # up, right, down, left\n",
    "probs[1, 1] = [1/4, 1/4, 1/4, 1/4]   # up, right, down, left\n",
    "probs[1, 2] = [1/4, 1/4, 1/4, 1/4]    # up, right, down, left\n",
    "probs[1, 3] = [1/4, 1/4, 1/4, 1/4]   # up, right, down, left\n",
    "probs[1, 4] = [1/2, 0.0, 1/2, 0.0]   # up, right, down, left\n",
    "\n",
    "probs[2, 0] = [1/2, 0.0, 1/2, 0.0]   # up, right, down, left\n",
    "probs[2, 1] = [1/4, 1/4, 1/4, 1/4]   # up, right, down, left\n",
    "probs[2, 2] = [1/4, 1/4, 1/4, 1/4]    # up, right, down, left\n",
    "probs[2, 3] = [0.0, 1/2, 0.0, 1/2]   # up, right, down, left\n",
    "probs[2, 4] = [1/3, 0.0, 1/3, 1/3]   # up, right, down, left\n",
    "\n",
    "probs[3, 0] = [1/2, 0.0, 1/2, 0.0]   # up, right, down, left\n",
    "probs[3, 1] = [1/4, 1/4, 1/4, 1/4]   # up, right, down, left\n",
    "probs[3, 2] = [1/4, 1/4, 1/4, 1/4]    # up, right, down, left\n",
    "probs[3, 3] = [1/4, 1/4, 1/4, 1/4]   # up, right, down, left\n",
    "probs[3, 4] = [1/2, 0.0, 1/2, 0.0]   # up, right, down, left\n",
    "\n",
    "probs[4, 0] = [1/2, 1/2, 0.0, 0.0]   # up, right, down, left\n",
    "probs[4, 1] = [0.0, 1/2, 0.0, 1/2]   # up, right, down, left\n",
    "probs[4, 2] = [0.0, 1/2, 0.0, 1/2]    # up, right, down, left\n",
    "probs[4, 3] = [0.0, 1/2, 0.0, 1/2]   # up, right, down, left\n",
    "probs[4, 4] = [1/2, 0.0, 0.0, 1/2]   # up, right, down, left\n",
    "\n",
    "probs = np.maximum(probs, eps)   # ensure no zeros\n",
    "probs = probs / probs.sum(axis=2, keepdims=True)  # normalize per state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00fc9358-47d9-4f91-935b-18ffa2769701",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-11T15:07:52.765848Z",
     "iopub.status.busy": "2025-12-11T15:07:52.764850Z",
     "iopub.status.idle": "2025-12-11T15:21:36.227960Z",
     "shell.execute_reply": "2025-12-11T15:21:36.227960Z",
     "shell.execute_reply.started": "2025-12-11T15:07:52.765848Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         \n",
      "  # # #  \n",
      "0 # \u001b[92m0\u001b[0m    \n",
      "  # # #  \n",
      "         \n",
      "----------\n",
      "\n",
      "==================================================\n",
      "=== Round 1 ===\n",
      "==================================================\n",
      "\n",
      "--- Training Agent 0 ---\n",
      " Agent0 Episode 1 | avgR=0.0391 | violations=0.03125 | avgBarrier=0.0319 | gradNorm=6.9195\n",
      " Agent0 Episode 2 | avgR=0.0566 | violations=0.02734375 | avgBarrier=0.0462 | gradNorm=7.4630\n",
      " Agent0 Episode 3 | avgR=0.0508 | violations=0.01953125 | avgBarrier=0.0302 | gradNorm=4.9314\n",
      " Agent0 Episode 4 | avgR=0.0713 | violations=0.015625 | avgBarrier=0.0231 | gradNorm=4.3913\n",
      " Agent0 Episode 5 | avgR=0.0813 | violations=0.015625 | avgBarrier=0.0247 | gradNorm=4.9050\n",
      " Agent0 Episode 6 | avgR=0.0944 | violations=0.015625 | avgBarrier=0.0301 | gradNorm=4.9243\n",
      " Agent0 Episode 7 | avgR=0.1044 | violations=0.01171875 | avgBarrier=0.0266 | gradNorm=4.0317\n",
      " Agent0 Episode 8 | avgR=0.1182 | violations=0.01171875 | avgBarrier=0.0081 | gradNorm=3.8294\n",
      " Agent0 Episode 9 | avgR=0.1280 | violations=0.015625 | avgBarrier=0.0099 | gradNorm=4.9205\n",
      " Agent0 Episode 10 | avgR=0.1457 | violations=0.02734375 | avgBarrier=0.0202 | gradNorm=8.9192\n",
      " Agent0 Episode 11 | avgR=0.1499 | violations=0.0078125 | avgBarrier=0.0271 | gradNorm=3.0283\n",
      " Agent0 Episode 12 | avgR=0.1562 | violations=0.01171875 | avgBarrier=0.0036 | gradNorm=3.7297\n",
      " Agent0 Episode 13 | avgR=0.1614 | violations=0.01171875 | avgBarrier=0.0254 | gradNorm=4.3513\n",
      " Agent0 Episode 14 | avgR=0.1657 | violations=0.0078125 | avgBarrier=0.0314 | gradNorm=4.0668\n",
      " Agent0 Episode 15 | avgR=0.1727 | violations=0.01171875 | avgBarrier=0.0184 | gradNorm=3.7464\n",
      " Agent0 Episode 16 | avgR=0.1792 | violations=0.01171875 | avgBarrier=0.0219 | gradNorm=3.6350\n",
      " Agent0 Episode 17 | avgR=0.1877 | violations=0.00390625 | avgBarrier=0.0110 | gradNorm=2.1076\n",
      " Agent0 Episode 18 | avgR=0.1984 | violations=0.0078125 | avgBarrier=0.0057 | gradNorm=2.9736\n",
      " Agent0 Episode 19 | avgR=0.2052 | violations=0.00390625 | avgBarrier=0.0121 | gradNorm=2.1123\n",
      " Agent0 Episode 20 | avgR=0.2098 | violations=0.00390625 | avgBarrier=0.0009 | gradNorm=2.0776\n",
      " Agent0 Episode 21 | avgR=0.2246 | violations=0.0078125 | avgBarrier=0.0186 | gradNorm=2.9382\n",
      " Agent0 Episode 22 | avgR=0.2371 | violations=0.015625 | avgBarrier=0.0298 | gradNorm=4.4886\n",
      " Agent0 Episode 23 | avgR=0.2539 | violations=0.0078125 | avgBarrier=0.0075 | gradNorm=2.9403\n",
      " Agent0 Episode 24 | avgR=0.2699 | violations=0.00390625 | avgBarrier=0.0010 | gradNorm=2.0636\n",
      " Agent0 Episode 25 | avgR=0.2818 | violations=0.00390625 | avgBarrier=0.0019 | gradNorm=2.1116\n",
      " Agent0 Episode 26 | avgR=0.2941 | violations=0.00390625 | avgBarrier=0.0009 | gradNorm=2.0394\n",
      " Agent0 Episode 27 | avgR=0.3059 | violations=0.01171875 | avgBarrier=0.0230 | gradNorm=3.6437\n",
      " Agent0 Episode 28 | avgR=0.3137 | violations=0.00390625 | avgBarrier=0.0085 | gradNorm=2.1118\n",
      " Agent0 Episode 29 | avgR=0.3264 | violations=0.00390625 | avgBarrier=0.0029 | gradNorm=2.0328\n",
      " Agent0 Episode 30 | avgR=0.3346 | violations=0.015625 | avgBarrier=0.0214 | gradNorm=4.6156\n",
      " Agent0 Episode 31 | avgR=0.3531 | violations=0.0078125 | avgBarrier=0.0192 | gradNorm=2.8294\n",
      " Agent0 Episode 32 | avgR=0.3670 | violations=0.0078125 | avgBarrier=0.0020 | gradNorm=2.8579\n",
      " Agent0 Episode 33 | avgR=0.3822 | violations=0.00390625 | avgBarrier=0.0020 | gradNorm=2.0548\n",
      " Agent0 Episode 34 | avgR=0.3967 | violations=0.00390625 | avgBarrier=0.0010 | gradNorm=2.0779\n",
      " Agent0 Episode 35 | avgR=0.4105 | violations=0.00390625 | avgBarrier=0.0248 | gradNorm=2.0386\n",
      " Agent0 Episode 36 | avgR=0.4252 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.2057\n",
      " Agent0 Episode 37 | avgR=0.4385 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.1739\n",
      " Agent0 Episode 38 | avgR=0.4482 | violations=0.00390625 | avgBarrier=0.0030 | gradNorm=2.0267\n",
      " Agent0 Episode 39 | avgR=0.4598 | violations=0.00390625 | avgBarrier=0.0010 | gradNorm=2.1000\n",
      " Agent0 Episode 40 | avgR=0.4744 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.1799\n",
      " Agent0 Episode 41 | avgR=0.4859 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.1576\n",
      " Agent0 Episode 42 | avgR=0.5020 | violations=0.00390625 | avgBarrier=0.0189 | gradNorm=1.9476\n",
      " Agent0 Episode 43 | avgR=0.5131 | violations=0.00390625 | avgBarrier=0.0010 | gradNorm=1.9169\n",
      " Agent0 Episode 44 | avgR=0.5240 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.1718\n",
      " Agent0 Episode 45 | avgR=0.5379 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.1608\n",
      " Agent0 Episode 46 | avgR=0.5523 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.1744\n",
      " Agent0 Episode 47 | avgR=0.5686 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.1713\n",
      " Agent0 Episode 48 | avgR=0.5840 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.1607\n",
      " Agent0 Episode 49 | avgR=0.5959 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.1651\n",
      " Agent0 Episode 50 | avgR=0.6111 | violations=0.00390625 | avgBarrier=0.0023 | gradNorm=1.9121\n",
      " Agent0 Episode 51 | avgR=0.6201 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.1631\n",
      " Agent0 Episode 52 | avgR=0.6328 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.1615\n",
      " Agent0 Episode 53 | avgR=0.6459 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.1491\n",
      " Agent0 Episode 54 | avgR=0.6568 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.1414\n",
      " Agent0 Episode 55 | avgR=0.6703 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.1586\n",
      " Agent0 Episode 56 | avgR=0.6791 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.1345\n",
      " Agent0 Episode 57 | avgR=0.6904 | violations=0.01171875 | avgBarrier=0.0279 | gradNorm=3.3940\n",
      " Agent0 Episode 58 | avgR=0.7021 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.1439\n",
      " Agent0 Episode 59 | avgR=0.7156 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.1549\n",
      " Agent0 Episode 60 | avgR=0.7291 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.1638\n",
      " Agent0 Episode 61 | avgR=0.7443 | violations=0.00390625 | avgBarrier=0.0201 | gradNorm=1.9038\n",
      " Agent0 Episode 62 | avgR=0.7551 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.1393\n",
      " Agent0 Episode 63 | avgR=0.7691 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.1241\n",
      " Agent0 Episode 64 | avgR=0.7795 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.1366\n",
      " Agent0 Episode 65 | avgR=0.7920 | violations=0.00390625 | avgBarrier=0.0133 | gradNorm=1.8169\n",
      " Agent0 Episode 66 | avgR=0.8012 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.1186\n",
      " Agent0 Episode 67 | avgR=0.8070 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.1195\n",
      " Agent0 Episode 68 | avgR=0.8176 | violations=0.00390625 | avgBarrier=0.0012 | gradNorm=2.0249\n",
      " Agent0 Episode 69 | avgR=0.8270 | violations=0.00390625 | avgBarrier=0.0012 | gradNorm=1.7996\n",
      " Agent0 Episode 70 | avgR=0.8352 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.1199\n",
      " Agent0 Episode 71 | avgR=0.8449 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.1154\n",
      " Agent0 Episode 72 | avgR=0.8537 | violations=0.00390625 | avgBarrier=0.0026 | gradNorm=2.3621\n",
      " Agent0 Episode 73 | avgR=0.8594 | violations=0.00390625 | avgBarrier=0.0026 | gradNorm=1.8726\n",
      " Agent0 Episode 74 | avgR=0.8697 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.1113\n",
      " Agent0 Episode 75 | avgR=0.8748 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0881\n",
      " Agent0 Episode 76 | avgR=0.8852 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.1057\n",
      " Agent0 Episode 77 | avgR=0.8912 | violations=0.00390625 | avgBarrier=0.0013 | gradNorm=1.8811\n",
      " Agent0 Episode 78 | avgR=0.8992 | violations=0.00390625 | avgBarrier=0.0013 | gradNorm=1.9350\n",
      " Agent0 Episode 79 | avgR=0.9055 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.1070\n",
      " Agent0 Episode 80 | avgR=0.9098 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0974\n",
      " Agent0 Episode 81 | avgR=0.9139 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0803\n",
      " Agent0 Episode 82 | avgR=0.9180 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0981\n",
      " Agent0 Episode 83 | avgR=0.9223 | violations=0.00390625 | avgBarrier=0.0014 | gradNorm=1.7934\n",
      " Agent0 Episode 84 | avgR=0.9275 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0816\n",
      " Agent0 Episode 85 | avgR=0.9309 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0762\n",
      " Agent0 Episode 86 | avgR=0.9352 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0898\n",
      " Agent0 Episode 87 | avgR=0.9420 | violations=0.00390625 | avgBarrier=0.0069 | gradNorm=1.8717\n",
      " Agent0 Episode 88 | avgR=0.9443 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0758\n",
      " Agent0 Episode 89 | avgR=0.9492 | violations=0.00390625 | avgBarrier=0.0014 | gradNorm=1.8886\n",
      " Agent0 Episode 90 | avgR=0.9506 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0763\n",
      " Agent0 Episode 91 | avgR=0.9527 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0810\n",
      " Agent0 Episode 92 | avgR=0.9553 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0800\n",
      " Agent0 Episode 93 | avgR=0.9590 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0799\n",
      " Agent0 Episode 94 | avgR=0.9613 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0741\n",
      " Agent0 Episode 95 | avgR=0.9648 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0678\n",
      " Agent0 Episode 96 | avgR=0.9664 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0745\n",
      " Agent0 Episode 97 | avgR=0.9693 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0739\n",
      " Agent0 Episode 98 | avgR=0.9697 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0695\n",
      " Agent0 Episode 99 | avgR=0.9719 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0548\n",
      " Agent0 Episode 100 | avgR=0.9742 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0636\n",
      " Agent0 Episode 101 | avgR=0.9748 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0598\n",
      " Agent0 Episode 102 | avgR=0.9770 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0646\n",
      " Agent0 Episode 103 | avgR=0.9781 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0689\n",
      " Agent0 Episode 104 | avgR=0.9785 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0589\n",
      " Agent0 Episode 105 | avgR=0.9801 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0707\n",
      " Agent0 Episode 106 | avgR=0.9811 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0717\n",
      " Agent0 Episode 107 | avgR=0.9816 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0682\n",
      " Agent0 Episode 108 | avgR=0.9844 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0648\n",
      " Agent0 Episode 109 | avgR=0.9848 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0634\n",
      " Agent0 Episode 110 | avgR=0.9863 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0520\n",
      " Agent0 Episode 111 | avgR=0.9873 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0663\n",
      " Agent0 Episode 112 | avgR=0.9875 | violations=0.00390625 | avgBarrier=0.0015 | gradNorm=1.6950\n",
      " Agent0 Episode 113 | avgR=0.9885 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0622\n",
      " Agent0 Episode 114 | avgR=0.9889 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0617\n",
      " Agent0 Episode 115 | avgR=0.9891 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0623\n",
      " Agent0 Episode 116 | avgR=0.9895 | violations=0.00390625 | avgBarrier=0.0030 | gradNorm=1.6296\n",
      " Agent0 Episode 117 | avgR=0.9895 | violations=0.00390625 | avgBarrier=0.0089 | gradNorm=1.5946\n",
      " Agent0 Episode 118 | avgR=0.9904 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0472\n",
      " Agent0 Episode 119 | avgR=0.9902 | violations=0.00390625 | avgBarrier=0.0015 | gradNorm=1.7126\n",
      " Agent0 Episode 120 | avgR=0.9902 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0547\n",
      " Agent0 Episode 121 | avgR=0.9918 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0586\n",
      " Agent0 Episode 122 | avgR=0.9924 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0445\n",
      " Agent0 Episode 123 | avgR=0.9930 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0490\n",
      " Agent0 Episode 124 | avgR=0.9934 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0597\n",
      " Agent0 Episode 125 | avgR=0.9939 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0544\n",
      " Agent0 Episode 126 | avgR=0.9945 | violations=0.00390625 | avgBarrier=0.0352 | gradNorm=1.6217\n",
      " Agent0 Episode 127 | avgR=0.9947 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0563\n",
      " Agent0 Episode 128 | avgR=0.9949 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0422\n",
      " Agent0 Episode 129 | avgR=0.9953 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0797\n",
      " Agent0 Episode 130 | avgR=0.9953 | violations=0.00390625 | avgBarrier=0.0015 | gradNorm=1.6040\n",
      " Agent0 Episode 131 | avgR=0.9953 | violations=0.00390625 | avgBarrier=0.0062 | gradNorm=1.6262\n",
      " Agent0 Episode 132 | avgR=0.9959 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0620\n",
      " Agent0 Episode 133 | avgR=0.9957 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0478\n",
      " Agent0 Episode 134 | avgR=0.9957 | violations=0.00390625 | avgBarrier=0.0016 | gradNorm=1.5179\n",
      " Agent0 Episode 135 | avgR=0.9959 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0411\n",
      " Agent0 Episode 136 | avgR=0.9963 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0404\n",
      " Agent0 Episode 137 | avgR=0.9965 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0436\n",
      " Agent0 Episode 138 | avgR=0.9967 | violations=0.00390625 | avgBarrier=0.0016 | gradNorm=1.5144\n",
      " Agent0 Episode 139 | avgR=0.9971 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0557\n",
      " Agent0 Episode 140 | avgR=0.9975 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0463\n",
      " Agent0 Episode 141 | avgR=0.9973 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0525\n",
      " Agent0 Episode 142 | avgR=0.9973 | violations=0.00390625 | avgBarrier=0.0470 | gradNorm=1.3784\n",
      " Agent0 Episode 143 | avgR=0.9973 | violations=0.00390625 | avgBarrier=0.0016 | gradNorm=1.3724\n",
      " Agent0 Episode 144 | avgR=0.9973 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0377\n",
      " Agent0 Episode 145 | avgR=0.9975 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0534\n",
      " Agent0 Episode 146 | avgR=0.9975 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0540\n",
      " Agent0 Episode 147 | avgR=0.9979 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0358\n",
      " Agent0 Episode 148 | avgR=0.9979 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0559\n",
      " Agent0 Episode 149 | avgR=0.9979 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0395\n",
      " Agent0 Episode 150 | avgR=0.9980 | violations=0.00390625 | avgBarrier=0.0032 | gradNorm=1.8225\n",
      " Agent0 Episode 151 | avgR=0.9980 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0454\n",
      " Agent0 Episode 152 | avgR=0.9980 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0408\n",
      " Agent0 Episode 153 | avgR=0.9984 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0394\n",
      " Agent0 Episode 154 | avgR=0.9988 | violations=0.0 | avgBarrier=0.0000 | gradNorm=0.0462\n",
      "Agent 0 converged by LB-SGD condition!\n",
      "\n",
      "--- End of Round 1: Computing Round Averages ---\n",
      "         \n",
      "  # # #  \n",
      "0 # \u001b[92m0\u001b[0m    \n",
      "  # # #  \n",
      "         \n",
      "----------\n",
      "         \n",
      "0 # # #  \n",
      "  # \u001b[92m0\u001b[0m    \n",
      "  # # #  \n",
      "         \n",
      "----------\n",
      "0        \n",
      "  # # #  \n",
      "  # \u001b[92m0\u001b[0m    \n",
      "  # # #  \n",
      "         \n",
      "----------\n",
      "  0      \n",
      "  # # #  \n",
      "  # \u001b[92m0\u001b[0m    \n",
      "  # # #  \n",
      "         \n",
      "----------\n",
      "    0    \n",
      "  # # #  \n",
      "  # \u001b[92m0\u001b[0m    \n",
      "  # # #  \n",
      "         \n",
      "----------\n",
      "      0  \n",
      "  # # #  \n",
      "  # \u001b[92m0\u001b[0m    \n",
      "  # # #  \n",
      "         \n",
      "----------\n",
      "        0\n",
      "  # # #  \n",
      "  # \u001b[92m0\u001b[0m    \n",
      "  # # #  \n",
      "         \n",
      "----------\n",
      "         \n",
      "  # # # 0\n",
      "  # \u001b[92m0\u001b[0m    \n",
      "  # # #  \n",
      "         \n",
      "----------\n",
      "         \n",
      "  # # #  \n",
      "  # \u001b[92m0\u001b[0m   0\n",
      "  # # #  \n",
      "         \n",
      "----------\n",
      "         \n",
      "  # # #  \n",
      "  # \u001b[92m0\u001b[0m 0  \n",
      "  # # #  \n",
      "         \n",
      "----------\n",
      "         \n",
      "  # # #  \n",
      "  # 0    \n",
      "  # # #  \n",
      "         \n",
      "----------\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize env with 2 agents\n",
    "env = random_maze_env(size=(5,5), n_agents=1, n_walls=6, seed=489200903)\n",
    "env = MultiAgentMazeEnv(\n",
    "        size=(5,5),\n",
    "        starts=[(2,0)],\n",
    "        goals=[(2,2)],\n",
    "        walls=[(1,3),(3,3),(1,1),(1,2),(2,1),(3,1),(3,2)]\n",
    "    )\n",
    "env.render()\n",
    "\n",
    "policies = [SoftmaxPolicy(width=W, height=H, num_actions=A) for _ in range(env.n_agents)]\n",
    "optimizers = [torch.optim.Adam(p.parameters(), lr=0.1) for p in policies]\n",
    "\n",
    "initialize_policy_with_manual_probs(policies[0], probs)\n",
    "scores, V, barrier, violation = reinforce_multi_rwd2go_alt_barrier(env, policies, optimizers)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "for i in range(len(barrier)):\n",
    "    ax1.plot(barrier[i], label=f\"Agent {i} Penalty\", linestyle='-')\n",
    "    ax2.plot(violation[i], label=f\"Agent {i} Violations\", linestyle='--', color='red')\n",
    "\n",
    "ax1.set_xlabel(\"Episode\")\n",
    "ax1.set_ylabel(\"Mean Barrier Penalty\", color='tab:blue')\n",
    "ax2.set_ylabel(\"Violation Rate (fraction of 256 episodes)\", color='tab:red')\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "plt.title(\"Barrier Penalty and Constraint Violation Rate Over Training\")\n",
    "plt.show()\n",
    "\n",
    "# --- Plot Expected Reward (Value Function) ---\n",
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "for i in range(len(V)):\n",
    "    plt.plot(V[i], label=f\"Agent {i} Value\", linewidth=2)\n",
    "\n",
    "plt.xlabel(\"Batch Number\")\n",
    "plt.ylabel(\"Estimated Expected Return V(s)\")\n",
    "plt.title(\"Expected Return (Value Function) Over Training\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea1f218-01d2-4c94-8a56-119414929ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_Project",
   "language": "python",
   "name": "rl_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
