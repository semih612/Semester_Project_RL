{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71b87532-9cf1-41ec-a947-e52421a3e73e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T00:50:37.253122Z",
     "iopub.status.busy": "2025-12-15T00:50:37.253122Z",
     "iopub.status.idle": "2025-12-15T00:50:42.933916Z",
     "shell.execute_reply": "2025-12-15T00:50:42.932924Z",
     "shell.execute_reply.started": "2025-12-15T00:50:37.253122Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "# #   0   #   # #\n",
      "# #       # \u001b[92m0\u001b[0m # #\n",
      "# #       #   # #\n",
      "# #       #   # #\n",
      "# #           # #\n",
      "# # # # # # # # #\n",
      "# # # # # # # # #\n",
      "----------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 91\u001b[39m\n\u001b[32m     88\u001b[39m optimizers = [torch.optim.Adam(p.parameters(), lr=\u001b[32m0.1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m policies]\n\u001b[32m     90\u001b[39m initialize_policy_with_manual_probs(policies[\u001b[32m0\u001b[39m], probs_padded)\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m scores, barrier, violation = \u001b[43mreinforce_multi_rwd2go_alt_barrier\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m fig, ax1 = plt.subplots()\n\u001b[32m     94\u001b[39m ax2 = ax1.twinx()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive - sabanciuniv.edu\\Desktop\\SemesterPr_RL\\ContrainedRL\\training\\reinforce.py:30\u001b[39m, in \u001b[36mreinforce_multi_rwd2go_alt_barrier\u001b[39m\u001b[34m(env, policies, optimizers, n_episodes, max_t, gamma, batch_size, num_cons)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_episodes):\n\u001b[32m     28\u001b[39m     batch_data, batch_violation_count  = collect_batch_trajectories(env, policies, phase, batch_size, max_t)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     C_tau_batch = \u001b[43mtorch\u001b[49m.stack([traj[\u001b[33m\"\u001b[39m\u001b[33mC_tau\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m traj \u001b[38;5;129;01min\u001b[39;00m batch_data])\n\u001b[32m     31\u001b[39m     V_i_estimate = update_constraint_value_estimates_batch(C_tau_batch, V_i_estimate, alpha_vi)\n\u001b[32m     33\u001b[39m     batch_loss = \u001b[32m0.0\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- env ---\n",
    "from envs import MultiAgentMazeEnv\n",
    "\n",
    "# --- policies ---\n",
    "from policies import SoftmaxPolicy\n",
    "from utils.init_policy import initialize_policy_with_manual_probs   # ðŸ‘ˆ REQUIRED\n",
    "\n",
    "# --- training ---\n",
    "from training import reinforce_multi_rwd2go_alt_barrier\n",
    "\n",
    "H0, W0 = 5, 5\n",
    "A = 4\n",
    "eps = 1e-3\n",
    "pad = 2\n",
    "# new padded size\n",
    "H = H0 + 2 * pad\n",
    "W = W0 + 2 * pad\n",
    "\n",
    "# Define your action probabilities manually for each cell\n",
    "probs = np.zeros((H0, W0, A))\n",
    "probs[0, 0] = [0.0, 1/2, 1/2, 0.0]   # up, right, down, left\n",
    "probs[0, 1] = [0.0, 1/3, 1/3, 1/3]   # up, right, down, left\n",
    "probs[0, 2] = [0.0, 0.0, 1/2, 1/2]   # up, right, down, left\n",
    "probs[0, 3] = [1/4, 1/4, 1/4, 1/4]   # up, right, down, left\n",
    "probs[0, 4] = [0.0, 0.0, 1.0, 0.0]   # up, right, down, left\n",
    "\n",
    "probs[1, 0] = [1/3, 1/3, 1/3, 0.0]   # up, right, down, left\n",
    "probs[1, 1] = [1/4, 1/4, 1/4, 1/4]   # up, right, down, left\n",
    "probs[1, 2] = [1/3, 0.0, 1/3, 1/3]    # up, right, down, left\n",
    "probs[1, 3] = [1/4, 1/4, 1/4, 1/4]   # up, right, down, left\n",
    "probs[1, 4] = [1/4, 1/4, 1/4, 1/4]   # up, right, down, left\n",
    "\n",
    "probs[2, 0] = [1/3, 1/3, 1/3, 0.0]   # up, right, down, left\n",
    "probs[2, 1] = [1/4, 1/4, 1/4, 1/4]   # up, right, down, left\n",
    "probs[2, 2] = [1/3, 0.0, 1/3, 1/3]    # up, right, down, left\n",
    "probs[2, 3] = [1/4, 1/4, 1/4, 1/4]   # up, right, down, left\n",
    "probs[2, 4] = [1/2, 0.0, 1/2, 0.0]   # up, right, down, left\n",
    "\n",
    "probs[3, 0] = [1/3, 1/3, 1/3, 0.0]   # up, right, down, left\n",
    "probs[3, 1] = [1/4, 1/4, 1/4, 1/4]   # up, right, down, left\n",
    "probs[3, 2] = [1/3, 0.0, 1/3, 1/3]    # up, right, down, left\n",
    "probs[3, 3] = [1/4, 1/4, 1/4, 1/4]   # up, right, down, left\n",
    "probs[3, 4] = [1/2, 0.0, 1/2, 0.0]   # up, right, down, left\n",
    "\n",
    "probs[4, 0] = [1/2, 1/2, 0.0, 0.0]   # up, right, down, left\n",
    "probs[4, 1] = [1/3, 1/3, 0.0, 1/3]   # up, right, down, left\n",
    "probs[4, 2] = [1/3, 1/3, 0.0, 1/3]    # up, right, down, left\n",
    "probs[4, 3] = [0.0, 1/2, 0.0, 1/2]   # up, right, down, left\n",
    "probs[4, 4] = [1/2, 0.0, 0.0, 1/2]   # up, right, down, left\n",
    "\n",
    "# initialize padded grid with uniform wall policy\n",
    "probs_padded = np.ones((H, W, A)) / A\n",
    "\n",
    "# copy original policy into the center\n",
    "probs_padded[pad:pad+H0, pad:pad+W0, :] = probs\n",
    "\n",
    "# numerical safety (optional but consistent)\n",
    "probs_padded = np.maximum(probs_padded, eps)\n",
    "probs_padded = probs_padded / probs_padded.sum(axis=2, keepdims=True)\n",
    "\n",
    "outer_walls = []\n",
    "for i in range(H):\n",
    "    for j in range(W):\n",
    "        if i < pad or i >= H - pad or j < pad or j >= W - pad:\n",
    "            outer_walls.append((i, j)) \n",
    "\n",
    "inner_walls = [(0,3), (1,3), (2,3), (3,3)]\n",
    "# shift internal walls into padded grid\n",
    "inner_walls = [(x + pad, y + pad) for (x, y) in inner_walls]\n",
    "\n",
    "\n",
    "# Initialize env with 2 agents\n",
    "env = MultiAgentMazeEnv(\n",
    "        size=(9,9),\n",
    "        starts=[(2,3)],\n",
    "        goals=[(3,6)],\n",
    "        inner_walls=inner_walls,\n",
    "        outer_walls=outer_walls\n",
    "    )\n",
    "env.render()\n",
    "\n",
    "policies = [SoftmaxPolicy(width=W, height=H, num_actions=A) for _ in range(env.n_agents)]\n",
    "optimizers = [torch.optim.Adam(p.parameters(), lr=0.1) for p in policies]\n",
    "\n",
    "initialize_policy_with_manual_probs(policies[0], probs_padded)\n",
    "scores, barrier, violation = reinforce_multi_rwd2go_alt_barrier(env, policies, optimizers)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "for i in range(len(barrier)):\n",
    "    ax1.plot(barrier[i], label=f\"Agent {i} Penalty\", linestyle='-')\n",
    "    ax2.plot(violation[i], label=f\"Agent {i} Violations\", linestyle='--', color='red')\n",
    "\n",
    "ax1.set_xlabel(\"Episode\")\n",
    "ax1.set_ylabel(\"Mean Barrier Penalty\", color='tab:blue')\n",
    "ax2.set_ylabel(\"Violation Number\", color='tab:red')\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "plt.title(\"Barrier Penalty and Constraint Violations Over Training\")\n",
    "plt.show()\n",
    "\n",
    "# --- Plot Expected Reward (Value Function) ---\n",
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "for i in range(len(V)):\n",
    "    plt.plot(V[i], label=f\"Agent {i} Value\", linewidth=2)\n",
    "\n",
    "plt.xlabel(\"Batch Number\")\n",
    "plt.ylabel(\"Estimated Expected Return V(s)\")\n",
    "plt.title(\"Expected Return (Value Function) Over Training\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL_Project",
   "language": "python",
   "name": "rl_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
